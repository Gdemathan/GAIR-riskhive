{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file generates the seed for the rag database (json containing all the elements). Be careful, it takes a lot of time (25min) to run, and requires a text file of the reliability notebook (not commited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from src.utils import save_json, logger, read_json\n",
    "from src.client import openai_client\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the paragraphs that could contain equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 467 entries with equations.\n"
     ]
    }
   ],
   "source": [
    "# Load the file content\n",
    "with open(\"data/RT_textbook.txt\", \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split content into paragraphs separated by \"\\n\\n\"\n",
    "paragraphs = content.split(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# Function to extract paragraphs containing equations and their context\n",
    "def extract_equation_context(paragraphs):\n",
    "    equation_pattern = re.compile(r\".*\\s=\\s.*\")  # Regex to detect equations\n",
    "    extracted_content = []\n",
    "\n",
    "    for idx, paragraph in enumerate(paragraphs):\n",
    "        if equation_pattern.search(paragraph):  # Detect if the paragraph contains \" = \"\n",
    "            # Get previous, current, and next paragraphs\n",
    "            previous_paragraph = (\n",
    "                paragraphs[idx - 1].strip() if idx > 0 else \"No previous paragraph\"\n",
    "            )\n",
    "            next_paragraph = (\n",
    "                paragraphs[idx + 1].strip()\n",
    "                if idx + 1 < len(paragraphs)\n",
    "                else \"No next paragraph\"\n",
    "            )\n",
    "            extracted_content.append(\n",
    "                previous_paragraph + \"\\n\\n\" + paragraph + \"\\n\\n\" + next_paragraph\n",
    "            )\n",
    "\n",
    "    return extracted_content\n",
    "\n",
    "\n",
    "# Extract paragraphs with equations and their context\n",
    "all_content_raw = extract_equation_context(paragraphs)\n",
    "\n",
    "# Save results to JSON\n",
    "save_json(all_content_raw, fname=\"generated/extracted_rag.json\")\n",
    "\n",
    "logger.info(f\"Extracted {len(all_content_raw)} entries with equations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanitize the output and condense the paragraphs with chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from index 486 to 467 (excluded)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing paragraphs: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Condense the extracted paragraphs\n",
    "import os\n",
    "\n",
    "SYST_PROMPT = \"\"\"\n",
    "The following paragraph is extracted from a reliability textbook, containing equations and their context. It was extracted from a pdf,\n",
    "so some data might not be perfectly organised. I want you to condense the information in this paragraph, to:\n",
    "- make it more concise\n",
    "- clear the equations\n",
    "If you come across an equation, rewrite it in \"humanly understandable\" language, for example: \n",
    "f(x) = x^2 + 2*x + (some formulation of an integral easily understandable)\n",
    "The paragraph has to have the structure : \n",
    "**(Title that summarizes the paragraph)**\n",
    "(Condensed paragraph)\n",
    "Your must make the shortest paragraph possible, without losing the main information. Ideally, make it less than 800 symbols.\n",
    "\"\"\"\n",
    "first_index = 0\n",
    "if os.path.exists(\"generated/condensed_rag.json\"):\n",
    "    condensed_data = read_json(\"generated/condensed_rag.json\")\n",
    "first_index = len(condensed_data)\n",
    "\n",
    "added_length = 110\n",
    "\n",
    "print(\n",
    "    f\"Starting from index {first_index} to {min(first_index + added_length, len(all_content_raw))} (excluded)\"\n",
    ")\n",
    "\n",
    "for i in tqdm(\n",
    "    range(first_index, min(first_index + added_length, len(all_content_raw))),\n",
    "    desc=\"Processing paragraphs\",\n",
    "):\n",
    "    paragraph = all_content_raw[i]\n",
    "    condensed = (\n",
    "        openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYST_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": paragraph},\n",
    "            ],\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "    condensed_data.append(condensed)\n",
    "\n",
    "save_json(condensed_data, fname=\"generated/condensed_rag.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add the data specific to probabilities and the first handmade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final length 553\n"
     ]
    }
   ],
   "source": [
    "masterclass = read_json(\"generated/condensed_rag.json\")\n",
    "data_proba = read_json(\"data/rag_crafting/probstat_rag.json\")[\"RAG\"]\n",
    "data_risk_management = read_json(\"data/rag_crafting/risk_management_rag.json\")\n",
    "first_data = read_json(\"data/rag_crafting/general_rag_data.json\")\n",
    "for i in range(len(data_proba)):\n",
    "    title = list(data_proba[i].keys())[0]\n",
    "    entry = list(data_proba[i].values())[0]\n",
    "    new_entry = f\"**{title}**\\n{entry}\"\n",
    "    masterclass.append(new_entry)\n",
    "\n",
    "for i in range(len(first_data)):\n",
    "    new_entry = first_data[i]\n",
    "    masterclass.append(new_entry)\n",
    "\n",
    "for i in range(len(data_risk_management)):\n",
    "    entry = data_risk_management[i][\"entry\"]\n",
    "    masterclass.append(entry)\n",
    "\n",
    "save_json(masterclass, fname=\"data/rag_db_seed.json\")\n",
    "print(\"final length\", len(masterclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters:  40\n",
      "Clusters:  [[\"**Average Availability and Key Metrics for Repairable Systems**\\n\\nA machine with a Mean Time to Failure (MTTF) of 1,000 hours and a Mean Downtime (MDT) of 5 hours has an average availability (A_avg) of approximately 99.5%. This implies about 44 hours of downtime annually. The MTTF equals Mean Uptime (MUT) under perfect repair conditions. For items with constant rates, availability A(t) transitions to a limit A as time approaches infinity, calculated as (MUT)/(MUT + MDT). The approximation for average unavailability is MDT/(λ MDT), where λ is the failure rate. Operational availability (A_OP) is calculated based on planned and unplanned downtimes within a mission period. Additionally, production metrics include deliverability (actual deliveries/planned deliveries) and on-stream availability, assessing the system's operational performance against benchmarks.\", '**Availability and Unavailability of Repairable Items**\\n\\nThe average availability \\\\( A_{avg} \\\\) of a repairable item is calculated using the formula \\\\( A_{avg} = \\\\frac{MUT}{MUT + MDT} \\\\) where \\\\( MUT \\\\) is the Mean Up-Time and \\\\( MDT \\\\) is the Mean Downtime. For a machine with a Mean Time to Failure (MTTF) of 1,000 hours and a Mean Downtime of 5 hours, \\\\( A_{avg} \\\\approx 99.5\\\\% \\\\), indicating about 44 hours of downtime annually. Unavailability \\\\( U_{avg} \\\\) is defined as \\\\( U_{avg} = \\\\frac{MDT}{MUT + MDT} \\\\). The operational availability \\\\( A_{OP} \\\\) incorporates all downtimes over a mission period as \\\\( A_{OP} = 1 - \\\\left(\\\\frac{\\\\text{Mean Downtime}}{\\\\text{Mission Period}}\\\\right) \\\\). As time approaches infinity, limiting availability \\\\( A \\\\) converges to \\\\( A_{avg} \\\\). Additional metrics such as deliverability and on-stream availability evaluate operational performance, considering actual versus planned outputs, while the failure rate influences the Rate of Occurrence of Failures (ROCOF).', 'The availability \\\\( A(t) \\\\) of a repairable item at time \\\\( t \\\\) is the probability it is functioning, defined as \\\\( A(t) = P(X(t) = 1) \\\\), while unavailability is \\\\( U(t) = 1 - A(t) \\\\). Average availability, \\\\( A_{avg} \\\\), is calculated as \\\\( A_{avg} = \\\\frac{MUT}{MUT + MDT} \\\\), where \\\\( MUT \\\\) is Mean Up-Time and \\\\( MDT \\\\) is Mean Downtime. For instance, with a Mean Time to Failure (MTTF) of 1,000 hours and a MDT of 5 hours, \\\\( A_{avg} \\\\approx 0.995 \\\\), signifying roughly 44 hours of downtime annually. As time approaches infinity, the limiting availability \\\\( A \\\\) converges to \\\\( A_{avg} \\\\). Operational availability \\\\( A_{OP} \\\\) incorporates the Mean Downtime over a mission period. Additional metrics, such as deliverability and on-stream availability, further assess system performance against operational benchmarks, while the failure rate affects the Rate of Occurrence of Failures (ROCOF).'], ['The High-Performance Poisson Process (HPP) is characterized by independent interoccurrence times \\\\(T_1, T_2, \\\\ldots\\\\) that follow an exponential distribution with rate \\\\(\\\\lambda\\\\). The arrival times \\\\(S_n\\\\) are gamma distributed with parameters \\\\(n\\\\) and \\\\(\\\\lambda\\\\). The counting process \\\\(N(t)\\\\) adheres to properties such as \\\\(N(0) = 0\\\\), independent and stationary increments, and for a given interval \\\\(t\\\\), \\\\(P(N(t) = n) = \\\\frac{(\\\\lambda t)^n e^{-\\\\lambda t}}{n!}\\\\). The expected number of occurrences \\\\(E[N(t)] = \\\\lambda t\\\\), with variance \\\\(\\\\text{Var}(N(t)) = \\\\lambda t\\\\). As \\\\(t \\\\to \\\\infty\\\\), \\\\(N(t)\\\\) converges to a normal distribution: \\\\(\\\\frac{N(t) - \\\\lambda t}{\\\\sqrt{\\\\lambda t}} \\\\to N(0,1)\\\\). An unbiased estimator for \\\\(\\\\lambda\\\\) is \\\\(\\\\hat{\\\\lambda} = \\\\frac{N(t)}{t}\\\\) with variance \\\\(\\\\frac{\\\\lambda}{t}\\\\). Compound HPPs can be formed by summing rates of independent events.', \"**Understanding the High-Performance Poisson Process (HPP)**  \\nThe High-Performance Poisson Process (HPP) involves interoccurrence times \\\\(T_1, T_2, \\\\ldots\\\\) that are independent and follow an exponential distribution with rate \\\\(\\\\lambda\\\\). The arrival times \\\\(S_n\\\\) are gamma distributed with parameters \\\\(n\\\\) and \\\\(\\\\lambda\\\\). The quantity \\\\(N(t) = n\\\\) holds true when \\\\(S_n \\\\leq t < S_{n+1}\\\\). Using the law of total probability, we express the probability of \\\\(N(t) = n\\\\) as follows:  \\nThe probability equals the integral from 0 to \\\\(t\\\\) of the conditional probability that \\\\(T_{n+1}\\\\) exceeds \\\\(t - s\\\\) given \\\\(S_n = s\\\\), multiplied by the probability density of \\\\(S_n\\\\). This yields a formula relating \\\\(e^{-\\\\lambda(t-s)}\\\\) and factorial expressions, illustrating the HPP's characteristics.\"], ['In renewal theory, characterized by independent identically distributed intervals \\\\( T_1, T_2, \\\\ldots \\\\), the expected number of renewals by time \\\\( t \\\\), denoted \\\\( W(t) \\\\), satisfies the equation:  \\n\\\\[ W(t) = F_T(t) + \\\\int W(t - x) dF_T(x). \\\\]  \\nThe renewal density \\\\( w(t) \\\\) is given by \\\\( w(t) = \\\\lambda e^{-\\\\lambda t} \\\\frac{1 - e^{-2\\\\lambda t}}{2} \\\\), which approximates \\\\( \\\\frac{1}{\\\\mu} \\\\) as \\\\( t \\\\to \\\\infty \\\\), where \\\\( \\\\mu \\\\) is the mean. For Weibull-distributed renewal periods with shape parameter \\\\( \\\\alpha \\\\) and scale \\\\( \\\\lambda \\\\), the renewal function is expressed as:  \\n\\\\[ W(t) = \\\\sum_{k=1}^{\\\\infty} \\\\frac{(-1)^{k-1} A_k (\\\\lambda t)^k}{\\\\Gamma(k \\\\alpha + 1)}, \\\\]  \\nwhere coefficients \\\\( A_k \\\\) are recursively determined, and converges to \\\\( \\\\lambda t \\\\) for \\\\( \\\\alpha=1 \\\\). Additionally, \\\\( w(t) \\\\) can be derived via differentiation or Laplace transforms, illustrating the interconnections among mean, variance, and renewal dynamics. In a gamma distribution context, it is represented as \\\\( f_T(t) = \\\\lambda^2 t e^{-\\\\lambda t} \\\\) and \\\\( w(t) = \\\\lambda e^{-\\\\lambda t} \\\\).', '**Renewal Density and Function for Weibull Distributed Periods**  \\nThe renewal density \\\\( w(t) \\\\) is expressed as \\\\( w(t) = \\\\lambda e^{-\\\\lambda t} \\\\frac{1 - e^{-2\\\\lambda t}}{2} \\\\) and the renewal function \\\\( W(t) \\\\) is derived from \\\\( W(t) = \\\\int_0^t w(x) \\\\, dx \\\\), which evaluates to \\\\( W(t) = \\\\frac{\\\\lambda t}{2} - \\\\frac{(1 - e^{-2\\\\lambda t})}{4} \\\\). As \\\\( t \\\\) approaches infinity, both densities converge to fundamental ratios involving mean \\\\( \\\\mu \\\\). For Weibull-distributed renewal periods with shape parameter \\\\( \\\\alpha \\\\) and scale parameter \\\\( \\\\lambda \\\\), the renewal function can be recursively defined as an infinite series: \\\\( W(t) = \\\\sum_{k=1}^{\\\\infty} (-1)^{k-1} A_k (\\\\lambda t)^k \\\\frac{1}{\\\\Gamma(k \\\\alpha + 1)} \\\\). The coefficients \\\\( A_k \\\\) are recursively determined. For \\\\( \\\\alpha=1 \\\\), it simplifies to \\\\( W(t) = \\\\lambda t \\\\).', 'Renewal theory models the timing of replacements in stochastic processes involving independent, identically distributed intervals \\\\( T_1, T_2, \\\\ldots \\\\). The expected number of renewals by time \\\\( t \\\\), denoted \\\\( W(t) \\\\), satisfies the fundamental renewal equation:  \\n\\\\[ W(t) = F_T(t) + \\\\int W(t - x) dF_T(x). \\\\]  \\nAs \\\\( t \\\\) increases, \\\\( W(t) \\\\approx \\\\frac{t}{\\\\mu} \\\\), where \\\\( \\\\mu \\\\) is the average renewal length. The renewal density \\\\( w(t) \\\\) approaches \\\\( \\\\frac{1}{\\\\mu} \\\\) over time and can be derived through differentiation, integration, or Laplace transforms. Blackwell’s theorem refines estimates, indicating that for small intervals \\\\( (t, t + u] \\\\), the mean number of renewals approximates \\\\( \\\\frac{u}{\\\\mu} \\\\). The nth arrival time is \\\\( S_n = T_1 + T_2 + \\\\ldots + T_n \\\\), where \\\\( S_n \\\\to \\\\mu n \\\\) as \\\\( n \\\\) grows, supporting results from the strong law of large numbers and the central limit theorem. In specific distributions like gamma or Weibull, the renewal function exhibits unique forms, impacting reliability metrics and availability.'], ['Poisson Distribution\\nThe **Poisson distribution** models the probability of a given number of events occurring within a fixed interval of time or space, assuming these events occur with a constant mean rate and independently of the time since the last event.\\n\\n**Key Formula**  \\nIf X is a random variable following a Poisson distribution, then:  \\nP(X = k) = (?^k * e^(-?)) / k!  \\nwhere:  \\n- ? = the average number of events (mean of the distribution).  \\n- k = the number of events of interest (must be a non-negative integer).  \\n\\n**Key Properties**  \\n1. Mean: E[X] = ?  \\n2. Variance: Var(X) = ?  \\n\\n**Cumulative Probability**  \\nTo calculate P(X < k), sum the probabilities for all values of k less than the threshold:  \\nP(X < k) = ? (?^i * e^(-?)) / i! for i = 0 to k-1  \\n\\n**Steps to Solve Poisson Problems**  \\n1. Identify Parameters: Determine ? (mean number of events).  \\n2. Plug into the Formula:  \\n   - For exact probabilities (P(X = k)): Use the formula directly.  \\n   - For cumulative probabilities (P(X ? k) or P(X < k)): Sum the probabilities for the desired range.  \\n3. Use Tables or Calculators: To save time, use cumulative probability tables or statistical calculators for larger values of k.  '], ['Reliability Apportionment for Series Components\\n**Key Formula:**  \\nFor a series system: \\\\( R_{total} = R_1 \\\\cdot R_2 \\\\cdot R_3 \\\\cdot R_4 \\\\cdot R_5 \\\\)  \\nWhere \\\\( R_{total} \\\\) is the total system reliability and \\\\( R_i \\\\) are individual component reliabilities.\\n\\n**Steps to Solve:**  \\n1. Compute \\\\( R_{remaining} = R_{total} / (R_1 \\\\cdot R_2 \\\\cdot R_3 \\\\cdot R_4) \\\\).'], ['Normal Distribution and \\\\( B_{10} \\\\) Life\\n**Key Concept:**  \\nThe \\\\( B_{10} \\\\) life represents the time at which 10% of items have failed.\\n\\n**Key Formula:**  \\n\\\\[ B_{10} = \\\\mu + Z_{0.1} \\\\cdot \\\\sigma \\\\]  \\nWhere:  \\n- \\\\( \\\\mu \\\\) = mean, \\\\( \\\\sigma \\\\) = standard deviation, \\\\( Z_{0.1} \\\\) = standard score for the 10th percentile.\\n\\n**Steps to Solve:**  \\n1. Calculate \\\\( \\\\sigma = \\\\sqrt{\\\\text{variance}} \\\\).  \\n2. Use python to compute \\\\( Z_{0.1} \\\\) (for 10th percentile in standard normal distribution).  \\n3. Plug values into the formula to find \\\\( B_{10} \\\\).'], [\"Reliability Engineering:\\n- Focuses on the performance and consistency of a product over time.\\n- Concerned with how well the product performs throughout its life cycle, especially under varying conditions.\\nMeasures:\\n- Failure rate over time: Probability of failure occurring at any point during the product's operational life.\\n- Product design: Involves designing products for durability and minimal failure under intended use.\"], [\"Consequences of Attributing Failure to Human Error\\nHuman Error Assumption in Failure Analysis:\\n\\nWhen management assumes that human error is the main cause of a failure, it often leads to surface-level solutions that don't address underlying systemic issues.\\nThis approach typically focuses on individual blame, without considering potential flaws in systems, processes, training, or design.\\nConsequences of Assuming Human Error as the Main Root Cause:\\nRepeat of the Same Failure:\\n\\nRoot cause not addressed: If human error is seen as the main cause, the systemic or procedural factors that contribute to the error may go unaddressed. This can result in the same failure occurring again.\\nReactive solutions: Focusing on individual mistakes may lead to ineffective solutions, which don't prevent the recurrence of the issue.\\nDisciplinary Action for the Employee:\\n\\nBlaming the individual: Management may respond to human error by punishing or reprimanding employees, rather than investigating broader organizational issues (e.g., inadequate training, unclear procedures).\\nDecreased morale: This approach can lead to low employee morale and a culture of fear rather than one focused on continuous improvement.\\nLess Productivity:\\n\\nFear of blame: When employees fear being blamed for errors, they may be less likely to take initiative or report issues, leading to reduced productivity.\\nStress and disengagement: A focus on punishment for errors can lead to stress and disengagement among employees, further decreasing overall productivity.\\nCultural Impact:\\n\\nLack of continuous improvement: Focusing on individual blame rather than addressing process or system flaws reduces the opportunity for learning and improvement.\\nBlame culture: Encourages a culture where employees are afraid to speak up, which stifles innovation and continuous improvement.\"], ['**Exponential Distribution**\\n\\nThe **Exponential distribution** models the time between events in a process where events occur continuously and independently at a constant average rate. It is often used to model the time between failures or arrival times in queuing systems.\\n\\n### **Key Formula**  \\nIf \\\\( X \\\\) is a random variable following an Exponential distribution, then:  \\n\\\\[\\nP(X = x) = \\\\lambda e^{-\\\\lambda x}\\n\\\\]\\nwhere:  \\n- \\\\( \\\\lambda \\\\) = the rate parameter (inverse of the mean, i.e., \\\\( \\\\lambda = \\\\frac{1}{\\\\mu} \\\\)).  \\n- \\\\( x \\\\) = the time between events (must be a non-negative value, \\\\( x \\\\geq 0 \\\\)).\\n\\n### **Key Properties**  \\n1. **Mean**:  \\n\\\\[\\nE[X] = \\\\frac{1}{\\\\lambda}\\n\\\\]  \\n2. **Variance**:  \\n\\\\[\\n\\\\text{Var}(X) = \\\\frac{1}{\\\\lambda^2}\\n\\\\]  \\n3. **Memoryless Property**: The probability of an event occurring in the next \\\\( t \\\\) time units is independent of how much time has already passed.  \\n   \\\\[\\n   P(X > t + s \\\\mid X > t) = P(X > s)\\n   \\\\]  \\n   This means the distribution \"forgets\" the past and has no memory of previous events.\\n\\n### **Cumulative Distribution Function (CDF)**  \\nTo calculate the probability that the event occurs before a certain time \\\\( x \\\\), use the CDF:  \\n\\\\[\\nP(X \\\\leq x) = 1 - e^{-\\\\lambda x}\\n\\\\]  \\nThis represents the probability that the time between events is less than or equal to \\\\( x \\\\).\\n\\n### **Survival Function**  \\nThe survival function represents the probability that the event occurs after time \\\\( x \\\\):  \\n\\\\[\\nP(X > x) = e^{-\\\\lambda x}\\n\\\\]\\n\\n### **Steps to Solve Exponential Distribution Problems**  \\n1. **Identify Parameters**: Determine \\\\( \\\\lambda \\\\), the rate parameter (often given or derived from the mean \\\\( \\\\mu \\\\)).  \\n   - \\\\( \\\\lambda = \\\\frac{1}{\\\\mu} \\\\), where \\\\( \\\\mu \\\\) is the mean time between events.  \\n2. **Calculate Probabilities**:  \\n   - For exact probabilities \\\\( P(X = x) \\\\): Use the probability density function (PDF) formula.  \\n   - For cumulative probabilities \\\\( P(X \\\\leq x) \\\\): Use the CDF formula.  \\n   - For survival probabilities \\\\( P(X > x) \\\\): Use the survival function formula.  \\n3. **Use Tables or Calculators**:  \\n   - For quick calculations, use statistical tables or calculators that provide the cumulative distribution function or survival function values.'], ['Taguchi DOE Approach\\nThe **Taguchi Design of Experiments (DOE)** approach includes:  \\n1. Loss function concept.  \\n2. Variability reduction to meet target values.  \\n3. Continuous loss functions (not step functions).'], ['Confidence Interval for Population Mean\\n**Key Formula:**  \\nFor a 95% confidence interval (CI):  \\n\\\\[ \\\\text{CI} = \\\\bar{x} \\\\pm Z_{0.025} \\\\cdot \\\\frac{s}{\\\\sqrt{n}} \\\\]  \\nWhere:  \\n- \\\\( \\\\bar{x} \\\\) = sample mean, \\\\( s \\\\) = sample standard deviation, \\\\( n \\\\) = sample size.\\n\\n**Steps to Solve:**  \\n1. Find \\\\( Z_{0.025} \\\\). Using python for example\\n2. Compute \\\\( \\\\frac{s}{\\\\sqrt{n}} \\\\).  \\n3. Add/subtract from \\\\( \\\\bar{x} \\\\).'], [\"Conditional Probabilities\\n\\n### **Understanding the Context**\\n\\nIn diagnostic problems (e.g., medical tests, quality control), we often deal with terms like:\\n- **True Positive (TP):** The test correctly identifies the presence of a condition (e.g., crack detected when a crack exists).\\n- **False Positive (FP):** The test incorrectly signals the presence of a condition (e.g., crack detected when there�s no crack).\\n- **True Negative (TN):** The test correctly identifies the absence of a condition (e.g., no crack detected when there�s no crack).\\n- **False Negative (FN):** The test fails to identify the presence of a condition (e.g., no crack detected when a crack exists).\\n\\nIn such scenarios:\\n1. **Sensitivity (True Positive Rate):** \\\\( P(B|A) \\\\), the probability the test signals a crack when a crack is present.\\n2. **False Positive Rate:** \\\\( P(B|\\\\neg A) \\\\), the probability the test signals a crack when no crack is present.\\n3. **Prevalence:** \\\\( P(A) \\\\), the proportion of cases where the condition (crack) exists in the population.\\n\\n---\\n\\n### **When to Use Each Method**\\n\\n#### **1. Conditional Probability**\\nUse this to calculate the probability of one event occurring given that another event has already occurred.  \\nFor instance, you might calculate:\\n\\\\[\\nP(\\\\text{crack present | test signals a crack}) = \\\\frac{P(\\\\text{test signals a crack AND crack present})}{P(\\\\text{test signals a crack})}\\n\\\\]\\nThis is useful for understanding how reliable a test is when it gives a positive result.\\n\\n---\\n\\n#### **2. Total Probability Rule**\\nUse this when you want to find the overall probability of an event (\\\\( P(B) \\\\)), accounting for all possible causes.  \\nIn the problem:\\n\\\\[\\nP(\\\\text{test signals a crack}) = P(\\\\text{test signals a crack | crack present}) \\\\cdot P(\\\\text{crack present}) + P(\\\\text{test signals a crack | no crack}) \\\\cdot P(\\\\text{no crack})\\n\\\\]\\nThis helps aggregate the probabilities of a signal coming from both true and false scenarios.\\n\\n---\\n\\n#### **3. Bayes' Theorem**\\nUse this to update the probability of an event based on new evidence.  \\nFor example, if a part signals a crack, Bayes� theorem helps calculate:\\n\\\\[\\nP(\\\\text{crack present | test signals a crack}) = \\\\frac{P(\\\\text{test signals a crack | crack present}) \\\\cdot P(\\\\text{crack present})}{P(\\\\text{test signals a crack})}\\n\\\\]\\nThis is especially important when the prior probability of the condition (\\\\( P(A) \\\\)) is low, as it adjusts for the rarity of the event and the accuracy of the test.\\n\\n---\\n\\n### **Application in the Problem**\\n1. Use the **Total Probability Rule** to find \\\\( P(B) \\\\), the probability that the test signals a crack (true or false).\\n2. Use **Bayes� Theorem** to find \\\\( P(A|B) \\\\), the probability that the part actually has a crack given a positive test result.\\n\\nThis process helps determine how reliable the test result is when it indicates a problem, balancing the high sensitivity against the false positive rate.\\n\\n---\\n\\n### Final Insight\\nThis framework is applicable in fields like medical diagnostics, fraud detection, and quality control, where tests or algorithms produce signals, and we need to evaluate their real-world implications. Use these methods whenever you encounter probabilities of detection, errors, or prior information that influence a decision.\\n\"], [\"## **Confidence Levels and Reliability**\\n\\n### **1. Key Definitions**\\n- **Reliability (\\\\( R \\\\)):** Probability that a system performs its function without failure over a specified time or conditions.\\n- **Confidence Level (\\\\( CL \\\\)):** Probability that the true reliability \\\\( R \\\\) of a system is at least the specified value, based on test results.\\n\\n### **2. Binomial Testing Basics**\\nReliability and confidence levels are often assessed using a binomial distribution in pass/fail tests:\\n- \\\\( n \\\\): Total number of tests performed.\\n- \\\\( x \\\\): Number of successful tests (no failures).\\n\\n---\\n\\n### **3. Reliability Test Formula**\\nFor a given reliability \\\\( R \\\\) and confidence level \\\\( CL \\\\), the inequality to check is:\\n\\\\[\\nCL \\\\geq 1 - (1 - R)^n\\n\\\\]\\n- Rearrange to compute \\\\( n \\\\), the minimum number of tests required for a given \\\\( R \\\\) and \\\\( CL \\\\):\\n\\\\[\\nn \\\\geq \\\\frac{\\\\log(1 - CL)}{\\\\log(1 - R)}\\n\\\\]\\n\\nAlternatively, rearrange to check whether the result meets the customer's requirements:\\n\\\\[\\nCL = 1 - (1 - R)^n\\n\\\\]\"], ['### **Burn-In Testing**\\n\\n### **1. What is Burn-In Testing?**\\n- **Definition:** A process where electronic assemblies or systems are operated under specified conditions (e.g., stress, elevated temperature) for a predetermined time before being placed in service.\\n- **Purpose:** To detect and eliminate early-life failures (also known as \"infant mortality failures\") that occur due to manufacturing defects or material flaws.\\n\\n---\\n\\n### **2. Key Objectives of Burn-In Testing**\\n- **Minimize Early Life Failures:** \\n  - Identify and remove weak units that would fail soon after deployment.\\n  - Ensures higher reliability for units that pass the test.\\n- **Do Not Extend Product Life:** \\n  - Burn-in testing does not change the intrinsic reliability of a properly functioning product.\\n- **Establish System Failure Rate:** \\n  - Useful for estimating the reliability parameters for a population of systems.\\n- **Prevent Catastrophic Failures in the Field:** \\n  - Focused on ensuring quality for high-stakes applications (e.g., aerospace, medical devices).\\n\\n---\\n\\n### **3. When to Use Burn-In Testing?**\\n- **Products with High Cost of Failure:** Electronics, critical systems, or high-reliability requirements.\\n- **New Product Lines:** To ensure manufacturing processes are reliable.\\n- **High Failure Variability:** If early-life failures are a significant proportion of total failures.\\n\\n---\\n\\n### **4. Key Benefits of Burn-In Testing**\\n- Improves reliability and customer satisfaction by removing early failures.\\n- Reduces warranty and repair costs.\\n- Provides data to refine manufacturing processes.'], ['Reliability tests:\\n\\n1. The **Arrhenius** model is typically used for thermal stress and describes the effect of temperature on failure rates through an exponential relationship.  \\n2. The **Coffin-Manson** model is applied for fatigue failure under mechanical or thermal cycling, focusing on strain or stress cycles.  \\n3. The **Inverse Power Law** model relates failure rates to a single stressor, like voltage or pressure, often in an accelerated life testing scenario.  \\n4. The **Eyring** model, unlike the others, can incorporate multiple stress types (e.g., temperature and humidity) simultaneously, making it the exception.'], ['**Weibull Distribution**  \\nThe **Weibull distribution** is often used in reliability analysis to model the life data of products, particularly when the failure rate is not constant over time.\\n\\n**Key Formula**  \\nThe probability density function (PDF) for the Weibull distribution is:  \\n\\\\[\\nf(x; \\\\lambda, k) = \\\\frac{k}{\\\\lambda} \\\\left( \\\\frac{x}{\\\\lambda} \\\\right)^{k-1} e^{-(x/\\\\lambda)^k}\\n\\\\]  \\nwhere:  \\n- \\\\( \\\\lambda \\\\) = scale parameter (characteristic life).  \\n- \\\\( k \\\\) = shape parameter (controls the failure rate behavior).  \\n\\n**Key Properties**  \\n1. Mean: \\\\( E[X] = \\\\lambda \\\\Gamma \\\\left( 1 + \\\\frac{1}{k} \\\\right) \\\\), where \\\\( \\\\Gamma \\\\) is the Gamma function.  \\n2. Variance: \\\\( Var(X) = \\\\lambda^2 \\\\left( \\\\Gamma \\\\left( 1 + \\\\frac{2}{k} \\\\right) - \\\\left( \\\\Gamma \\\\left( 1 + \\\\frac{1}{k} \\\\right) \\\\right)^2 \\\\right) \\\\)\\n\\n**Common Use Cases**  \\n- **\\\\( k = 1 \\\\)**: Exponential distribution (constant failure rate).  \\n- **\\\\( k < 1 \\\\)**: Decreasing failure rate (early failures).  \\n- **\\\\( k > 1 \\\\)**: Increasing failure rate (wear-out failures).\\n\\nFor parametric estimation of the parameters, use Python to compute them.\\nUse python to compute number of defaults after a given period of time and the CDF of the law.'], ['**Normal Distribution**  \\nThe **Normal distribution** is widely used for modeling symmetrical data, particularly for failure times when the data is not heavily skewed.\\n\\n**Key Formula**  \\nThe probability density function (PDF) for the Normal distribution is:  \\n\\\\[\\nf(x; \\\\mu, \\\\sigma^2) = \\\\frac{1}{\\\\sigma \\\\sqrt{2\\\\pi}} e^{-\\\\frac{(x-\\\\mu)^2}{2\\\\sigma^2}}\\n\\\\]  \\nwhere:  \\n- \\\\( \\\\mu \\\\) = mean (expected value).  \\n- \\\\( \\\\sigma^2 \\\\) = variance.  \\n\\n**Key Properties**  \\n1. Mean: \\\\( E[X] = \\\\mu \\\\)  \\n2. Variance: \\\\( Var(X) = \\\\sigma^2 \\\\)\\n\\n**Common Use Cases**  \\n- When failure times are symmetrically distributed around a central value (i.e., product life or performance).'], ['**Log-Normal Distribution**  \\nThe **Log-Normal distribution** is often used in reliability engineering when the logarithm of the variable follows a normal distribution. This is useful when modeling life data with a positive skew.\\n\\n**Key Formula**  \\nThe probability density function (PDF) for the Log-Normal distribution is:  \\n\\\\[\\nf(x; \\\\mu, \\\\sigma^2) = \\\\frac{1}{x \\\\sigma \\\\sqrt{2\\\\pi}} e^{-\\\\frac{(\\\\ln x - \\\\mu)^2}{2 \\\\sigma^2}}\\n\\\\]  \\nwhere:  \\n- \\\\( \\\\mu \\\\) = mean of the logarithmic values.  \\n- \\\\( \\\\sigma^2 \\\\) = variance of the logarithmic values.\\n\\n**Key Properties**  \\n1. Mean: \\\\( E[X] = e^{\\\\mu + \\\\frac{\\\\sigma^2}{2}} \\\\)  \\n2. Variance: \\\\( Var(X) = e^{2\\\\mu + \\\\sigma^2} \\\\left( e^{\\\\sigma^2} - 1 \\\\right) \\\\)'], ['**Failure Rate (Hazard Function)**  \\nThe **failure rate**, or **hazard function**, is crucial in reliability engineering, as it represents the instantaneous rate of failure at a given time.\\n\\n**Formula**  \\nThe hazard function \\\\( h(t) \\\\) is defined as:  \\n\\\\[\\nh(t) = \\\\frac{f(t)}{S(t)}  \\n\\\\]  \\nwhere:  \\n- \\\\( f(t) \\\\) is the probability density function (PDF).  \\n- \\\\( S(t) \\\\) is the survival function: \\\\( S(t) = 1 - F(t) \\\\), where \\\\( F(t) \\\\) is the cumulative distribution function (CDF).\\n\\n**Interpretation**  \\n- **Constant hazard rate** implies exponential distribution.  \\n- **Increasing hazard rate** implies wear-out failure mode (e.g., Weibull with \\\\( k > 1 \\\\)).'], ['**Mean Time Between Failures (MTBF)**  \\n**MTBF** is an important reliability metric representing the average time between two consecutive failures in a system.\\n\\n**Formula**  \\n\\\\[\\nMTBF = \\\\frac{\\\\text{Total operating time}}{\\\\text{Number of failures}}\\n\\\\]'], ['**Bath Tub Curve**  \\nThe **Bath Tub Curve** is a graphical representation of the failure rate over time, showing three distinct phases:\\n- **Infant mortality**: High failure rate at the start (early life failures).\\n- **Normal life**: Constant failure rate (random failures).\\n- **Wear-out**: Increasing failure rate at the end (late life failures).\\n\\nThis curve is often modeled using the **Weibull distribution** with \\\\( k < 1 \\\\) for early failures and \\\\( k > 1 \\\\) for wear-out failures.'], ['**weibull**\\nThe Weibull distribution is a fundamental tool in reliability engineering, widely used to model time-to-failure data because it is very flexible and it can be adapted to many situations thanks to the parameters. Its probability density function is f(t) = (β/η)(t/η)^(β-1)e^(-(t/η)^β), where β is the shape parameter and η is the scale parameter. The cumulative distribution function is F(t) = 1 - e^(-(t/η)^β), and the survivor function is R(t) = e^(-(t/η)^β). The failure rate function is z(t) = (β/η)(t/η)^(β-1), highlighting failure rate dynamics. The mean time to failure (MTTF) is ηΓ(1 + 1/β), where Γ is the gamma function. The conditional survivor function is R(t | t₀) = e^(-(t/η)^β) / e^(-(t₀/η)^β), describing survival probability beyond t given survival up to t₀. The mean residual lifetime (MRL) is MRL(t) = ηΓ(1 + 1/β, (t/η)^β) / R(t), where Γ is the upper incomplete gamma function.', 'The time-to-failure \\\\( T \\\\) is modeled by a Weibull distribution \\\\( \\\\text{Weibull}(\\\\alpha, \\\\theta) \\\\), leading to the survivor function \\\\( R(t) = P(T > t) = e^{-\\\\left( \\\\frac{t}{\\\\theta} \\\\right)^\\\\alpha} \\\\). For \\\\( T^\\\\alpha \\\\), we find \\\\( P(T^\\\\alpha > t) = e^{-\\\\frac{t}{\\\\theta^\\\\alpha}} \\\\), indicating it is exponentially distributed with failure rate \\\\( \\\\lambda = \\\\frac{1}{\\\\theta^\\\\alpha} \\\\). In a series of \\\\( n \\\\) components, \\\\( R_s(t) = e^{-\\\\sum_{i=1}^{n} \\\\left( \\\\frac{t}{\\\\theta_i} \\\\right)^\\\\alpha} \\\\). For identical components, the equivalent scale parameter is \\\\( \\\\theta_s = \\\\frac{\\\\theta}{n^{1/\\\\alpha}} \\\\), resulting in \\\\( MTTF_s = \\\\frac{\\\\theta \\\\Gamma(1 + \\\\frac{1}{\\\\alpha})}{n^{1/\\\\alpha}} \\\\). The three-parameter model includes a threshold \\\\( \\\\xi \\\\), yielding \\\\( MTTF = \\\\xi + \\\\theta \\\\Gamma(1 + \\\\frac{1}{\\\\alpha}) \\\\). The failure rate function is \\\\( z(t) = \\\\frac{\\\\alpha}{\\\\theta} \\\\left( \\\\frac{t}{\\\\theta} \\\\right)^{\\\\alpha - 1} \\\\). The exponential distribution, characterized by \\\\( f(t) = \\\\lambda e^{-\\\\lambda t} \\\\) and \\\\( R(t) = e^{-\\\\lambda t} \\\\), exhibits memoryless properties, highlighting its significance in reliability analysis.', \"The Weibull distribution is characterized by its probability density function:\\n\\n\\\\[\\nf(t) = \\\\frac{\\\\alpha}{\\\\theta} \\\\left(\\\\frac{t}{\\\\theta}\\\\right)^{\\\\alpha - 1} e^{-\\\\left(\\\\frac{t}{\\\\theta}\\\\right)^\\\\alpha} \\\\quad (t > 0)\\n\\\\]\\n\\nwith scale parameter \\\\(\\\\theta\\\\) and shape parameter \\\\(\\\\alpha\\\\). The survivor function is given by:\\n\\n\\\\[\\nR(t) = e^{-\\\\left(\\\\frac{t}{\\\\theta}\\\\right)^\\\\alpha} \\\\quad (t > 0)\\n\\\\]\\n\\nThe failure rate function is:\\n\\n\\\\[\\nz(t) = \\\\frac{\\\\alpha}{\\\\theta} \\\\left(\\\\frac{t}{\\\\theta}\\\\right)^{\\\\alpha - 1} \\\\quad (t > 0)\\n\\\\]\\n\\nwhich reveals characteristics: if \\\\(\\\\alpha < 1\\\\), \\\\(z(t)\\\\) decreases; if \\\\(\\\\alpha = 1\\\\), it's constant; if \\\\(\\\\alpha > 1\\\\), it increases. The Mean Time to Failure (MTTF) and median are:\\n\\n\\\\[\\n\\\\text{MTTF} = \\\\theta \\\\Gamma(1 + \\\\frac{1}{\\\\alpha}), \\\\quad t_m = \\\\theta (\\\\log 2)^{1/\\\\alpha}\\n\\\\]\\n\\nIn series systems of \\\\(n\\\\) components, the survivor function modifies to:\\n\\n\\\\[\\nR_s(t) = e^{-\\\\sum_{i=1}^{n} \\\\left(\\\\frac{t}{\\\\theta_i}\\\\right)^{\\\\alpha}}, \\\\quad \\\\text{MTTF}_{s} = \\\\frac{\\\\theta \\\\Gamma(1 + \\\\frac{1}{\\\\alpha})}{n^{1/\\\\alpha}}\\n\\\\]\\n\\nFor a three-parameter Weibull distribution, the mean and variance become:\\n\\n\\\\[\\n\\\\text{MTTF} = \\\\xi + \\\\theta \\\\Gamma(1 + \\\\frac{1}{\\\\alpha}), \\\\quad \\\\text{Var}(T) = \\\\theta^2 \\\\left[\\\\Gamma\\\\left(1 + \\\\frac{2}{\\\\alpha}\\\\right) - \\\\Gamma^2\\\\left(1 + \\\\frac{1}{\\\\alpha}\\\\right)\\\\right]\\n\\\\]\\n\\nThis distribution effectively models varied time-to-failure scenarios, emphasizing reliability analysis.\"], ['The failure rate function \\\\( z(t) \\\\) for a gamma distribution with shape parameter \\\\( \\\\alpha \\\\) and scale \\\\( \\\\lambda = 1 \\\\) diverges (\\\\( z(t) \\\\to \\\\infty \\\\)) as \\\\( t \\\\to 0 \\\\) when \\\\( 0 < \\\\alpha < 1 \\\\) and approaches zero (\\\\( z(t) \\\\to 0 \\\\)) for \\\\( \\\\alpha > 1 \\\\), exhibiting a discontinuity at \\\\( \\\\alpha = 1 \\\\). If \\\\( T_1 \\\\sim \\\\Gamma(\\\\alpha_1, \\\\lambda) \\\\) and \\\\( T_2 \\\\sim \\\\Gamma(\\\\alpha_2, \\\\lambda) \\\\) are independent, then \\\\( T_1 + T_2 \\\\sim \\\\Gamma(\\\\alpha_1 + \\\\alpha_2, \\\\lambda) \\\\). Special cases include the exponential distribution (\\\\( \\\\alpha = 1 \\\\)), chi-square distribution (\\\\( \\\\alpha = n/2 \\\\), \\\\( \\\\lambda = 1/2 \\\\)), and Erlangian distribution (integer \\\\( \\\\alpha \\\\)). The gamma PDF is given by \\\\( f(t) = \\\\frac{\\\\lambda^k t^{k-1} e^{-\\\\lambda t}}{\\\\Gamma(k)} \\\\), where mean time to failure \\\\( \\\\text{MTTF} = \\\\frac{k}{\\\\lambda} \\\\) and variance is \\\\( \\\\text{var}(T) = \\\\frac{k}{\\\\lambda^2} \\\\). \\n\\nThe implementation includes functions for failure and survivor rates:\\n```python\\ndef gamma_pdf(t, alpha, lambda_):\\n    return (lambda_ * (lambda_ * t)**(alpha - 1) * np.exp(-lambda_ * t)) / gamma(alpha)\\ndef survival_function(t, alpha, lambda_):\\n    return 1 - gamma.cdf(t, a=alpha, scale=1/lambda_)\\ndef failure_rate_function(t, alpha, lambda_):\\n    return gamma_pdf(t, alpha, lambda_) / survival_function(t, alpha, lambda_)\\n```', 'Given independent gamma variables \\\\( T_1 \\\\sim \\\\Gamma(\\\\alpha_1, \\\\lambda) \\\\) and \\\\( T_2 \\\\sim \\\\Gamma(\\\\alpha_2, \\\\lambda) \\\\), their sum \\\\( T_1 + T_2 \\\\sim \\\\Gamma(\\\\alpha_1 + \\\\alpha_2, \\\\lambda) \\\\). Special cases include \\\\( \\\\alpha = 1 \\\\) (exponential distribution with rate \\\\( \\\\lambda \\\\)) and \\\\( \\\\alpha = n/2, \\\\lambda = 1/2 \\\\) (chi-square distribution \\\\( \\\\chi^2(n) \\\\)). The chi-square PDF is:\\n\\n\\\\[\\nf_n(x) = \\\\frac{1}{\\\\Gamma(n/2) \\\\cdot 2^{n/2}} x^{n/2 - 1} e^{-x/2}\\n\\\\]\\n\\nThe gamma PDF is:\\n\\n```python\\ndef gamma_pdf(t, alpha, lambda_):\\n    return (lambda_ (lambda_ t)^{\\\\alpha - 1} e^{-\\\\lambda t}) / \\\\Gamma(\\\\alpha)\\n```\\n\\nwith mean \\\\( \\\\text{MTTF} = \\\\frac{\\\\alpha}{\\\\lambda} \\\\) and variance \\\\( \\\\text{Var}(T) = \\\\frac{\\\\alpha}{\\\\lambda^2} \\\\). The failure rate function \\\\( z(t) \\\\) diverges as \\\\( t \\\\to 0 \\\\) for \\\\( 0 < \\\\alpha < 1 \\\\) and approaches zero for \\\\( \\\\alpha > 1 \\\\). Implementations for the survival function and failure rates facilitate further analysis:\\n\\n```python\\ndef survival_function(t, alpha, lambda_):\\n    return 1 - gamma.cdf(t, a=alpha, scale=1/lambda_)\\ndef failure_rate_function(t, alpha, lambda_):\\n    return gamma_pdf(t, alpha, lambda_) / survival_function(t, alpha, lambda_)\\n``` \\n\\nThis models time-to-failure effectively, especially as \\\\( \\\\alpha \\\\) approaches 1.'], ['The reliability of systems is influenced by their structure, with critical differences in series and parallel configurations. In series structures, the component with the lowest reliability is vital to overall system reliability. Reliability is calculated as \\\\( h(p) = p_1 p_2 \\\\cdots p_n \\\\), and the importance of component \\\\( i \\\\) is determined by \\\\( I_B(i) = \\\\prod_{j \\\\neq i} p_j \\\\). Conversely, in parallel structures, the component with the highest reliability is most significant, where system reliability is given by \\\\( h(p) = 1 - \\\\prod_{j}(1 - p_j) \\\\) and importance by \\\\( I_B(i) = \\\\prod_{j \\\\neq i}(1 - p_j) \\\\). Improvement potential \\\\( I_{IP}(i|t) = h[1_i, p(t)] - h[p(t)] \\\\) quantifies the impact of replacing component \\\\( i \\\\) with a perfect one. Recognizing weak or strong components aids in maintenance and reliability optimization strategies.', \"In reliability analysis, the structure of independent components significantly influences system reliability metrics. For a series structure with reliabilities \\\\( p_1, p_2, \\\\ldots, p_n \\\\), system reliability is the product \\\\( h(p) = p_1 p_2 \\\\cdots p_n\\\\), and the importance of component \\\\( i \\\\) is described by Birnbaum's metric: \\\\( I_B(i) = \\\\prod_{j \\\\neq i} p_j \\\\). In contrast, for parallel structures, the reliability is given by \\\\( h(p) = 1 - (1 - p_1)(1 - p_2) \\\\cdots (1 - p_n) \\\\), with the importance metric defined as \\\\( I_B(i) = \\\\prod_{j \\\\neq i} (1 - p_j) \\\\). When component reliabilities equal \\\\( \\\\frac{1}{2} \\\\), \\\\( I_B(i) \\\\) aligns with structural importance \\\\( I_\\\\phi(i) \\\\). Improvement potential \\\\( I_{IP}(i|t) \\\\) measures the increase in reliability when replacing component \\\\( i \\\\) with a perfect one, given by \\\\( I_{IP}(i|t) = h[1_i, p(t)] - h[p(t)] \\\\). Barlow-Proschan’s metric evaluates the contribution of individual components to system failures but is more complex in parallel structures. Recognizing weak or strong components helps inform maintenance and reliability strategies.\"], ['This section discusses reliability models for systems with identical components, particularly focusing on common cause failures (CCFs) in a 2-out-of-3 Good (2oo3:G) system. The beta-factor model assesses failure rates using individual failures at \\\\( 3(1 - \\\\beta) \\\\lambda t \\\\) and CCFs at \\\\( \\\\beta \\\\lambda t \\\\). Reliability for \\\\( n \\\\) identical components is expressed as \\\\( R_S(t) = e^{-n(1 - \\\\beta)\\\\lambda t} \\\\cdot e^{-\\\\beta \\\\lambda t} \\\\), with Mean Time to Failure (MTTF) defined as \\\\( MTTF = \\\\frac{1}{n\\\\lambda(n - (n-1)\\\\beta)} \\\\). Failure probabilities for three components are \\\\( Q_1 = (1 - \\\\beta)Q \\\\), \\\\( Q_2 = \\\\beta(1 - \\\\gamma)Q \\\\), and \\\\( Q_3 = \\\\beta \\\\gamma Q \\\\). As \\\\( \\\\beta \\\\) approaches 1, MTTF and reliability improve, but complexity increases for nonidentical components. Alternate models like the alpha-factor and multiple beta-factor models provide additional framework for reliability assessments, especially in aerospace and nuclear applications.', 'This section discusses reliability models for assessing failure rates in parallel structures, particularly under the beta-factor model, which distinguishes between individual failures and common cause failures (CCFs) over a time interval \\\\( t \\\\). Individual failures are represented as \\\\( 3(1 - \\\\beta) \\\\lambda t \\\\) and CCFs as \\\\( \\\\beta \\\\lambda t \\\\). The overall reliability function for \\\\( n \\\\) identical components is expressed as \\\\( R_S(t) = e^{-n(1 - \\\\beta)\\\\lambda t} e^{-\\\\beta \\\\lambda t} \\\\), and the Mean Time to Failure (MTTF) is given by \\\\( MTTF = \\\\frac{1}{n\\\\lambda(n - (n-1)\\\\beta)} \\\\), indicating that reliability increases with higher \\\\( \\\\beta \\\\). For a 2-out-of-3 Good (2oo3:G) system, MTTF trends reveal maximum reliability at \\\\( \\\\beta = 1 \\\\). While the model is applicable to nonidentical components, calculating \\\\( \\\\beta \\\\) becomes complex as it may rely on geometric averages of individual failure rates. Alternative models, including the C-factor model, further refine CCF analysis, crucial in aerospace and nuclear applications.', 'This section discusses the beta-factor model for assessing failure rates in parallel structures involving both identical and nonidentical components. The model defines two failure types over a time interval \\\\( t \\\\): individual failures at \\\\( 3(1 - \\\\beta) \\\\lambda t \\\\) and common cause failures (CCFs) at \\\\( \\\\beta \\\\lambda t \\\\). The overall reliability function for \\\\( n \\\\) identical components is given by \\\\( R_S(t) = e^{-n(1 - \\\\beta)\\\\lambda t} \\\\cdot e^{-\\\\beta \\\\lambda t} \\\\), with mean time-to-failure (MTTF) adjusted as \\\\( MTTF = \\\\frac{1}{n\\\\lambda(n - (n-1)\\\\beta)} \\\\), illustrating that increased \\\\( \\\\beta \\\\) enhances reliability. For systems with three identical components, failure probabilities can be articulated as \\\\( Q_1 = (1 - \\\\beta)Q \\\\), \\\\( Q_2 = \\\\beta(1 - \\\\gamma)Q \\\\), and \\\\( Q_3 = \\\\beta \\\\gamma Q \\\\). While applicable to nonidentical components, caution is necessary due to increased complexity in defining \\\\( \\\\beta \\\\), often derived from the geometric average of failure rates. The development of alternative models, including the alpha-factor and multiple beta-factor models, aids in refining assessments, particularly in aerospace and nuclear contexts.'], ['In a non-homogeneous Poisson process (NHPP), failures occur independently over time, with a conditional rate of occurrence of failures (ROCOF), \\\\( w(t) \\\\), remaining constant during repairs, reflecting the minimal repair assumption. The cumulative rate function \\\\( W(t) \\\\) defines both the mean \\\\( E[N(t)] = W(t) \\\\) and variance \\\\( \\\\text{var}[N(t)] = W(t) \\\\) of failures in time interval \\\\( (0, t] \\\\). The survival function for the time until the first failure is expressed as \\\\( R_1(t) = e^{-W(t)} \\\\). In an alternating renewal process with exponential up-times and constant downtimes, the availability approaches \\\\( A = \\\\frac{\\\\text{MTTF}}{\\\\text{MTTF} + \\\\text{MDT}} \\\\). The NHPP, particularly suitable for modeling repairable systems like automotive reliability, can be transformed into a homogeneous Poisson process when \\\\( W(t) \\\\) is invertible, where events follow the probability \\\\( P(N = n) = \\\\frac{(\\\\lambda t)^n e^{-\\\\lambda t}}{n!} \\\\).', 'This section explores Non-Homogeneous Poisson Processes (NHPP) and their application in reliability models, particularly in failure rate analysis. The failure rate \\\\( w(t) \\\\) is defined as \\\\( 6 - 2t \\\\) for \\\\( 0 \\\\leq t \\\\leq 2 \\\\), \\\\( -18 + t \\\\) for \\\\( 2 < t \\\\leq 20 \\\\), and \\\\( 0 \\\\) for \\\\( t > 20 \\\\). The cumulative rate of occurrence function (ROCOF) is given by \\\\( W(t) = \\\\int_0^t w(u) du \\\\), where \\\\( E[N(t)] = W(t) \\\\) and \\\\( \\\\text{var}[N(t)] = W(t) \\\\). The survival function until the first failure is \\\\( R_1(t) = e^{-W(t)} \\\\). Availability \\\\( A(t) \\\\) can be evaluated as \\\\( \\\\frac{\\\\text{MTTF}}{\\\\text{MTTF} + \\\\text{MDT}} \\\\). Forward recurrence time is represented by \\\\( Y(t) = SN(t) + 1 - t \\\\). Key metrics such as counting processes and stochastic behaviors enhance the understanding of reliability in systems, indicating varying failure rates and trends crucial for effective reliability modeling.', 'This section discusses Non-Homogeneous Poisson Processes (NHPP) and their application in reliability models, particularly failure rates and the cumulative rate of occurrence of failures (ROCOF). The failure rate is defined as: \\\\( w(t) = 6 - 2t \\\\) for \\\\( 0 \\\\leq t \\\\leq 2 \\\\), \\\\( w(t) = -18 + t \\\\) for \\\\( 2 < t \\\\leq 20 \\\\), and \\\\( w(t) = 0 \\\\) for \\\\( t > 20 \\\\). The cumulative rate function \\\\( W(t) = \\\\int_0^t w(u) du \\\\), defines both \\\\( E[N(t)] \\\\) and \\\\( \\\\text{var}[N(t)] \\\\) as \\\\( W(t) \\\\). The survival function until the first failure is \\\\( R_1(t) = e^{-W(t)} \\\\). Various parametric models for \\\\( w(t) \\\\) include: Power Law \\\\( w(t) = \\\\lambda \\\\beta t^{\\\\beta-1} \\\\), Linear \\\\( w(t) = \\\\lambda(1 + \\\\alpha t) \\\\), and Log-Linear \\\\( w(t) = e^{\\\\alpha + \\\\beta t} \\\\). Statistical tests like the Laplace and Military Handbook tests analyze trends, critical for understanding reliability in systems like automotive repairs. The NHPP can be transformed into a homogeneous Poisson process under certain conditions, aiding the analysis of life data from repairable items.'], [\"In renewal processes, \\\\( W(t) \\\\) represents the expected renewals in the interval \\\\( (0, t] \\\\), with the average renewal length \\\\( \\\\mu \\\\) approximating as \\\\( \\\\mu \\\\approx t / W(t) \\\\). The elementary renewal equation states \\\\( W(t) \\\\approx t / \\\\mu \\\\) as \\\\( t \\\\to \\\\infty \\\\). For item failures, the expected number of failures \\\\( E[N(t)] \\\\approx W(t) \\\\approx t / \\\\mu \\\\), and Blackwell's theorem indicates that for large \\\\( t \\\\) and small intervals \\\\( u \\\\), the mean number of renewals in \\\\( (t, t + u] \\\\) is approximately \\\\( u / \\\\mu \\\\). The fundamental renewal equation is given by \\\\( W(t) = F_T(t) + \\\\int W(t - x)dF_T(x) \\\\). The renewal density \\\\( w(t) \\\\) can be derived through differentiation, integration, or Laplace transforms, approximating to \\\\( 1/\\\\mu \\\\) as \\\\( t \\\\to \\\\infty \\\\). The nth arrival time \\\\( S_n \\\\) is \\\\( S_n = T_1 + T_2 + \\\\ldots + T_n \\\\), and as \\\\( n \\\\) grows, \\\\( S_n \\\\to \\\\mu n \\\\), reinforcing results from the strong law of large numbers and the central limit theorem on distribution effects.\", 'Renewal theory models the timing of replacements in stochastic processes involving independent, identically distributed intervals \\\\( T_1, T_2, \\\\ldots \\\\). The expected number of renewals by time \\\\( t \\\\), denoted \\\\( W(t) \\\\), satisfies the fundamental renewal equation:  \\n\\\\[ W(t) = F_T(t) + \\\\int W(t - x) dF_T(x). \\\\]  \\nAs \\\\( t \\\\) increases, \\\\( W(t) \\\\approx \\\\frac{t}{\\\\mu} \\\\), where \\\\( \\\\mu \\\\) is the average renewal length. The renewal density \\\\( w(t) \\\\) approaches \\\\( \\\\frac{1}{\\\\mu} \\\\) over time and can be derived through differentiation, integration, or Laplace transforms. Blackwell’s theorem refines estimates, indicating that for small intervals \\\\( (t, t + u] \\\\), the mean number of renewals approximates \\\\( \\\\frac{u}{\\\\mu} \\\\). The nth arrival time is \\\\( S_n = T_1 + T_2 + \\\\ldots + T_n \\\\), where \\\\( S_n \\\\to \\\\mu n \\\\) as \\\\( n \\\\) grows, supporting results from the strong law of large numbers and the central limit theorem. In specific distributions like gamma or Weibull, the renewal function exhibits unique forms, impacting reliability metrics and availability.', 'In renewal theory, characterized by independent identically distributed intervals \\\\( T_1, T_2, \\\\ldots \\\\), the expected number of renewals by time \\\\( t \\\\), denoted \\\\( W(t) \\\\), satisfies the equation:  \\n\\\\[ W(t) = F_T(t) + \\\\int W(t - x) dF_T(x). \\\\]  \\nThe renewal density \\\\( w(t) \\\\) is given by \\\\( w(t) = \\\\lambda e^{-\\\\lambda t} \\\\frac{1 - e^{-2\\\\lambda t}}{2} \\\\), which approximates \\\\( \\\\frac{1}{\\\\mu} \\\\) as \\\\( t \\\\to \\\\infty \\\\), where \\\\( \\\\mu \\\\) is the mean. For Weibull-distributed renewal periods with shape parameter \\\\( \\\\alpha \\\\) and scale \\\\( \\\\lambda \\\\), the renewal function is expressed as:  \\n\\\\[ W(t) = \\\\sum_{k=1}^{\\\\infty} \\\\frac{(-1)^{k-1} A_k (\\\\lambda t)^k}{\\\\Gamma(k \\\\alpha + 1)}, \\\\]  \\nwhere coefficients \\\\( A_k \\\\) are recursively determined, and converges to \\\\( \\\\lambda t \\\\) for \\\\( \\\\alpha=1 \\\\). Additionally, \\\\( w(t) \\\\) can be derived via differentiation or Laplace transforms, illustrating the interconnections among mean, variance, and renewal dynamics. In a gamma distribution context, it is represented as \\\\( f_T(t) = \\\\lambda^2 t e^{-\\\\lambda t} \\\\) and \\\\( w(t) = \\\\lambda e^{-\\\\lambda t} \\\\).'], ['**Optimal Age and Block Replacement Strategies in Reliability Management**\\n\\nThe cost function for age replacement, C*(x0), where x0 is the ratio of time t0 to parameter θ, is minimized to find optimal replacement age t0. For failure occurrences, time-to-failure TF,i integrates interval counts Ni, forming a renewal process with expected replacements E(Ni) from a geometric distribution. Mean unavailability Aav(t0) is hinged on mean downtimes (MDTP and MDTF), crucial for effective intervals. The block replacement strategy focuses on regular preventive replacements, with costs structured as \\\\( C_{\\\\infty}(t_0; m) = \\\\frac{c + kF_{(m+1)}(t_0) + k_u \\\\int_0^{t_0} F_{(m+1)}(t) \\\\, dt}{t_0} \\\\). Without spares (m = 0), average costs are \\\\( C_{\\\\infty}(t_0; 0) = \\\\frac{c + kF(t_0) + ku \\\\int_0^{t_0} F(t) \\\\, dt}{t_0} \\\\). The age replacement strategy has total costs expressed as \\\\( E[C(T_R)] = c + k \\\\cdot F(t_0) \\\\), with the asymptotic cost \\\\( C_{\\\\infty}(t_0) = \\\\frac{c + k F(t_0)}{E[C(T_R)]} \\\\). The optimal t0 reduces costs by balancing preventive and corrective strategies, fostering maintenance efficiency.', 'In reliability management, optimal age replacement and block replacement strategies minimize costs associated with equipment failure. The age replacement cost function \\\\( C^*(x_0) \\\\) is defined with \\\\( x_0 = \\\\frac{t_0}{\\\\theta} \\\\), guiding the determination of optimal replacement age \\\\( t_0 \\\\). The time-to-failure \\\\( TF,i \\\\) integrates interval counts \\\\( N_i \\\\) into a renewal process with expected replacements \\\\( E(N_i) \\\\) based on a geometric distribution. Mean unavailability \\\\( A_{av}(t_0) \\\\) relates to mean downtimes (MDTP and MDTF). Under the age replacement strategy, total expected costs are \\\\( E[C(T_R)] = c + k F(t_0) \\\\), with asymptotic costs given by \\\\( C_{\\\\infty}(t_0) = \\\\frac{E[C(T_R)]}{t_0} \\\\). The block replacement strategy features scheduled replacements, with costs structured as \\\\( C_{\\\\infty}(t_0; m) = \\\\frac{c + k F_{(m+1)}(t_0) + k_u \\\\int_0^{t_0} F_{(m+1)}(t) \\\\, dt}{t_0} \\\\). Both strategies require optimization against unavailability, potentially utilizing distributions like Weibull to enhance maintenance efficiency.'], [\"Degradation models are essential for condition-based maintenance strategies in reliability theory, depicting item states through various degradation levels, either continuous or discrete. The state \\\\(X(t)\\\\) illustrates the item's condition, while \\\\(Y(t)\\\\) indicates observable degradation, both considered stochastic processes. The Remaining Useful Lifetime (RUL) at time \\\\(t_j\\\\) is defined as \\\\(Pr(RUL(t_j) \\\\leq t)\\\\). Total degradation increments \\\\(I(t_j, t_k)\\\\) are given by \\\\(Y(t_k) - Y(t_j)\\\\), typically modeled with exponential distributions, yielding expected increment \\\\(E[I(t_1, t_2)] = (t_2 - t_1) / \\\\lambda\\\\) and variance \\\\(var[I(t_1, t_2)] = (t_2 - t_1)^2 / \\\\lambda^2\\\\). Techniques like Wiener and gamma processes, along with trend models (e.g., linear \\\\(Y(t_k) = c + a \\\\cdot t_k + \\\\epsilon(t_k)\\\\)), support RUL predictions, optimizing maintenance by correlating current degradation indicators with future reliability.\", 'Degradation models are crucial in reliability theory for condition-based maintenance, illustrating item states through continuous or discrete degradation levels. The item condition is represented by state \\\\(X(t)\\\\), while observable degradation is denoted by \\\\(Y(t)\\\\), both treated as stochastic processes. The Remaining Useful Lifetime (RUL) at time \\\\(t_j\\\\) is expressed as \\\\(Pr(RUL(t_j) \\\\leq t)\\\\). Total degradation increments \\\\(I(t_j, t_k)\\\\) are modeled using exponential distributions, yielding expected increment \\\\(E[I(t_1, t_2)] = (t_2 - t_1)/\\\\lambda\\\\) and variance \\\\(var[I(t_1, t_2)] = (t_2 - t_1)^2/\\\\lambda^2\\\\). Techniques such as Wiener and gamma processes, with the latter ensuring positive increments, optimize RUL predictions. Additionally, degradation modeling involves generating history with Gaussian noise in observations, employing Monte Carlo methods for simulations, and assessing model accuracy through empirical cumulative distributions and probability density functions. This integration enables effective maintenance strategies based on monitored degradation indicators.'], ['Maximum Likelihood Estimation (MLE), introduced by Ronald Aylmer Fisher in 1922, estimates parameters by maximizing the likelihood function. For an exponential distribution with n events over a time interval τ, the likelihood is \\\\( L(λ | n, τ) = \\\\frac{(λτ)^n e^{-λτ}}{n!} \\\\) and log-likelihood is \\\\( ℓ(λ | n, τ) = n \\\\log(λ) - λτ \\\\). Setting \\\\( \\\\frac{dℓ}{dλ} = 0 \\\\) yields \\\\( ˆλ = \\\\frac{n}{τ} \\\\); for instance, \\\\( ˆλ \\\\) is approximately 3.2 × 10⁻⁴ (hours)⁻¹ with n = 5 and τ = 15,600 hours. MLE properties include asymptotic unbiasedness and consistency. Similarly, for a binomial model \\\\( L(p | x) = \\\\binom{n}{x} p^x (1 - p)^{n-x} \\\\), the MLE \\\\( ˆp = \\\\frac{x}{n} \\\\) is derived from \\\\( ℓ(p | x) = x \\\\log(p) + (n-x) \\\\log(1-p) + \\\\text{constant} \\\\). For a Poisson process, \\\\( ℓ(λ | n, τ) = n \\\\log(λτ) - λτ \\\\) leads to \\\\( ˆλ = \\\\frac{n}{τ} \\\\). MLE is preferred for parameter estimation across various statistical models.', \"Maximum Likelihood Estimation (MLE) estimates parameters like the rate parameter \\\\( \\\\lambda \\\\) for exponential and Weibull distributions from observed data. For \\\\( n \\\\) events over time interval \\\\( \\\\tau \\\\), the likelihood function for an exponential distribution is given by \\\\( L(\\\\lambda | n, \\\\tau) = \\\\frac{(\\\\lambda \\\\tau)^n e^{-\\\\lambda \\\\tau}}{n!} \\\\), leading to the log-likelihood \\\\( \\\\ell(\\\\lambda | n, \\\\tau) = n \\\\log(\\\\lambda) - \\\\lambda \\\\tau \\\\). Setting \\\\( \\\\frac{d\\\\ell}{d\\\\lambda} = 0 \\\\) yields MLE \\\\( \\\\hat{\\\\lambda} = \\\\frac{n}{\\\\tau} \\\\). For example, with \\\\( n = 10 \\\\) and \\\\( \\\\tau = 68,450 \\\\) hours, \\\\( \\\\hat{\\\\lambda} \\\\approx 1.461 \\\\times 10^{-4} \\\\) hours⁻¹, though it's biased; an unbiased estimator is \\\\( \\\\lambda^* = \\\\frac{(n-1)\\\\hat{\\\\lambda}}{n} \\\\). In Weibull distributions with shape \\\\( \\\\alpha \\\\) and scale \\\\( \\\\theta \\\\), MLE can be implemented via R packages like WeibullR, applicable for both complete and censored data analysis, demonstrating MLE's flexibility in statistical modeling.\", 'In Maximum Likelihood Estimation (MLE) for exponential and Weibull distributions, the MLE for the rate parameter λ of an exponential distribution can be derived from observed data as λ̂ = n/τ, where n is the number of observations and τ is the total time. For a sample of 10 lifetimes totaling 68,450 hours, the estimate λ̂ ≈ 1.461 × 10⁻⁴ hours⁻¹. Though λ̂ is biased, the unbiased estimator λ* = (n-1)λ̂/n has variance var(λ*) = 4(n-1)²λ²/n. In Type II censoring, the MLE becomes λ² = r/Total-Time, integrating contributions from failing and surviving items to yield a chi-squared distribution. The Weibull distribution involves parameters α (shape) and θ (scale) with its log-likelihood leading to parameter estimation. R packages like WeibullR provide tools for analyzing both complete and censored datasets.'], [\"The survivor function \\\\( R(t) \\\\) is defined with continuous probability density \\\\( f(t) = R'(t) \\\\), where \\\\( f(t) > 0 \\\\) for \\\\( t > 0 \\\\). The failure rate function is \\\\( z(t) = \\\\frac{f(t)}{R(t)} \\\\), with the cumulative failure rate \\\\( Z(t) = \\\\int_0^t z(u) \\\\, du \\\\), leading to \\\\( R(t) = e^{-Z(t)} \\\\). The Nelson-Aalen estimate for the cumulative failure rate is given by \\\\( \\\\hat{Z}(t) = \\\\sum_{j; t_j < t} \\\\frac{d_j}{n_j} \\\\), where \\\\( d_j \\\\) is the number of failures at time \\\\( t_j \\\\) and \\\\( n_j \\\\) is the at-risk number. This results in \\\\( \\\\hat{R}^*(t) = \\\\exp(-\\\\hat{Z}(t)} \\\\). For specific distributions, \\\\( Z(t) = \\\\lambda t \\\\) for exponential and \\\\( Z(t) = \\\\left(\\\\frac{t}{\\\\theta}\\\\right)^\\\\alpha \\\\) for Weibull. Nelson-Aalen estimates agree closely with Kaplan-Meier estimates for shorter times but diverge for longer ones. TTT plots can be used to analyze failure rates, and R can efficiently create these plots, despite the lack of specific packages for Nelson-Aalen visualizations.\", \"The Nelson-Aalen estimate, utilized for analyzing lifetimes in both censored and uncensored datasets, begins with an ordered set of lifetimes where \\\\( n_j \\\\) indicates items at risk before time \\\\( t(j) \\\\) and \\\\( d_j \\\\) represents the failures at that time. The cumulative failure rate estimator is given by \\\\( Z(t) = \\\\sum (d_j/n_j) \\\\), leading to the survival function \\\\( R(t) = e^{-Z(t)} \\\\). The failure rate, \\\\( \\\\lambda_j \\\\), is defined as \\\\( \\\\lambda_j = d_j / \\\\text{Total functioning time} \\\\), and the probability density function is \\\\( f(t) = R'(t) \\\\), yielding the failure rate function \\\\( z(t) = f(t) / R(t) \\\\) and cumulative failure rate \\\\( Z(t) = \\\\int_0^t z(u) du \\\\). TTT plots can illustrate lifespan analysis, indicating whether failure rates are increasing or decreasing, while comparisons with Kaplan-Meier estimates often show good agreement, particularly for shorter failure times. The precise plotting can be conducted using R, despite limited specific packages for Nelson-Aalen plots.\"], ['The Total-Time-on-Test (TTT) transform quantifies the area under the survivor function \\\\( R(t) \\\\) for a life distribution \\\\( F(t) \\\\), defined as:\\n\\n\\\\[\\nH_{F^{-1}}(v) = \\\\int_0^{F^{-1}(v)} [1 - F(u)] \\\\, du \\\\quad \\\\text{for } 0 \\\\leq v \\\\leq 1.\\n\\\\]\\n\\nFor large samples, the ratio \\\\(\\\\frac{\\\\mathcal{T}(T_i)}{\\\\mathcal{T}(T_n)}\\\\) approximates \\\\(\\\\frac{i}{n}\\\\). If \\\\(F(t)\\\\) is exponential, with \\\\(F(t) = 1 - e^{-\\\\lambda t}\\\\), the transform simplifies to \\\\(H_{F^{-1}}(v) = -\\\\frac{1 - v}{\\\\lambda}\\\\), yielding a linear function from (0,0) to (1,1). For the Weibull distribution \\\\(F(t) = 1 - e^{-(\\\\frac{t}{\\\\theta})^\\\\alpha}\\\\), the inverse is \\\\(F^{-1}(v) = \\\\theta[-\\\\log(1 - v)]^{1/\\\\alpha}\\\\), resulting in a more complex TTT expressed in terms of the incomplete gamma function, with mean time-to-failure given by \\\\(E(T) = H_{F^{-1}}(1) = \\\\theta \\\\Gamma(1/\\\\alpha + 1)\\\\). This analysis aids in understanding distributions based on failure rates and empirical observations.', 'The Total-Time-on-Test (TTT) transform quantifies the relationship between life distributions \\\\(F(t)\\\\) and their survivor functions, defined as:\\n\\n\\\\[\\nH_{F^{-1}}(v) = \\\\int_0^{F^{-1}(v)} [1 - F(u)] \\\\, du \\\\quad \\\\text{for } 0 \\\\leq v \\\\leq 1.\\n\\\\]\\n\\nFor large samples, the ratios \\\\(\\\\frac{\\\\mathcal{T}(T_i)}{\\\\mathcal{T}(T_n)}\\\\) approximate \\\\(\\\\frac{i}{n}\\\\). If \\\\(F(t)\\\\) is exponential, with \\\\(F(t) = 1 - e^{-\\\\lambda t}\\\\), the transform simplifies to \\\\(H_{F^{-1}}(v) = -\\\\frac{1 - v}{\\\\lambda}\\\\), yielding a linear function. For Weibull distributions, \\\\(F(t) = 1 - e^{-(\\\\frac{t}{\\\\theta})^\\\\alpha}\\\\), the inverse is \\\\(F^{-1}(v) = \\\\theta[-\\\\log(1 - v)]^{1/\\\\alpha}\\\\), leading to a TTT transform tied to the incomplete gamma function and dependent on the shape parameter \\\\(\\\\alpha\\\\). The mean time-to-failure, \\\\(E(T)\\\\), is \\\\(H_{F^{-1}}(1) = \\\\theta \\\\Gamma(1/\\\\alpha + 1)\\\\). This analysis aids in understanding the nature of distributions based on failure rates and empirical observations.'], ['In Bayesian analysis of failure rates \\\\(\\\\lambda\\\\) using a gamma prior \\\\(\\\\text{gamma}(\\\\alpha, \\\\beta)\\\\), the prior mean is \\\\(E(\\\\lambda) = \\\\frac{\\\\alpha}{\\\\beta}\\\\). The likelihood for observed failures \\\\(n_1\\\\) over time \\\\(t\\\\) is given by \\\\(L(\\\\lambda | n_1, t) \\\\propto \\\\lambda^{n_1} e^{-\\\\lambda t}\\\\), leading to a posterior distribution that remains gamma with updated parameters \\\\((\\\\alpha + n_1)\\\\) and \\\\((\\\\beta + t)\\\\), resulting in a new posterior mean \\\\(E(\\\\lambda | n_1, t) = \\\\frac{\\\\alpha + n_1}{\\\\beta + t}\\\\). This process allows for sequential updates where \\\\( \\\\alpha_n = \\\\alpha_{n-1} + 1 \\\\) and \\\\( \\\\beta_n = \\\\beta_{n-1} + t_n\\\\), enhancing estimation accuracy for nonrepairable valves. The marginal distribution of failures \\\\(N(t)\\\\) follows a negative binomial distribution, and noninformative priors yield equal likelihoods, simplifying posterior estimation to reflect predominantly the likelihood.', 'In Bayesian analysis, the failure rate \\\\(\\\\lambda\\\\) is modeled with a gamma prior \\\\(\\\\text{gamma}(\\\\alpha, \\\\beta)\\\\), where the prior mean is \\\\(E(\\\\lambda) = \\\\frac{\\\\alpha}{\\\\beta}\\\\). The likelihood for observed failures \\\\(n_1\\\\) in time \\\\(t\\\\) is given by \\\\(L(\\\\lambda | n_1, t) \\\\propto \\\\lambda^{n_1} e^{-\\\\lambda t}\\\\), resulting in a posterior distribution that remains gamma with updated parameters \\\\((\\\\alpha + n_1)\\\\) and \\\\((\\\\beta + t)\\\\), leading to a posterior mean \\\\(E(\\\\lambda | n_1, t) = \\\\frac{\\\\alpha + n_1}{\\\\beta + t}\\\\). The marginal distribution for the number of failures \\\\(N(t)\\\\) follows a negative binomial distribution when \\\\(\\\\alpha\\\\) is an integer. Noninformative priors allow for equal likelihoods, simplifying the posterior to reflect only the likelihood. This iterative updating process continuously refines our understanding of \\\\(\\\\lambda\\\\) through joint density calculations and observations, depicted as \\\\(\\\\pi(\\\\lambda | T_1) = \\\\frac{\\\\lambda^2 e^{-\\\\lambda(t_1 + 1)}}{(t_1 + 1)^3}\\\\) with parameters updated at each stage.'], ['In a 2-out-of-3 (2oo3) structure, minimal path and cut sets are \\\\(P_1 = \\\\{1, 2\\\\}, P_2 = \\\\{1, 3\\\\}, P_3 = \\\\{2, 3\\\\}\\\\) and \\\\(K_1 = \\\\{1, 2\\\\}, K_2 = \\\\{1, 3\\\\}, K_3 = \\\\{2, 3\\\\}\\\\). The path function for each minimal path set is defined as \\\\( \\\\rho_j(x) = \\\\prod_{i \\\\in P_j} x[i] \\\\), leading to the overall structure function \\\\( \\\\phi(x) = 1 - \\\\prod_{j=1}^{p} (1 - \\\\rho_j(x)) \\\\). For cut sets, the function is \\\\( \\\\kappa_j(x) = \\\\prod_{i \\\\in K_j} x[i] \\\\), where minimal cut sets indicate failures causing structural breakdown. Reliability analysis hinges on these concepts, as illustrated in Python functions for calculating path and structure functions. The pivotal decomposition approach assesses reliability focused on component \\\\(i\\\\). This highlights the critical role of minimal path and cut sets in evaluating structural reliability, essential in applications like bridge assessments.', 'The minimal cut set \\\\( K_j \\\\) for a binary function and the overall structure function \\\\( \\\\phi(x) \\\\) for a parallel system of minimal cut structures can be defined using Python functions. The cut set is represented by:\\n\\n```python\\ndef k_j(x):\\n    return all(x[i] for i in K_j) - prod(1 - x[i] for i in K_j)\\n```\\n\\nThe overall structure function is:\\n\\n```python\\ndef phi(x):\\n    return 1 - prod(1 - rho(j, x) for j in range(1, p + 1))\\n```\\n\\nwhere \\\\( \\\\rho(j, x) \\\\) evaluates minimal paths as:\\n\\n```python\\ndef rho(j, x):\\n    return prod(x[i] for i in minimal_path(j))\\n```\\n\\nIn pivotal decomposition, the reliability of focusing on component \\\\( i \\\\) is:\\n\\n```python\\ndef pivotal_decomposition(x, i):\\n    return x[i] * phi(1, x) + (1 - x[i]) * phi(0, x)\\n```\\n\\nFor a bridge structure, using pivotal decomposition:\\n\\n```python\\ndef phi_bridge(x):\\n    return x[3] * ((x[1] | x[2]) * (x[4] | x[5])) + (1 - x[3]) * (x[1] * x[4] + x[2] * x[5])\\n```\\n\\nIn a 2-out-of-3 (2oo3) structure, both minimal path sets \\\\( P \\\\) and minimal cut sets \\\\( K \\\\) are defined similarly. Each path set can be evaluated with:\\n\\n```python\\ndef rho_j(x):\\n    return prod(x[i] for i in P_j)\\n```\\n\\nThe overall structure function for this is given by:\\n\\n```python\\ndef phi(x):\\n    return 1 - prod(1 - rho_j(x) for j in range(1, p + 1))\\n```\\n\\nThese foundations illustrate the critical role of minimal paths and cuts in reliability analysis.'], ['The time-to-failure \\\\( T \\\\) is modeled by a Weibull distribution \\\\( \\\\text{Weibull}(\\\\alpha, \\\\theta) \\\\), leading to the survivor function \\\\( R(t) = P(T > t) = e^{-\\\\left( \\\\frac{t}{\\\\theta} \\\\right)^\\\\alpha} \\\\). For \\\\( T^\\\\alpha \\\\), we find \\\\( P(T^\\\\alpha > t) = e^{-\\\\frac{t}{\\\\theta^\\\\alpha}} \\\\), indicating it is exponentially distributed with failure rate \\\\( \\\\lambda = \\\\frac{1}{\\\\theta^\\\\alpha} \\\\). In a series of \\\\( n \\\\) components, \\\\( R_s(t) = e^{-\\\\sum_{i=1}^{n} \\\\left( \\\\frac{t}{\\\\theta_i} \\\\right)^\\\\alpha} \\\\). For identical components, the equivalent scale parameter is \\\\( \\\\theta_s = \\\\frac{\\\\theta}{n^{1/\\\\alpha}} \\\\), resulting in \\\\( MTTF_s = \\\\frac{\\\\theta \\\\Gamma(1 + \\\\frac{1}{\\\\alpha})}{n^{1/\\\\alpha}} \\\\). The three-parameter model includes a threshold \\\\( \\\\xi \\\\), yielding \\\\( MTTF = \\\\xi + \\\\theta \\\\Gamma(1 + \\\\frac{1}{\\\\alpha}) \\\\). The failure rate function is \\\\( z(t) = \\\\frac{\\\\alpha}{\\\\theta} \\\\left( \\\\frac{t}{\\\\theta} \\\\right)^{\\\\alpha - 1} \\\\). The exponential distribution, characterized by \\\\( f(t) = \\\\lambda e^{-\\\\lambda t} \\\\) and \\\\( R(t) = e^{-\\\\lambda t} \\\\), exhibits memoryless properties, highlighting its significance in reliability analysis.', \"The Weibull distribution is characterized by its probability density function:\\n\\n\\\\[\\nf(t) = \\\\frac{\\\\alpha}{\\\\theta} \\\\left(\\\\frac{t}{\\\\theta}\\\\right)^{\\\\alpha - 1} e^{-\\\\left(\\\\frac{t}{\\\\theta}\\\\right)^\\\\alpha} \\\\quad (t > 0)\\n\\\\]\\n\\nwith scale parameter \\\\(\\\\theta\\\\) and shape parameter \\\\(\\\\alpha\\\\). The survivor function is given by:\\n\\n\\\\[\\nR(t) = e^{-\\\\left(\\\\frac{t}{\\\\theta}\\\\right)^\\\\alpha} \\\\quad (t > 0)\\n\\\\]\\n\\nThe failure rate function is:\\n\\n\\\\[\\nz(t) = \\\\frac{\\\\alpha}{\\\\theta} \\\\left(\\\\frac{t}{\\\\theta}\\\\right)^{\\\\alpha - 1} \\\\quad (t > 0)\\n\\\\]\\n\\nwhich reveals characteristics: if \\\\(\\\\alpha < 1\\\\), \\\\(z(t)\\\\) decreases; if \\\\(\\\\alpha = 1\\\\), it's constant; if \\\\(\\\\alpha > 1\\\\), it increases. The Mean Time to Failure (MTTF) and median are:\\n\\n\\\\[\\n\\\\text{MTTF} = \\\\theta \\\\Gamma(1 + \\\\frac{1}{\\\\alpha}), \\\\quad t_m = \\\\theta (\\\\log 2)^{1/\\\\alpha}\\n\\\\]\\n\\nIn series systems of \\\\(n\\\\) components, the survivor function modifies to:\\n\\n\\\\[\\nR_s(t) = e^{-\\\\sum_{i=1}^{n} \\\\left(\\\\frac{t}{\\\\theta_i}\\\\right)^{\\\\alpha}}, \\\\quad \\\\text{MTTF}_{s} = \\\\frac{\\\\theta \\\\Gamma(1 + \\\\frac{1}{\\\\alpha})}{n^{1/\\\\alpha}}\\n\\\\]\\n\\nFor a three-parameter Weibull distribution, the mean and variance become:\\n\\n\\\\[\\n\\\\text{MTTF} = \\\\xi + \\\\theta \\\\Gamma(1 + \\\\frac{1}{\\\\alpha}), \\\\quad \\\\text{Var}(T) = \\\\theta^2 \\\\left[\\\\Gamma\\\\left(1 + \\\\frac{2}{\\\\alpha}\\\\right) - \\\\Gamma^2\\\\left(1 + \\\\frac{1}{\\\\alpha}\\\\right)\\\\right]\\n\\\\]\\n\\nThis distribution effectively models varied time-to-failure scenarios, emphasizing reliability analysis.\", 'Assuming \\\\( T \\\\) follows a Weibull distribution \\\\( Weibull(\\\\alpha, \\\\theta) \\\\), we derive the survivor function for \\\\( T^{\\\\alpha} \\\\) as \\\\( P(T^{\\\\alpha} > t) = e^{- \\\\frac{t}{\\\\theta^{\\\\alpha}}} \\\\), indicating that \\\\( T^{\\\\alpha} \\\\) is exponentially distributed with failure rate \\\\( \\\\lambda = \\\\frac{1}{\\\\theta^{\\\\alpha}} \\\\). For \\\\( n \\\\) independent components, the survivor function is \\\\( R_s(t) = e^{-\\\\sum_{i=1}^{n} \\\\left( \\\\frac{t}{\\\\theta_i} \\\\right)^{\\\\alpha}} \\\\). For identical components, \\\\( \\\\theta_s = \\\\frac{\\\\theta}{n^{1/\\\\alpha}} \\\\), leading to \\\\( MTTF_s = \\\\frac{\\\\theta \\\\Gamma(1 + \\\\frac{1}{\\\\alpha})}{n^{1/\\\\alpha}} \\\\). In the three-parameter Weibull distribution, the mean is \\\\( MTTF = \\\\xi + \\\\theta \\\\Gamma\\\\left(1 + \\\\frac{1}{\\\\alpha}\\\\right) \\\\) and variance is \\\\( \\\\text{Var}(T) = \\\\theta^2 \\\\left[\\\\Gamma(1 + \\\\frac{2}{\\\\alpha}) - \\\\Gamma^2(1 + \\\\frac{1}{\\\\alpha})\\\\right] \\\\).'], ['The survivor function \\\\( R_S(t) \\\\) for standby systems quantifies the probability that a system remains operational over time \\\\( t \\\\). For a system with \\\\( n \\\\) items, it is given by \\\\( R_S(t) = \\\\sum_{k=0}^{n-1} \\\\frac{(\\\\lambda t)^k e^{-\\\\lambda t}}{k!} \\\\). In a cold standby system with two items, this simplifies to \\\\( R_S(t) = e^{-\\\\lambda_1 t} + (1-p) \\\\lambda_1 e^{-\\\\lambda_2 t} \\\\int_0^t e^{-(\\\\lambda_1 - \\\\lambda_2)\\\\tau} d\\\\tau \\\\), or \\\\( R_S(t) = e^{-\\\\lambda t} + (1-p)\\\\lambda t e^{-\\\\lambda t} \\\\) if both items have the same failure rate \\\\( \\\\lambda \\\\). The mean time to failure (MTTFS) is calculated as \\\\( MTTFS = \\\\int R_S(t) dt \\\\). For example, with \\\\( \\\\lambda = 10^{-3} \\\\) failures/hour and \\\\( p = 0.015 \\\\), \\\\( R_S(1000) \\\\approx 0.7302 \\\\) yields \\\\( MTTFS \\\\approx 1985 \\\\) hours. As \\\\( n \\\\) increases, the distribution of total time-to-failure approaches normal distribution by the Central Limit Theorem.', 'The survivor function \\\\( R_S(t) \\\\) for standby systems represents the probability that a system remains operational over time \\\\( t \\\\). For a system with \\\\( n \\\\) items, it is given by \\\\( R_S(t) = \\\\sum_{k=0}^{n-1} \\\\frac{(\\\\lambda t)^k e^{-\\\\lambda t}}{k!} \\\\). In a two-item system, this simplifies to \\\\( R_S(t) = e^{-\\\\lambda_1 t} + (1-p) \\\\lambda_1 e^{-\\\\lambda_2 t} \\\\int_0^t e^{-(\\\\lambda_1 - \\\\lambda_2) \\\\tau} d\\\\tau \\\\). If both items have the same failure rate, this reduces to \\\\( R_S(t) = e^{-\\\\lambda t} + (1-p) \\\\lambda t e^{-\\\\lambda t} \\\\). The mean time to failure (MTTFS) is calculated as \\\\( MTTFS = \\\\int R_S(t) dt \\\\). In a cold standby system, where reserve items activate sequentially, the total time-to-failure \\\\( T_S \\\\) is the sum \\\\( T_S = T_1 + T_2 + \\\\ldots + T_n \\\\), and for large \\\\( n \\\\), the distribution of \\\\( T_S \\\\) approaches normality due to the Central Limit Theorem. For example, with \\\\( \\\\lambda = 10^{-3} \\\\) failures/hour and \\\\( p = 0.015 \\\\), \\\\( R_S(1000) \\\\approx 0.7302 \\\\) leads to \\\\( MTTFS \\\\approx 1985 \\\\) hours.'], ['To calculate the Mean Time to First Failure (MTTF) for a system, create a transition rate matrix \\\\( A \\\\) based on states {0, 1, ..., r} and define the initial distribution \\\\( P(0) \\\\). Identify absorbing failed states and form the reduced matrix \\\\( A_R \\\\) by removing these states. Compute the Laplace transform \\\\( P^*(s) \\\\) excluding absorbing states to yield \\\\( P^*_R(s) \\\\), then establish \\\\( P^*_R(s) A_R = [s P^*(s) - P(0)]_R \\\\) and evaluate at \\\\( s=0 \\\\). MTTF is computed as \\\\( MTTF = \\\\sum P_j(0) \\\\) over non-absorbing states. In parallel structures of \\\\( n \\\\) identical components, MTTFS simplifies to \\\\( MTTFS = \\\\sum_{i=1}^n \\\\frac{1}{i\\\\lambda} \\\\). For CCFs, \\\\( MTTF \\\\) adjusts to \\\\( \\\\frac{1}{n\\\\lambda(n-(n-1)\\\\beta)} \\\\), and in load-sharing systems defined by failure rates \\\\( \\\\lambda_h, \\\\lambda_f \\\\) and repair rates \\\\( \\\\mu_h, \\\\mu_f \\\\), MTTF can be derived based on transition matrices and Laplace transforms, focusing on reliability under various operational conditions.', 'To calculate the Mean Time to First Failure (MTTF) for a system, construct a transition rate matrix \\\\( A \\\\) based on states {0, 1, ..., r} and define the initial distribution \\\\( P(0) \\\\). Identify absorbing failed states and form the reduced matrix \\\\( A_R \\\\) by removing related rows and columns. Compute the Laplace transform \\\\( P^*(s) \\\\) excluding absorbing states, leading to the equation \\\\( P^*_R(s) A_R = [s P^*(s) - P(0)]_R \\\\). MTTF is derived as \\\\( \\\\text{MTTF} = \\\\sum P_j(0) \\\\) over non-absorbing states. In parallel systems, with failure rates \\\\( \\\\lambda \\\\) and repair rates \\\\( \\\\mu \\\\), MTTF for identical components is given by \\\\( \\\\frac{\\\\mu}{2\\\\lambda} \\\\). With common cause failures characterized by a beta-factor \\\\( \\\\beta \\\\), it becomes \\\\( \\\\text{MTTF} = \\\\frac{1}{n\\\\lambda(n-(n-1)\\\\beta)} \\\\) for \\\\( n \\\\) components. Reliability is represented by \\\\( R(t) = e^{-n(1 - \\\\beta) \\\\lambda t} e^{-\\\\beta \\\\lambda t} \\\\). For dependent failure rates in load-sharing systems, transition dynamics can be analyzed similarly, simplifying MTTF calculations amid varied operational stresses.', 'To calculate the Mean Time to First Failure (MTTF) for a system, construct a transition rate matrix \\\\( A \\\\) based on states {0, 1, ..., r}, and define the initial state distribution \\\\( P(0) \\\\). Identify absorbing failed states, creating a reduced matrix \\\\( A_R \\\\) by removing those states. Compute the Laplace transform \\\\( P^*(s) \\\\) excluding the absorbing states, yielding \\\\( P^*_R(s) \\\\). Establish the equation \\\\( P^*_R(s) A_R = [s P^*(s) - P(0)]_R \\\\) and evaluate at \\\\( s=0 \\\\) to find \\\\( P^*_R(0) \\\\). MTTF is then calculated as \\\\( \\\\text{MTTF} = \\\\sum P_j(0) \\\\) over non-absorbing states. For systems with \\\\( n \\\\) identical components, MTTF is formulated as \\\\( \\\\text{MTTF} = \\\\sum_{i=1}^n \\\\frac{1}{i\\\\lambda} \\\\). In parallel systems with independent failure rates \\\\( \\\\lambda_1 \\\\) and \\\\( \\\\lambda_2 \\\\), MTTF can be analyzed similarly, also incorporating common cause failures characterized by a beta-factor \\\\( \\\\beta \\\\) for comprehensive reliability assessment.'], ['This section evaluates system reliability, focusing on the Probability of Failure on Demand (PFD) across configurations. For a 1oo2:G structure with distinct fire detectors having failure rates λDU,1 and λDU,2, the PFD is approximated as PFD ≈ (λDU * τ) / 2 for minimal λDU * τ. The mean number of successful test intervals until the first detected demand-uncorrectable (DU) failure is determined, along with the average duration of malfunctioning post-identification. Considering a Poisson process at a rate of 1 fire every 10 years, the likelihood of a fire during a DU failure over 50 years is assessed. The common cause failure (CCF) model is analyzed, particularly the impact of reducing the beta factor (β) and the similar PFDs of 1oo4:G and 2oo4:G structures under identical conditions. Testing over 3 months (2190 hours) yields a PFD of approximately 0.00023, indicating high reliability, where a 2oo3:G structure sees an increase to about 2.1 × 10⁻⁷.', 'This section evaluates system reliability, focusing on the Probability of Failure on Demand (PFD) across different configurations. For a 1oo2:G structure with distinct fire detectors having failure rates λDU,1 and λDU,2, the PFD can be approximated for minimal λDU,iτ. The failure rate of fire detectors is approximately 0.21 × 10⁻⁶ failures per hour. Testing over 3 months (about 2190 hours), the PFD is about 0.00023, indicating roughly 1 in 4350 fires may go undetected. In a 1-out-of-2 setup, the PFD is 7.1 × 10⁻⁸, and for a 2-out-of-3 structure, it’s about 2.1 × 10⁻⁷. The PFD is computed as PFD = λDU × τ, with τ set at 4,380 hours for gas detectors (yielding PFD ≈ 0.0079). Additionally, a beta-factor of 0.08 adjusts PFD for common cause failures, highlighting debates on beta reduction and similar PFD between 1oo4:G and 2oo4:G structures under identical conditions, which aids in evaluating overall reliability.'], ['Maximum Likelihood Estimation (MLE), introduced by Ronald Aylmer Fisher in 1922, estimates parameters by maximizing the likelihood function. For an exponential distribution with n events over a time interval τ, the likelihood is \\\\( L(λ | n, τ) = \\\\frac{(λτ)^n e^{-λτ}}{n!} \\\\) and log-likelihood is \\\\( ℓ(λ | n, τ) = n \\\\log(λ) - λτ \\\\). Setting \\\\( \\\\frac{dℓ}{dλ} = 0 \\\\) yields \\\\( ˆλ = \\\\frac{n}{τ} \\\\); for instance, \\\\( ˆλ \\\\) is approximately 3.2 × 10⁻⁴ (hours)⁻¹ with n = 5 and τ = 15,600 hours. MLE properties include asymptotic unbiasedness and consistency. Similarly, for a binomial model \\\\( L(p | x) = \\\\binom{n}{x} p^x (1 - p)^{n-x} \\\\), the MLE \\\\( ˆp = \\\\frac{x}{n} \\\\) is derived from \\\\( ℓ(p | x) = x \\\\log(p) + (n-x) \\\\log(1-p) + \\\\text{constant} \\\\). For a Poisson process, \\\\( ℓ(λ | n, τ) = n \\\\log(λτ) - λτ \\\\) leads to \\\\( ˆλ = \\\\frac{n}{τ} \\\\). MLE is preferred for parameter estimation across various statistical models.', \"Maximum Likelihood Estimation (MLE) estimates parameters like the rate parameter \\\\( \\\\lambda \\\\) for exponential and Weibull distributions from observed data. For \\\\( n \\\\) events over time interval \\\\( \\\\tau \\\\), the likelihood function for an exponential distribution is given by \\\\( L(\\\\lambda | n, \\\\tau) = \\\\frac{(\\\\lambda \\\\tau)^n e^{-\\\\lambda \\\\tau}}{n!} \\\\), leading to the log-likelihood \\\\( \\\\ell(\\\\lambda | n, \\\\tau) = n \\\\log(\\\\lambda) - \\\\lambda \\\\tau \\\\). Setting \\\\( \\\\frac{d\\\\ell}{d\\\\lambda} = 0 \\\\) yields MLE \\\\( \\\\hat{\\\\lambda} = \\\\frac{n}{\\\\tau} \\\\). For example, with \\\\( n = 10 \\\\) and \\\\( \\\\tau = 68,450 \\\\) hours, \\\\( \\\\hat{\\\\lambda} \\\\approx 1.461 \\\\times 10^{-4} \\\\) hours⁻¹, though it's biased; an unbiased estimator is \\\\( \\\\lambda^* = \\\\frac{(n-1)\\\\hat{\\\\lambda}}{n} \\\\). In Weibull distributions with shape \\\\( \\\\alpha \\\\) and scale \\\\( \\\\theta \\\\), MLE can be implemented via R packages like WeibullR, applicable for both complete and censored data analysis, demonstrating MLE's flexibility in statistical modeling.\", 'Maximum Likelihood Estimation (MLE), introduced by Ronald Aylmer Fisher in 1922, is a method for estimating parameters by maximizing the likelihood function. For a binomial model, the likelihood function for observing \\\\( x \\\\) successes in \\\\( n \\\\) trials is defined as:  \\n\\\\[ L(p | x) = \\\\binom{n}{x} p^x (1 - p)^{n-x} \\\\]  \\nFor example, with \\\\( n = 10 \\\\) and \\\\( x = 3 \\\\), the likelihood simplifies to \\\\( L(p | 3) = p^3 (1 - p)^7 \\\\). The MLE \\\\( \\\\hat{p} = \\\\frac{x}{n} \\\\) maximizes the likelihood, though \\\\( L(p | 3) \\\\) is not a probability distribution. The log-likelihood function \\\\( \\\\ell(\\\\theta | \\\\text{data}) = \\\\log L(\\\\theta | \\\\text{data}) \\\\) simplifies analysis, and minimizing the negative log-likelihood is often used to find MLEs. In a homogeneous Poisson process, the likelihood of observing \\\\( n \\\\) events in \\\\( \\\\tau \\\\) hours is \\\\( L(\\\\lambda | n, \\\\tau) = \\\\frac{(\\\\lambda \\\\tau)^n e^{-\\\\lambda \\\\tau}}{n!} \\\\), leading to \\\\( \\\\hat{\\\\lambda} = \\\\frac{n}{\\\\tau} \\\\).'], [\"The probability density function (pdf) for failure is defined as \\\\( f(t) = -R'(t) = \\\\frac{0.4}{(0.2t + 1)^3} \\\\), with the failure rate function \\\\( z(t) = \\\\frac{t}{t + 1} \\\\). The survival function, indicating the probability of survival by time \\\\( t \\\\), is expressed as \\\\( R(t) = \\\\frac{1}{(0.2t + 1)^2} \\\\), and the Mean Time To Failure (MTTF) is calculated as \\\\( MTTF = \\\\int_0^5 R(t) \\\\, dt = 5 \\\\) months. The mean residual lifetime (MRL) at age \\\\( x \\\\) is defined as \\\\( MRL(x) = \\\\frac{\\\\int_x^\\\\infty R(t) \\\\, dt}{R(x)} \\\\). Furthermore, \\\\( g(x) = \\\\frac{MTTF}{MRL(x)} \\\\) establishes a relationship between MTTF and MRL. Critical relationships include \\\\( F(t) = 1 - R(t) \\\\) and \\\\( z(t) = \\\\frac{f(t)}{R(t)} \\\\). The cumulative failure rate is modeled as \\\\( Z(t) = \\\\int_0^t z(u) \\\\, du \\\\), leading to \\\\( R(t) = e^{-Z(t)} \\\\), which aids in reliability assessment across various distributions in engineering contexts.\", \"The probability density function is defined as \\\\( f(t) = -R'(t) = \\\\frac{0.4}{(0.2t + 1)^3} \\\\), with the failure rate function given by \\\\( z(t) = \\\\frac{f(t)}{R(t)} \\\\). The survivor function is \\\\( R(t) = e^{-\\\\int_0^t z(u) \\\\, du} \\\\) and calculates the probability that an item survives up to time \\\\( t \\\\). The Mean Time to Failure (MTTF) is computed as \\\\( MTTF = \\\\int_0^\\\\infty t f(t) \\\\, dt = 5 \\\\) months, while the Mean Residual Lifetime (MRL) at age \\\\( x \\\\) is expressed as \\\\( MRL(x) = \\\\frac{\\\\int_x^\\\\infty R(t) \\\\, dt}{R(x)} \\\\). Relationships include \\\\( F(t) = 1 - R(t) \\\\), and the cumulative failure rate \\\\( Z(t) = \\\\int_0^t z(u) \\\\, du \\\\), with \\\\( R(t) = e^{-Z(t)} \\\\). This framework assists in assessing reliability and failure rates across various distributions, crucial in engineering contexts.\", \"The probability density function for failure is defined as \\\\( f(t) = -R'(t) = \\\\frac{0.4}{(0.2t + 1)^3} \\\\), where the survival function is \\\\( R(t) = \\\\frac{1}{(0.2t + 1)^2} \\\\). The failure rate function is \\\\( z(t) = \\\\frac{t}{t + 1} \\\\), capturing the conditional probability of failing in an interval given survival. The Mean Time to Failure (MTTF) is calculated as \\\\( MTTF = \\\\int_0^5 R(t) \\\\, dt = 5 \\\\) months, while the Mean Residual Lifetime (MRL) at age \\\\( x \\\\) is \\\\( MRL(x) = \\\\frac{\\\\int_x^\\\\infty R(t) \\\\, dt}{R(x)} \\\\). The cumulative failure rate is \\\\( Z(t) = \\\\int_0^t z(u) \\\\, du \\\\) with \\\\( R(t) = e^{-Z(t)} \\\\). Additional relationships include \\\\( F(t) = 1 - R(t) \\\\) and \\\\( g(x) = \\\\frac{MTTF}{MRL(x)} \\\\), aiding the reliability assessment across various distributions and phases of failure.\"]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing clusters: 100%|██████████| 40/40 [02:11<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final length of db:  316\n",
      "(check if the seed has same length) :  545\n"
     ]
    }
   ],
   "source": [
    "from src.handler_rag import HandMadeRAG, cosine_similarity\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load the RAG database\n",
    "manager = HandMadeRAG(db_path=\"data/rag_db.json\")\n",
    "seed_list = read_json(\"data/rag_db_seed.json\")\n",
    "\n",
    "# Define similarity threshold and clustering parameters\n",
    "THRESHOLD = 0.87\n",
    "MIN_SAMPLES = 2\n",
    "\n",
    "# Extract embeddings and calculate cosine similarity matrix\n",
    "embeddings = [np.array(entry[\"embedding\"]) for entry in manager.db]\n",
    "similarity_matrix = np.array([\n",
    "    [cosine_similarity(e1, e2) for e2 in embeddings]\n",
    "    for e1 in embeddings\n",
    "])\n",
    "\n",
    "# Filter pairs with similarity above the threshold\n",
    "valid_pairs = [\n",
    "    (i, j) for i in range(len(manager.db)) for j in range(i + 1, len(manager.db))\n",
    "    if similarity_matrix[i, j] > THRESHOLD\n",
    "]\n",
    "\n",
    "# Create clusters based only on valid pairs\n",
    "clusters = []\n",
    "visited = set()\n",
    "\n",
    "for i, j in valid_pairs:\n",
    "    if manager.db[i][\"text\"] in visited and manager.db[j][\"text\"] in visited:\n",
    "        continue\n",
    "    cluster = set()\n",
    "    for x, y in valid_pairs:\n",
    "        if x == i or x == j or y == i or y == j:\n",
    "            cluster.add(manager.db[x][\"text\"])\n",
    "            cluster.add(manager.db[y][\"text\"])\n",
    "    visited.update(cluster)\n",
    "    clusters.append(list(cluster))\n",
    "\n",
    "print(\"Number of clusters: \", len(clusters))\n",
    "print(\"Clusters: \", clusters)\n",
    "\n",
    "merge_prompt = \"\"\"\n",
    "These paragraphs seem related. Merge them into the best single paragraph that retains the most relevant information and ensures clear and concise math formulas and explanations. \n",
    "If any formulas or sections overlap significantly, include only the best or clearest version. Keep the final output under 1000 characters.\n",
    "\"\"\"\n",
    "\n",
    "# Interact with GPT for each cluster\n",
    "\n",
    "for i in tqdm(range(len(clusters)), desc=\"Processing clusters\"):\n",
    "    cluster = clusters[i]\n",
    "    if len(cluster) > 1:\n",
    "        # Combine all texts in the cluster\n",
    "        combined_text = \"\\n\\n\".join(cluster)\n",
    "\n",
    "        # Ask GPT to refine the cluster\n",
    "        merged_paragraph = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": merge_prompt},\n",
    "                {\"role\": \"user\", \"content\": combined_text},\n",
    "            ],\n",
    "        ).choices[0].message.content\n",
    "\n",
    "        for entry in manager.db:\n",
    "            if entry[\"text\"] in cluster:\n",
    "                manager.db.remove(entry)\n",
    "        for entry in seed_list:\n",
    "            if entry in cluster:\n",
    "                seed_list.remove(entry)\n",
    "        # Update the database with the refined paragraph\n",
    "        manager.db.append({\"text\": merged_paragraph, \"embedding\": list(manager._get_embedding(merged_paragraph))})\n",
    "        seed_list.append(merged_paragraph)\n",
    "\n",
    "# Push the updated database\n",
    "print(\"Final length of db: \", len(manager.db))\n",
    "print(\"(check if the seed has same length) : \", len(seed_list))\n",
    "manager.push_to_db()\n",
    "save_json(seed_list, fname=\"data/rag_db_seed.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final length of db:  306\n",
      "(check if the seed has same length) :  306\n"
     ]
    }
   ],
   "source": [
    "db_texts = [entry[\"text\"] for entry in manager.db]\n",
    "\n",
    "while len(seed_list) > len(db_texts):\n",
    "    for el in seed_list:\n",
    "        if el not in db_texts:\n",
    "            seed_list.remove(el)\n",
    "\n",
    "while len(manager.db) > len(seed_list):\n",
    "    for el in manager.db:\n",
    "        if el[\"text\"] not in seed_list:\n",
    "            manager.db.remove(el)\n",
    "\n",
    "save_json(seed_list, fname=\"data/rag_db_seed.json\")\n",
    "manager.push_to_db()\n",
    "\n",
    "print(\"Final length of db: \", len(manager.db))\n",
    "print(\"(check if the seed has same length) : \", len(seed_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output a PDF from the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from textwrap import wrap\n",
    "import json\n",
    "\n",
    "\n",
    "def sanitize_text(text):\n",
    "    \"\"\"Remove unsupported characters from the text.\"\"\"\n",
    "    return \"\".join(\n",
    "        c if ord(c) < 128 else \"?\" for c in text\n",
    "    )  # Replace non-ASCII characters\n",
    "\n",
    "\n",
    "def generate_pdf(data, output_file=\"data/cheatsheet.pdf\"):\n",
    "    c = canvas.Canvas(output_file, pagesize=letter)\n",
    "    width, height = letter\n",
    "\n",
    "    # Margins\n",
    "    margin_x = 50\n",
    "    margin_y = 50\n",
    "    x, y = margin_x, height - margin_y\n",
    "\n",
    "    max_width = width - 2 * margin_x  # Text area width\n",
    "    line_height = 15\n",
    "\n",
    "    def wrap_text(text, max_chars):\n",
    "        return \"\\n\".join(wrap(text, max_chars))\n",
    "\n",
    "    # Loop through strings and add them to the PDF\n",
    "    for text in data:\n",
    "        text = sanitize_text(text)  # Clean the text\n",
    "        # Split title and paragraph\n",
    "        parts = text.split(\"\\n\\n\", 1)\n",
    "        if len(parts) == 2:\n",
    "            title, paragraph = parts\n",
    "        else:\n",
    "            title = parts[0]\n",
    "            paragraph = \"\"\n",
    "\n",
    "        # Add title in bold font\n",
    "        c.setFont(\"Times-Bold\", 14)\n",
    "        title_lines = wrap_text(\n",
    "            title, int(max_width / 7)\n",
    "        )  # Approx char width in points\n",
    "        for line in title_lines.split(\"\\n\"):\n",
    "            c.drawString(x, y, line)\n",
    "            y -= line_height\n",
    "            if y < margin_y:  # Add a new page if needed\n",
    "                c.showPage()\n",
    "                c.setFont(\"Times-Bold\", 14)\n",
    "                y = height - margin_y\n",
    "\n",
    "        # Add paragraph in normal font\n",
    "        c.setFont(\"Times-Roman\", 12)\n",
    "        paragraph_lines = wrap_text(paragraph, int(max_width / 7))\n",
    "        for line in paragraph_lines.split(\"\\n\"):\n",
    "            c.drawString(x, y, line)\n",
    "            y -= line_height\n",
    "            if y < margin_y:  # Add a new page if needed\n",
    "                c.showPage()\n",
    "                c.setFont(\"Times-Roman\", 12)\n",
    "                y = height - margin_y\n",
    "\n",
    "        y -= 30  # Add space after each block\n",
    "\n",
    "    c.save()\n",
    "\n",
    "\n",
    "data_proba = json.load(open(\"data/rag_db_seed.json\"))\n",
    "\n",
    "generate_pdf(data_proba)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
