{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file generates the seed for the rag db / cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanrobert/Documents/CS_2024/ORRA/GAIR/GAIR-riskhive/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from src.utils import save_json, logger, read_json\n",
    "from src.client import openai_client\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the paragraphs that could contain equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 467 entries with equations.\n"
     ]
    }
   ],
   "source": [
    "# Load the file content\n",
    "with open(\"data/RT_textbook.txt\", \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Split content into paragraphs separated by \"\\n\\n\"\n",
    "paragraphs = content.split(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# Function to extract paragraphs containing equations and their context\n",
    "def extract_equation_context(paragraphs):\n",
    "    equation_pattern = re.compile(r\".*\\s=\\s.*\")  # Regex to detect equations\n",
    "    extracted_content = []\n",
    "\n",
    "    for idx, paragraph in enumerate(paragraphs):\n",
    "        if equation_pattern.search(paragraph):  # Detect if the paragraph contains \" = \"\n",
    "            # Get previous, current, and next paragraphs\n",
    "            previous_paragraph = (\n",
    "                paragraphs[idx - 1].strip() if idx > 0 else \"No previous paragraph\"\n",
    "            )\n",
    "            next_paragraph = (\n",
    "                paragraphs[idx + 1].strip()\n",
    "                if idx + 1 < len(paragraphs)\n",
    "                else \"No next paragraph\"\n",
    "            )\n",
    "            extracted_content.append(\n",
    "                previous_paragraph + \"\\n\\n\" + paragraph + \"\\n\\n\" + next_paragraph\n",
    "            )\n",
    "\n",
    "    return extracted_content\n",
    "\n",
    "\n",
    "# Extract paragraphs with equations and their context\n",
    "all_content_raw = extract_equation_context(paragraphs)\n",
    "\n",
    "# Save results to JSON\n",
    "save_json(all_content_raw, fname=\"generated/extracted_rag.json\")\n",
    "\n",
    "logger.info(f\"Extracted {len(all_content_raw)} entries with equations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanitize the output and condense the paragraphs with chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from index 486 to 467 (excluded)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing paragraphs: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Condense the extracted paragraphs\n",
    "import os\n",
    "\n",
    "SYST_PROMPT = \"\"\"\n",
    "The following paragraph is extracted from a reliability textbook, containing equations and their context. It was extracted from a pdf,\n",
    "so some data might not be perfectly organised. I want you to condense the information in this paragraph, to:\n",
    "- make it more concise\n",
    "- clear the equations\n",
    "If you come across an equation, rewrite it in \"humanly understandable\" language, for example: \n",
    "f(x) = x^2 + 2*x + (some formulation of an integral easily understandable)\n",
    "The paragraph has to have the structure : \n",
    "**(Title that summarizes the paragraph)**\n",
    "(Condensed paragraph)\n",
    "Your must make the shortest paragraph possible, without losing the main information. Ideally, make it less than 800 symbols.\n",
    "\"\"\"\n",
    "first_index = 0\n",
    "if os.path.exists(\"generated/condensed_rag.json\"):\n",
    "    condensed_data = read_json(\"generated/condensed_rag.json\")\n",
    "first_index = len(condensed_data)\n",
    "\n",
    "added_length = 110\n",
    "\n",
    "print(\n",
    "    f\"Starting from index {first_index} to {min(first_index + added_length, len(all_content_raw))} (excluded)\"\n",
    ")\n",
    "\n",
    "for i in tqdm(\n",
    "    range(first_index, min(first_index + added_length, len(all_content_raw))),\n",
    "    desc=\"Processing paragraphs\",\n",
    "):\n",
    "    paragraph = all_content_raw[i]\n",
    "    condensed = (\n",
    "        openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYST_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": paragraph},\n",
    "            ],\n",
    "        )\n",
    "        .choices[0]\n",
    "        .message.content\n",
    "    )\n",
    "    condensed_data.append(condensed)\n",
    "\n",
    "save_json(condensed_data, fname=\"generated/condensed_rag.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add the data specific to probabilities and the first handmade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final length 547\n"
     ]
    }
   ],
   "source": [
    "masterclass = read_json(\"generated/condensed_rag.json\")\n",
    "data_proba = read_json(\"data/rag_crafting/probstat_rag.json\")[\"RAG\"]\n",
    "first_data = read_json(\"data/rag_crafting/general_rag_data.json\")\n",
    "for i in range(len(data_proba)):\n",
    "    title = list(data_proba[i].keys())[0]\n",
    "    entry =  list(data_proba[i].values())[0]\n",
    "    new_entry = f\"**{title}**\\n{entry}\"\n",
    "    masterclass.append(new_entry)\n",
    "\n",
    "for i in range(len(first_data)):\n",
    "    new_entry = first_data[i]\n",
    "    masterclass.append(new_entry)\n",
    "\n",
    "save_json(masterclass, fname=\"data/rag_db_seed.json\")\n",
    "print(\"final length\", len(masterclass))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output a PDF from the JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from textwrap import wrap\n",
    "import json\n",
    "\n",
    "\n",
    "def sanitize_text(text):\n",
    "    \"\"\"Remove unsupported characters from the text.\"\"\"\n",
    "    return \"\".join(\n",
    "        c if ord(c) < 128 else \"?\" for c in text\n",
    "    )  # Replace non-ASCII characters\n",
    "\n",
    "\n",
    "def generate_pdf(data, output_file=\"data/cheatsheet.pdf\"):\n",
    "    c = canvas.Canvas(output_file, pagesize=letter)\n",
    "    width, height = letter\n",
    "\n",
    "    # Margins\n",
    "    margin_x = 50\n",
    "    margin_y = 50\n",
    "    x, y = margin_x, height - margin_y\n",
    "\n",
    "    max_width = width - 2 * margin_x  # Text area width\n",
    "    line_height = 15\n",
    "\n",
    "    def wrap_text(text, max_chars):\n",
    "        return \"\\n\".join(wrap(text, max_chars))\n",
    "\n",
    "    # Loop through strings and add them to the PDF\n",
    "    for text in data:\n",
    "        text = sanitize_text(text)  # Clean the text\n",
    "        # Split title and paragraph\n",
    "        parts = text.split(\"\\n\\n\", 1)\n",
    "        if len(parts) == 2:\n",
    "            title, paragraph = parts\n",
    "        else:\n",
    "            title = parts[0]\n",
    "            paragraph = \"\"\n",
    "\n",
    "        # Add title in bold font\n",
    "        c.setFont(\"Times-Bold\", 14)\n",
    "        title_lines = wrap_text(\n",
    "            title, int(max_width / 7)\n",
    "        )  # Approx char width in points\n",
    "        for line in title_lines.split(\"\\n\"):\n",
    "            c.drawString(x, y, line)\n",
    "            y -= line_height\n",
    "            if y < margin_y:  # Add a new page if needed\n",
    "                c.showPage()\n",
    "                c.setFont(\"Times-Bold\", 14)\n",
    "                y = height - margin_y\n",
    "\n",
    "        # Add paragraph in normal font\n",
    "        c.setFont(\"Times-Roman\", 12)\n",
    "        paragraph_lines = wrap_text(paragraph, int(max_width / 7))\n",
    "        for line in paragraph_lines.split(\"\\n\"):\n",
    "            c.drawString(x, y, line)\n",
    "            y -= line_height\n",
    "            if y < margin_y:  # Add a new page if needed\n",
    "                c.showPage()\n",
    "                c.setFont(\"Times-Roman\", 12)\n",
    "                y = height - margin_y\n",
    "\n",
    "        y -= 30  # Add space after each block\n",
    "\n",
    "    c.save()\n",
    "\n",
    "\n",
    "data_proba = json.load(open(\"data/rag_db_seed.json\"))\n",
    "\n",
    "generate_pdf(data_proba)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
