{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.57.1)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (1.6.0)\n",
      "Requirement already satisfied: pydantic in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.10.3)\n",
      "Requirement already satisfied: qdrant-client in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.12.2)\n",
      "Requirement already satisfied: fastembed in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (0.5.0)\n",
      "Requirement already satisfied: pypdf in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (5.1.0)\n",
      "Requirement already satisfied: progressbar2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (4.5.0)\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (3.3.1)\n",
      "Requirement already satisfied: reliability in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.8.16)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./.venv/lib/python3.11/site-packages (from pydantic->-r requirements.txt (line 6)) (2.27.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in ./.venv/lib/python3.11/site-packages (from qdrant-client->-r requirements.txt (line 7)) (1.69.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in ./.venv/lib/python3.11/site-packages (from qdrant-client->-r requirements.txt (line 7)) (1.69.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in ./.venv/lib/python3.11/site-packages (from qdrant-client->-r requirements.txt (line 7)) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in ./.venv/lib/python3.11/site-packages (from qdrant-client->-r requirements.txt (line 7)) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (0.27.1)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (0.7.3)\n",
      "Requirement already satisfied: mmh3<5.0.0,>=4.1.0 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (4.1.0)\n",
      "Requirement already satisfied: onnx>=1.15.0 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (1.17.0)\n",
      "Requirement already satisfied: onnxruntime!=1.20.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (1.20.1)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (10.4.0)\n",
      "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (0.1.3)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1.0,>=0.15 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (0.21.0)\n",
      "Requirement already satisfied: python-utils>=3.8.1 in ./.venv/lib/python3.11/site-packages (from progressbar2->-r requirements.txt (line 10)) (3.9.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 11)) (4.47.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 11)) (2.5.1)\n",
      "Requirement already satisfied: autograd>=1.5 in ./.venv/lib/python3.11/site-packages (from reliability->-r requirements.txt (line 12)) (1.7.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.1 in ./.venv/lib/python3.11/site-packages (from reliability->-r requirements.txt (line 12)) (3.10.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.5.0 in ./.venv/lib/python3.11/site-packages (from reliability->-r requirements.txt (line 12)) (0.5.0)\n",
      "Requirement already satisfied: mplcursors>=0.5.2 in ./.venv/lib/python3.11/site-packages (from reliability->-r requirements.txt (line 12)) (0.6)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in ./.venv/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client->-r requirements.txt (line 7)) (5.29.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client->-r requirements.txt (line 7)) (65.5.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in ./.venv/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client->-r requirements.txt (line 7)) (4.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.20->fastembed->-r requirements.txt (line 8)) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.20->fastembed->-r requirements.txt (line 8)) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.20->fastembed->-r requirements.txt (line 8)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.20->fastembed->-r requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=3.7.1->reliability->-r requirements.txt (line 12)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib>=3.7.1->reliability->-r requirements.txt (line 12)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib>=3.7.1->reliability->-r requirements.txt (line 12)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=3.7.1->reliability->-r requirements.txt (line 12)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=3.7.1->reliability->-r requirements.txt (line 12)) (3.2.1)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.11/site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed->-r requirements.txt (line 8)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.11/site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed->-r requirements.txt (line 8)) (24.12.23)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed->-r requirements.txt (line 8)) (1.13.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0,>=2.31->fastembed->-r requirements.txt (line 8)) (3.4.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 11)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 11)) (3.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 11)) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 11)) (0.5.2)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in ./.venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client->-r requirements.txt (line 7)) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in ./.venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client->-r requirements.txt (line 7)) (4.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed->-r requirements.txt (line 8)) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 11)) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanrobert/Documents/CS_2024/ORRA/GAIR/GAIR-riskhive/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.kaggle_submission import SubmissionBase, test_submission\n",
    "from src.client import openai_client, qdrant_client\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from src.python_agent import PythonAgent\n",
    "from src.handler_rag import QdrantRAG, HandMadeRAG\n",
    "from src.utils import save_json\n",
    "\n",
    "test_questions = pd.read_csv(\"data/test.csv\")\n",
    "train_questions = pd.read_csv(\"data/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V1 - Simple Context Improvement**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "You are an AI expert in reliability engineering. Your task is to answer multiple-choice questions (MCQs) accurately and concisely. Each question will have exactly one correct answer.\n",
    "\n",
    "Instructions:\n",
    "- Read the question and the possible answers.\n",
    "- Identify the single correct answer based on your expertise in reliability engineering.\n",
    "- Respond only with the letter of the correct answer (e.g., a, b, c, or d). Do not provide any explanations or additional text.\n",
    "For example, if you want to say that answer [d] is the right one, you should only retun \"d\".\n",
    "\n",
    "\n",
    "Example usage:\n",
    "Question: Which metric measures the average time between system failures?\n",
    "a. MTTR\n",
    "b. MTBF\n",
    "c. Availability\n",
    "d. Failure Rate\n",
    "\n",
    "Expected response:\n",
    "b\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SimpleContext(SubmissionBase):\n",
    "    def get_1_answer(self, q):\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": context},\n",
    "                {\"role\": \"user\", \"content\": q},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "v1 = SimpleContext(test_questions, openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Prediction 1 for question 1 : d <-- \n",
      " --> Prediction 1 for question 2 : b <-- \n",
      " --> Prediction 1 for question 3 : a <-- \n",
      " --> Prediction 1 for question 4 : d <-- \n",
      " --> Prediction 1 for question 5 : a <-- \n",
      " --> Prediction 1 for question 6 : d <-- \n",
      " --> Prediction 1 for question 7 : b <-- \n",
      " --> Prediction 1 for question 8 : b <-- \n",
      " --> Prediction 1 for question 9 : c <-- \n",
      " --> Prediction 1 for question 10 : d <-- \n",
      " --> Prediction 1 for question 11 : d <-- \n",
      " --> Prediction 1 for question 12 : d <-- \n",
      " --> Prediction 1 for question 13 : b <-- \n",
      " --> Prediction 1 for question 14 : b <-- \n",
      " --> Prediction 1 for question 15 : c <-- \n",
      " --> Prediction 1 for question 16 : b <-- \n",
      " --> Prediction 1 for question 17 : d <-- \n",
      " --> Prediction 1 for question 18 : d <-- \n",
      " --> Prediction 1 for question 19 : a <-- \n",
      " --> Prediction 1 for question 20 : c <-- \n",
      " --> Prediction 1 for question 21 : b <-- \n",
      " --> Prediction 1 for question 22 : c <-- \n",
      " --> Prediction 1 for question 23 : a <-- \n",
      " --> Prediction 1 for question 24 : b <-- \n",
      " --> Prediction 1 for question 25 : a <-- \n",
      "--------------------\n",
      "Score : 0.6 for model SimpleContext\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_submission(v1, fake_multiple_attempts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reasoning with chain of thought**\n",
    "\n",
    "This uses prompt engineering to make the model think and reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a highly knowledgeable AI specializing in reliability engineering, with expertise in industry standards (e.g., MIL-HDBK-217, ARP4761, IEC 61508, ISO 9001) and methodologies (e.g., FMEA, FTA, reliability block diagrams).\n",
    "Your task is to answer multiple-choice questions about reliability engineering with 100% accuracy. Each question has exactly one correct answer.\n",
    "\n",
    "Instructions:\n",
    "Understand the Question and Options\n",
    "\n",
    "Carefully read and analyze the question and all answer choices.\n",
    "Ensure complete comprehension of technical terms, context, and nuances.\n",
    "Engage in Detailed Reasoning (Chain of Thought)\n",
    "\n",
    "Use a step-by-step internal reasoning process to evaluate each answer choice.\n",
    "Reference standard definitions, frameworks, and principles to support your analysis.\n",
    "Eliminate incorrect options by identifying inaccuracies or inconsistencies.\n",
    "Apply ReAct Methodology\n",
    "\n",
    "If you encounter any ambiguities or need additional clarification, address them proactively.\n",
    "Use your expertise to \"act\" on the information given, ensuring a thorough understanding before reaching a conclusion.\n",
    "Explain Your Reasoning Concisely\n",
    "\n",
    "Provide a brief explanation of your thought process to enhance transparency.\n",
    "Keep explanations focused and relevant to the question.\n",
    "End with the Correct Answer\n",
    "\n",
    "Always conclude your response with the phrase: So the answer is: <letter>\n",
    "Replace <letter> with the correct answer (e.g., a, b, c, d).\n",
    "The answer should appear as the last character in your response without any punctuation, brackets, or additional text following it.\n",
    "Example:\n",
    "Question: What is the expected operational time between failures called?\n",
    "\n",
    "Choices: a. MTTR\n",
    "b. MTBF\n",
    "c. Reliability Factor\n",
    "d. Failure Rate\n",
    "\n",
    "Response:\n",
    "\n",
    "Option a (MTTR): Mean Time To Repair refers to the average time required to repair a failed component or system, not the operational time between failures.\n",
    "Option b (MTBF): Mean Time Between Failures represents the expected operational time between inherent failures during normal operation.\n",
    "Option c (Reliability Factor): This is not a standard term used to describe operational time between failures.\n",
    "Option d (Failure Rate): This measures the frequency of failures over a specified period, not the time between them.\n",
    "Conclusion: MTBF accurately describes the expected operational time between failures.\n",
    "So the answer is: b\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class ChainOfThought(SubmissionBase):\n",
    "    messages_to_save = []\n",
    "    def get_1_answer(self, q):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": q},\n",
    "        ]\n",
    "\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0.61,\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "        letter = answer[-1]\n",
    "        messages += [\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "        self.messages_to_save += messages\n",
    "        return letter\n",
    "    \n",
    "    def get_submission(self, save_path = \"generated/submission.csv\", fake_multiple_attempts=False):\n",
    "        self.messages_to_save = []\n",
    "        sub = super().get_submission(save_path, fake_multiple_attempts)\n",
    "        save_json(self.messages_to_save, \"generated/all_messages.json\")\n",
    "        return sub\n",
    "        \n",
    "\n",
    "\n",
    "chain_of_thought = ChainOfThought(test_questions, openai_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Prediction 1 for question 1 : d <-- \n",
      " --> Prediction 1 for question 2 : b <-- \n",
      " --> Prediction 1 for question 3 : a <-- \n",
      " --> Prediction 1 for question 4 : d <-- \n",
      " --> Prediction 1 for question 5 : a <-- \n",
      " --> Prediction 1 for question 6 : d <-- \n",
      " --> Prediction 1 for question 7 : b <-- \n",
      " --> Prediction 1 for question 8 : c <-- \n",
      " --> Prediction 1 for question 9 : c <-- \n",
      " --> Prediction 1 for question 10 : d <-- \n",
      " --> Prediction 1 for question 11 : d <-- \n",
      " --> Prediction 1 for question 12 : d <-- \n",
      " --> Prediction 1 for question 13 : c <-- \n",
      " --> Prediction 1 for question 14 : b <-- \n",
      " --> Prediction 1 for question 15 : b <-- \n",
      " --> Prediction 1 for question 16 : b <-- \n",
      " --> Prediction 1 for question 17 : d <-- \n",
      " --> Prediction 1 for question 18 : d <-- \n",
      " --> Prediction 1 for question 19 : c <-- \n",
      " --> Prediction 1 for question 20 : d <-- \n",
      " --> Prediction 1 for question 21 : c <-- \n",
      " --> Prediction 1 for question 22 : a <-- \n",
      " --> Prediction 1 for question 23 : a <-- \n",
      " --> Prediction 1 for question 24 : a <-- \n",
      " --> Prediction 1 for question 25 : a <-- \n",
      "--------------------\n",
      "Score : 0.68 for model ChainOfThought\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.68)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_submission(chain_of_thought, fake_multiple_attempts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double Prompting**\n",
    "\n",
    "This time we tried to make the model \"doubt\".\n",
    "We first get a zero-shot answer, and then ask the model if it is sure and explain the reasoning steps. This ensures the model does not lose the context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullReasoning(BaseModel):\n",
    "    steps: list[str]\n",
    "    final_answer: Literal[\"a\", \"b\", \"c\", \"d\"]\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = context\n",
    "\n",
    "DOUBT_PROMPT = \"\"\"\n",
    "I have a doubt. Are you totally sure ? Double-check your answer and explain briefly in 2 steps.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DoublePrompting(SubmissionBase):\n",
    "    messages_to_save = []\n",
    "    def get_1_answer(self, q):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": q},\n",
    "        ]\n",
    "\n",
    "        first_response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        messages += [\n",
    "            {\"role\": \"assistant\", \"content\": first_response.choices[0].message.content},\n",
    "            {\"role\": \"user\", \"content\": DOUBT_PROMPT},\n",
    "        ]\n",
    "\n",
    "        response = openai_client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0.61,\n",
    "            response_format=FullReasoning,\n",
    "        )\n",
    "        answer = response.choices[0].message.parsed\n",
    "        messages += [\n",
    "            {\"role\": \"assistant\", \"content\": answer.steps},\n",
    "            {\"role\": \"assistant\", \"content\": answer.final_answer},\n",
    "        ]\n",
    "        self.messages_to_save += messages\n",
    "        return answer.final_answer\n",
    "    \n",
    "    def get_submission(self, save_path = \"generated/submission.csv\", fake_multiple_attempts=False):\n",
    "        self.messages_to_save = []\n",
    "        sub = super().get_submission(save_path, fake_multiple_attempts)\n",
    "        save_json(self.messages_to_save, \"generated/all_messages.json\")\n",
    "        return sub\n",
    "        \n",
    "\n",
    "\n",
    "double_prompting = DoublePrompting(test_questions, openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Prediction 1 for question 1 : d <-- \n",
      " --> Prediction 1 for question 2 : b <-- \n",
      " --> Prediction 1 for question 3 : a <-- \n",
      " --> Prediction 1 for question 4 : d <-- \n",
      " --> Prediction 1 for question 5 : a <-- \n",
      " --> Prediction 1 for question 6 : d <-- \n",
      " --> Prediction 1 for question 7 : b <-- \n",
      " --> Prediction 1 for question 8 : b <-- \n",
      " --> Prediction 1 for question 9 : c <-- \n",
      " --> Prediction 1 for question 10 : d <-- \n",
      " --> Prediction 1 for question 11 : d <-- \n",
      " --> Prediction 1 for question 12 : d <-- \n",
      " --> Prediction 1 for question 13 : b <-- \n",
      " --> Prediction 1 for question 14 : b <-- \n",
      " --> Prediction 1 for question 15 : b <-- \n",
      " --> Prediction 1 for question 16 : b <-- \n",
      " --> Prediction 1 for question 17 : d <-- \n",
      " --> Prediction 1 for question 18 : d <-- \n",
      " --> Prediction 1 for question 19 : b <-- \n",
      " --> Prediction 1 for question 20 : c <-- \n",
      " --> Prediction 1 for question 21 : c <-- \n",
      " --> Prediction 1 for question 22 : c <-- \n",
      " --> Prediction 1 for question 23 : b <-- \n",
      " --> Prediction 1 for question 24 : b <-- \n",
      " --> Prediction 1 for question 25 : a <-- \n",
      "--------------------\n",
      "Score : 0.64 for model DoublePrompting\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_submission(double_prompting, fake_multiple_attempts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiway prompting**\n",
    "\n",
    "This time we initially don't provide the choices to Chat. We first let it think about an answer, and then provide it the MCQ in order to choose the best answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a reliability expert. You will be asked to answer to several questions based on your knowledge and the definitions you know.\n",
    "You will need to explain your reasonning and explain the steps that allowed you to choose your answers.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SimpleAnswer(BaseModel):\n",
    "    choice: Literal[\"a\", \"b\", \"c\", \"d\"]\n",
    "\n",
    "\n",
    "def provide_choices(choices):\n",
    "    return f\"\"\"Based on your previous answer, you should now assess the veracity of each of the following possible answers one by one:\n",
    "{choices}\"\"\"\n",
    "\n",
    "\n",
    "SELECTION_PROMPT = \"\"\"Now please select the 1 possibility that fits bests the initial question.\n",
    "It is possible that none of the possible answer seems acceptable to you. In this case, please choose the one that is the closest to your opinion.\"\"\"\n",
    "\n",
    "\n",
    "class MultiPrompting(SubmissionBase):\n",
    "    def get_1_answer(self, q):\n",
    "        question, choices = q.split(\"[Choices]\")\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "\n",
    "        first_response = (\n",
    "            openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                temperature=0.61,\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "\n",
    "        messages += [\n",
    "            {\"role\": \"assistant\", \"content\": first_response},\n",
    "            {\"role\": \"user\", \"content\": provide_choices(choices)},\n",
    "        ]\n",
    "\n",
    "        chat_opinion = (\n",
    "            openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                temperature=0.61,\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "\n",
    "        messages += [\n",
    "            {\"role\": \"assistant\", \"content\": chat_opinion},\n",
    "            {\"role\": \"user\", \"content\": SELECTION_PROMPT},\n",
    "        ]\n",
    "\n",
    "        return (\n",
    "            openai_client.beta.chat.completions.parse(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                temperature=0.61,\n",
    "                response_format=SimpleAnswer,\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.parsed.choice\n",
    "        )\n",
    "\n",
    "\n",
    "mp = MultiPrompting(test_questions, openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Prediction 1 for question 1 : d\n",
      " --> Prediction 1 for question 2 : d\n",
      " --> Prediction 1 for question 3 : a\n",
      " --> Prediction 1 for question 4 : d\n",
      " --> Prediction 1 for question 5 : a\n",
      " --> Prediction 1 for question 6 : d\n",
      " --> Prediction 1 for question 7 : b\n",
      " --> Prediction 1 for question 8 : c\n",
      " --> Prediction 1 for question 9 : c\n",
      " --> Prediction 1 for question 10 : d\n",
      " --> Prediction 1 for question 11 : d\n",
      " --> Prediction 1 for question 12 : a\n",
      " --> Prediction 1 for question 13 : a\n",
      " --> Prediction 1 for question 14 : b\n",
      " --> Prediction 1 for question 15 : b\n",
      " --> Prediction 1 for question 16 : a\n",
      " --> Prediction 1 for question 17 : d\n",
      " --> Prediction 1 for question 18 : d\n",
      " --> Prediction 1 for question 19 : c\n",
      " --> Prediction 1 for question 20 : b\n",
      " --> Prediction 1 for question 21 : a\n",
      " --> Prediction 1 for question 22 : a\n",
      " --> Prediction 1 for question 23 : a\n",
      " --> Prediction 1 for question 24 : c\n",
      " --> Prediction 1 for question 25 : a\n",
      "--------------------\n",
      "Score : 0.72 for model MultiPrompting\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.72)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_submission(mp, fake_multiple_attempts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agentic system**\n",
    "\n",
    "Now we will give the model the capacity to write and execute Python scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import scipy.stats as stats\n",
      "\n",
      "# Given values\n",
      "mean = 150  # Mean (μ)\n",
      "std_dev = 20  # Standard deviation (σ)\n",
      "percentile = 0.10  # 10th percentile\n",
      "\n",
      "# Find the z-score for the 10th percentile\n",
      "z_score = stats.norm.ppf(percentile)\n",
      "\n",
      "# Calculate B10 life\n",
      "B10_life = mean + z_score * std_dev\n",
      "print(B10_life)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 124.36896868910799\n",
      " --> Prediction 1 for question 1 : b\n",
      " --> Prediction 1 for question 2 : a\n",
      " --> Prediction 1 for question 3 : c\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import scipy.stats as stats\n",
      "\n",
      "# Parameters\n",
      "n = 20  # sample size\n",
      "alpha = 0.05\n",
      "\n",
      "# Degrees of freedom\n",
      "df = n - 1\n",
      "\n",
      "# Chi-squared critical values\n",
      "chi2_lower = stats.chi2.ppf(alpha / 2, df)\n",
      "chi2_upper = stats.chi2.ppf(1 - alpha / 2, df)\n",
      "\n",
      "(chi2_lower, chi2_upper)\n",
      "```\n",
      "The script returned no output. Trying again\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import scipy.stats as stats\n",
      "\n",
      "# Parameters\n",
      "n = 20  # sample size\n",
      "alpha = 0.05\n",
      "\n",
      "# Degrees of freedom\n",
      "df = n - 1\n",
      "\n",
      "# Chi-squared critical values\n",
      "chi2_lower = stats.chi2.ppf(alpha / 2, df)\n",
      "chi2_upper = stats.chi2.ppf(1 - alpha / 2, df)\n",
      "\n",
      "# Sample variance\n",
      "s2 = 4\n",
      "\n",
      "# Confidence interval for population variance\n",
      "lower_bound = (df * s2) / chi2_upper\n",
      "upper_bound = (df * s2) / chi2_lower\n",
      "\n",
      "print(lower_bound, upper_bound)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 2.3133825594720308 8.533078016943891\n",
      " --> Prediction 1 for question 4 : a\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "# Given values\n",
      "mean = 10  # sample mean\n",
      "std_dev = 2  # sample standard deviation\n",
      "n = 50  # sample size\n",
      "Z = 1.96  # Z-score for 95% confidence\n",
      "\n",
      "# Calculate standard error\n",
      "SE = std_dev / math.sqrt(n)\n",
      "\n",
      "# Calculate margin of error\n",
      "ME = Z * SE\n",
      "\n",
      "# Calculate confidence interval\n",
      "lower_bound = mean - ME\n",
      "upper_bound = mean + ME\n",
      "lower_bound, upper_bound\n",
      "```\n",
      "The script returned no output. Trying again\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "# Given values\n",
      "mean = 10  # sample mean\n",
      "std_dev = 2  # sample standard deviation\n",
      "n = 50  # sample size\n",
      "Z = 1.96  # Z-score for 95% confidence\n",
      "\n",
      "# Calculate standard error\n",
      "SE = std_dev / math.sqrt(n)\n",
      "\n",
      "# Calculate margin of error\n",
      "ME = Z * SE\n",
      "\n",
      "# Calculate confidence interval\n",
      "lower_bound = mean - ME\n",
      "upper_bound = mean + ME\n",
      "\n",
      "print(lower_bound, upper_bound)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 9.445628283549746 10.554371716450254\n",
      " --> Prediction 1 for question 5 : d\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import numpy as np\n",
      "\n",
      "# Given data\n",
      "failure_times = np.array([700, 900, 1000, 1100, 1300])\n",
      "shape_parameter = 2\n",
      "n_failures = len(failure_times)\n",
      "\n",
      "# MLE for eta\n",
      "eta_hat = (1/n_failures) * np.sum(failure_times**shape_parameter) * (n_failures / np.sum(failure_times**(shape_parameter - 1)))**(1/shape_parameter)\n",
      "print(eta_hat)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 32887.687665751146\n",
      " --> Prediction 1 for question 6 : a\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "from math import comb\n",
      "\n",
      "R = 0.995  # Reliability\n",
      "F = 1 - R  # Failure probability\n",
      "\n",
      "# Probability of exactly 2 engines functioning\n",
      "P_2 = comb(3, 2) * (R ** 2) * (F ** 1)\n",
      "\n",
      "# Probability of all 3 engines functioning\n",
      "P_3 = comb(3, 3) * (R ** 3)\n",
      "\n",
      "# Total probability of success\n",
      "P_success = P_2 + P_3\n",
      "print(P_success)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.99992525\n",
      " --> Prediction 1 for question 7 : d\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "beta = 0.5\n",
      "eta = 10000\n",
      "t = 8760  # 1 year in hours\n",
      "\n",
      "# Calculate the probability that the component is still operational after 1 year\n",
      "probability_operational = math.exp(-((t / eta) ** beta))\n",
      "print(probability_operational)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.39221358944618695\n",
      " --> Prediction 1 for question 8 : a\n",
      " --> Prediction 1 for question 9 : a\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "# Given probabilities\n",
      "P_C = 0.05  # Probability of having a crack\n",
      "P_S_given_C = 0.98  # Probability of signaling a crack given there is a crack\n",
      "P_S_given_not_C = 0.03  # Probability of signaling a crack given there is no crack\n",
      "\n",
      "# Probability of not having a crack\n",
      "P_not_C = 1 - P_C\n",
      "\n",
      "# Total probability of signaling a crack\n",
      "P_S = (P_S_given_C * P_C) + (P_S_given_not_C * P_not_C)\n",
      "\n",
      "# Bayes' theorem to find the probability that a part has a crack given the signal\n",
      "P_C_given_S = (P_S_given_C * P_C) / P_S\n",
      "\n",
      "print(P_C_given_S)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.6322580645161291\n",
      " --> Prediction 1 for question 10 : c\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "lambda_ = 7\n",
      "\n",
      "# Poisson probability mass function\n",
      "def poisson_pmf(k, lambda_):\n",
      "    return (math.exp(-lambda_) * (lambda_ ** k)) / math.factorial(k)\n",
      "\n",
      "# Calculate P(X < 3)\n",
      "probability_less_than_3 = sum(poisson_pmf(k, lambda_) for k in range(3))\n",
      "\n",
      "print(probability_less_than_3)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.029636163880521777\n",
      " --> Prediction 1 for question 11 : c\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import numpy as np\n",
      "from scipy.stats import weibull_min\n",
      "\n",
      "data = np.array([-309, 229, 386, -104, 180, -217, -167, 168, 122, 138])\n",
      "uncensored_data = data[data > 0]\n",
      "params = weibull_min.fit(uncensored_data, floc=0)\n",
      "beta, loc, scale = params\n",
      "eta = scale\n",
      "t0 = loc\n",
      "beta, eta, t0\n",
      "```\n",
      "The script returned no output. Trying again\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import numpy as np\n",
      "from scipy.stats import weibull_min\n",
      "\n",
      "data = np.array([-309, 229, 386, -104, 180, -217, -167, 168, 122, 138])\n",
      "uncensored_data = data[data > 0]\n",
      "params = weibull_min.fit(uncensored_data, floc=0)\n",
      "beta, loc, scale = params\n",
      "eta = scale\n",
      "t0 = loc\n",
      "print(beta, eta, t0)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 2.446499106633478 230.7777675029722 0\n",
      " --> Prediction 1 for question 12 : c\n",
      " --> Prediction 1 for question 13 : a\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "from scipy.stats import binom\n",
      "\n",
      "# Parameters\n",
      "n = 25  # number of trials\n",
      "p = 0.10  # probability of a part being defective\n",
      "k = 6  # we want cumulative probability up to 6 defective parts\n",
      "\n",
      "# Cumulative probability of 6 or fewer defective parts\n",
      "P_X_leq_6 = binom.cdf(k, n, p)\n",
      "\n",
      "# Probability of 7 or more defective parts\n",
      "P_X_geq_7 = 1 - P_X_leq_6\n",
      "\n",
      "P_X_geq_7\n",
      "```\n",
      "The script returned no output. Trying again\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "from scipy.stats import binom\n",
      "\n",
      "# Parameters\n",
      "n = 25  # number of trials\n",
      "p = 0.10  # probability of a part being defective\n",
      "k = 6  # we want cumulative probability up to 6 defective parts\n",
      "\n",
      "# Cumulative probability of 6 or fewer defective parts\n",
      "P_X_leq_6 = binom.cdf(k, n, p)\n",
      "\n",
      "# Probability of 7 or more defective parts\n",
      "P_X_geq_7 = 1 - P_X_leq_6\n",
      "\n",
      "print(P_X_geq_7)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.009476360691506591\n",
      " --> Prediction 1 for question 14 : c\n",
      " --> Prediction 1 for question 15 : b\n",
      " --> Prediction 1 for question 16 : a\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import numpy as np\n",
      "from scipy.stats import chi2\n",
      "\n",
      "# Failure times\n",
      "failure_times = [632, 3450, 816, 928, 150]\n",
      "n = len(failure_times)\n",
      "\n",
      "# Calculate MTTF\n",
      "mttf = np.mean(failure_times)\n",
      "\n",
      "# Calculate chi-square values\n",
      "alpha = 0.1\n",
      "chi2_lower = chi2.ppf(alpha/2, 2*n)\n",
      "chi2_upper = chi2.ppf(1-alpha/2, 2*n)\n",
      "\n",
      "# Calculate confidence bounds\n",
      "lower_bound = (n / chi2_upper) * mttf\n",
      "upper_bound = (n / chi2_lower) * mttf\n",
      "\n",
      "lower_bound, upper_bound\n",
      "```\n",
      "The script returned no output. Trying again\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import numpy as np\n",
      "from scipy.stats import chi2\n",
      "\n",
      "# Failure times\n",
      "failure_times = [632, 3450, 816, 928, 150]\n",
      "n = len(failure_times)\n",
      "\n",
      "# Calculate MTTF\n",
      "mttf = np.mean(failure_times)\n",
      "\n",
      "# Calculate chi-square values\n",
      "alpha = 0.1\n",
      "chi2_lower = chi2.ppf(alpha/2, 2*n)\n",
      "chi2_upper = chi2.ppf(1-alpha/2, 2*n)\n",
      "\n",
      "# Calculate confidence bounds\n",
      "lower_bound = (n / chi2_upper) * mttf\n",
      "upper_bound = (n / chi2_lower) * mttf\n",
      "\n",
      "print(lower_bound, upper_bound)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 326.43183362646084 1516.6361216640962\n",
      " --> Prediction 1 for question 17 : c\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "MTTF = -100 / math.log(0.99)\n",
      "print(MTTF)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 9949.916247342207\n",
      " --> Prediction 1 for question 18 : b\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "# Given values\n",
      "R_t = 0.95  # reliability for one month\n",
      "t = 1       # time in months\n",
      "\n",
      "# Calculate the failure rate (lambda)\n",
      "lambda_ = -math.log(R_t) / t\n",
      "\n",
      "# Calculate the mean number of failures in one year\n",
      "mu = lambda_ * 12\n",
      "\n",
      "# Calculate P(X <= 2)\n",
      "P_X_le_2 = (math.exp(-mu) * (mu**0) / math.factorial(0) +\n",
      "             math.exp(-mu) * (mu**1) / math.factorial(1) +\n",
      "             math.exp(-mu) * (mu**2) / math.factorial(2))\n",
      "\n",
      "# Calculate P(X > 2)\n",
      "P_X_gt_2 = 1 - P_X_le_2\n",
      "\n",
      "# Print the result\n",
      "print(P_X_gt_2)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.02467615186688754\n",
      " --> Prediction 1 for question 19 : b\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "# Given values\n",
      "lambda_total = 1  # total failure rate\n",
      "t = 6  # time in months\n",
      "k = 2  # number of parts\n",
      "\n",
      "# Calculating P(X <= t)\n",
      "prob_X_le_t = 1 - math.exp(-lambda_total * t) * (1 + lambda_total * t)\n",
      "\n",
      "# Therefore, P(X > t)\n",
      "prob_X_gt_t = 1 - prob_X_le_t\n",
      "print(prob_X_gt_t)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.017351265236664526\n",
      " --> Prediction 1 for question 20 : c\n",
      " --> Prediction 1 for question 21 : c\n",
      " --> Prediction 1 for question 22 : a\n",
      " --> Prediction 1 for question 23 : a\n",
      " --> Prediction 1 for question 24 : d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prediction_1</th>\n",
       "      <th>prediction_2</th>\n",
       "      <th>prediction_3</th>\n",
       "      <th>prediction_4</th>\n",
       "      <th>prediction_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_id prediction_1 prediction_2 prediction_3 prediction_4  \\\n",
       "0             1            b            b            b            b   \n",
       "1             2            a            a            a            a   \n",
       "2             3            c            c            c            c   \n",
       "3             4            a            a            a            a   \n",
       "4             5            d            d            d            d   \n",
       "5             6            a            a            a            a   \n",
       "6             7            d            d            d            d   \n",
       "7             8            a            a            a            a   \n",
       "8             9            a            a            a            a   \n",
       "9            10            c            c            c            c   \n",
       "10           11            c            c            c            c   \n",
       "11           12            c            c            c            c   \n",
       "12           13            a            a            a            a   \n",
       "13           14            c            c            c            c   \n",
       "14           15            b            b            b            b   \n",
       "15           16            a            a            a            a   \n",
       "16           17            c            c            c            c   \n",
       "17           18            b            b            b            b   \n",
       "18           19            b            b            b            b   \n",
       "19           20            c            c            c            c   \n",
       "20           21            c            c            c            c   \n",
       "21           22            a            a            a            a   \n",
       "22           23            a            a            a            a   \n",
       "23           24            d            d            d            d   \n",
       "\n",
       "   prediction_5  \n",
       "0             b  \n",
       "1             a  \n",
       "2             c  \n",
       "3             a  \n",
       "4             d  \n",
       "5             a  \n",
       "6             d  \n",
       "7             a  \n",
       "8             a  \n",
       "9             c  \n",
       "10            c  \n",
       "11            c  \n",
       "12            a  \n",
       "13            c  \n",
       "14            b  \n",
       "15            a  \n",
       "16            c  \n",
       "17            b  \n",
       "18            b  \n",
       "19            c  \n",
       "20            c  \n",
       "21            a  \n",
       "22            a  \n",
       "23            d  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PythonAgent.inject_python(openai_client=openai_client)\n",
    "test_submission(double_prompting, fake_multiple_attempts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG prompting**\n",
    "\n",
    "We now augment the knowledge of our model using Retrieval Augmented Generation. We built a database containing specific information about reliability engineering, and will use it to augment our prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "||                               ||\n",
      "||          _________            ||\n",
      "||         |         |           ||\n",
      "||         |R I S K  |           ||\n",
      "||         |  H I V E|           ||\n",
      "||         |_________|           ||\n",
      "||                               ||\n",
      "===================================\n",
      "[STATUS]: All systems up. Ready to analyze some risk!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.utils import init_string\n",
    "\n",
    "use_qdrant = False\n",
    "\n",
    "if use_qdrant:\n",
    "    rag = QdrantRAG(rag_path=\"data/rag_db_seed.json\", qdrant_client=qdrant_client)\n",
    "else:\n",
    "    rag = HandMadeRAG(db_path=\"data/rag_db.json\", openai_client=openai_client)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 1200\n",
    "\n",
    "\n",
    "class RiskHive(ChainOfThought):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.augment_max_length = MAX_CONTEXT_LENGTH\n",
    "        init_string()\n",
    "\n",
    "    def inject_context(self, q: str):\n",
    "        question, _ = q.split(\"[Choices]\")\n",
    "        search = rag.search(question, limit=1)\n",
    "        injected_context = \"\"\n",
    "        for idx, el in enumerate(search):\n",
    "            injected_context += f\"{idx+1} : {el}\\n\\n\"\n",
    "        return f\"{q}\\n\\n Here is some information that could help:  ```{injected_context[0:self.augment_max_length]}(...)```\"\n",
    "\n",
    "    def get_1_answer(self, q):\n",
    "        return super().get_1_answer(self.inject_context(q))\n",
    "\n",
    "\n",
    "rh = RiskHive(test_questions, openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Prediction 1 for question 1 : d <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy.stats import weibull_min\n",
      "from scipy.special import gamma\n",
      "\n",
      "# Given data\n",
      "failure_times = [42]  # Failure time for the failed item\n",
      "censored_times = [50] * 4  # Censored times for the remaining items\n",
      "all_times = failure_times + censored_times\n",
      "\n",
      "# Parameters for Weibull distribution\n",
      "beta = 2.2\n",
      "n = len(all_times)\n",
      "\n",
      "# MLE for η calculation\n",
      "eta_hat = (np.mean(np.array(all_times) ** beta)) ** (1 / beta)\n",
      "\n",
      "# Standard error calculation for η\n",
      "# Using the variance formula for Weibull distribution\n",
      "variance = (eta_hat ** 2) * (gamma(1 + 2/beta) - (gamma(1 + 1/beta) ** 2))\n",
      "standard_error = np.sqrt(variance / n)\n",
      "\n",
      "# Z value for 95% confidence\n",
      "z_alpha = 1.96  # For 95% confidence\n",
      "\n",
      "# Lower confidence limit\n",
      "lower_confidence_limit = eta_hat - z_alpha * standard_error\n",
      "lower_confidence_limit\n",
      "print(lower_confidence_limit)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 30.450775094060177\n",
      " --> Prediction 1 for question 2 : d <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Given values\n",
      "n = 50  # number of tests performed\n",
      "R = 0.90  # required reliability\n",
      "CL_required = 0.80  # required confidence level\n",
      "\n",
      "# Calculate the confidence level achieved with the given number of tests\n",
      "CL_achieved = 1 - (1 - R)**n\n",
      "\n",
      "CL_achieved\n",
      "print(CL_achieved)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 1.0\n",
      " --> Prediction 1 for question 3 : a <-- \n",
      " --> Prediction 1 for question 4 : d <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "# Given data\n",
      "P_failure = 0.01  # Probability of failure for each system\n",
      "P_success = 1 - P_failure  # Probability of success for each system\n",
      "\n",
      "# Since the systems operate independently, the probability of both failing is:\n",
      "P_both_fail = P_failure * P_failure\n",
      "\n",
      "# Therefore, the probability of at least one functioning properly is:\n",
      "P_at_least_one_success = 1 - P_both_fail\n",
      "\n",
      "P_at_least_one_success\n",
      "print(P_at_least_one_success)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.9999\n",
      " --> Prediction 1 for question 5 : a <-- \n",
      " --> Prediction 1 for question 6 : d <-- \n",
      " --> Prediction 1 for question 7 : b <-- \n",
      " --> Prediction 1 for question 8 : c <-- \n",
      " --> Prediction 1 for question 9 : c <-- \n",
      " --> Prediction 1 for question 10 : d <-- \n",
      " --> Prediction 1 for question 11 : d <-- \n"
     ]
    }
   ],
   "source": [
    "PythonAgent.inject_python(openai_client=openai_client)\n",
    "test_submission(rh, fake_multiple_attempts=True)\n",
    "# rh.get_submission(save_path=\"generated/risk_hive_submission.csv\", fake_multiple_attempts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
