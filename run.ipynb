{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.57.1)\n",
      "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.0.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (1.6.0)\n",
      "Requirement already satisfied: pydantic in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.10.3)\n",
      "Requirement already satisfied: qdrant-client in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (1.12.2)\n",
      "Requirement already satisfied: fastembed in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (0.5.0)\n",
      "Requirement already satisfied: pypdf in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (5.1.0)\n",
      "Requirement already satisfied: progressbar2 in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (4.5.0)\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (3.3.1)\n",
      "Requirement already satisfied: reliability in ./.venv/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.8.16)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./.venv/lib/python3.11/site-packages (from openai->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 3)) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 5)) (3.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.11/site-packages (from pydantic->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./.venv/lib/python3.11/site-packages (from pydantic->-r requirements.txt (line 6)) (2.27.1)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in ./.venv/lib/python3.11/site-packages (from qdrant-client->-r requirements.txt (line 7)) (1.69.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in ./.venv/lib/python3.11/site-packages (from qdrant-client->-r requirements.txt (line 7)) (1.69.0)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in ./.venv/lib/python3.11/site-packages (from qdrant-client->-r requirements.txt (line 7)) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in ./.venv/lib/python3.11/site-packages (from qdrant-client->-r requirements.txt (line 7)) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (0.27.1)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (0.7.3)\n",
      "Requirement already satisfied: mmh3<5.0.0,>=4.1.0 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (4.1.0)\n",
      "Requirement already satisfied: onnx>=1.15.0 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (1.17.0)\n",
      "Requirement already satisfied: onnxruntime!=1.20.0,>=1.17.0 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (1.20.1)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (10.4.0)\n",
      "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (0.1.3)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1.0,>=0.15 in ./.venv/lib/python3.11/site-packages (from fastembed->-r requirements.txt (line 8)) (0.21.0)\n",
      "Requirement already satisfied: python-utils>=3.8.1 in ./.venv/lib/python3.11/site-packages (from progressbar2->-r requirements.txt (line 10)) (3.9.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 11)) (4.47.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.11/site-packages (from sentence-transformers->-r requirements.txt (line 11)) (2.5.1)\n",
      "Requirement already satisfied: autograd>=1.5 in ./.venv/lib/python3.11/site-packages (from reliability->-r requirements.txt (line 12)) (1.7.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.1 in ./.venv/lib/python3.11/site-packages (from reliability->-r requirements.txt (line 12)) (3.10.0)\n",
      "Requirement already satisfied: autograd-gamma>=0.5.0 in ./.venv/lib/python3.11/site-packages (from reliability->-r requirements.txt (line 12)) (0.5.0)\n",
      "Requirement already satisfied: mplcursors>=0.5.2 in ./.venv/lib/python3.11/site-packages (from reliability->-r requirements.txt (line 12)) (0.6)\n",
      "Requirement already satisfied: idna>=2.8 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in ./.venv/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client->-r requirements.txt (line 7)) (5.29.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (from grpcio-tools>=1.41.0->qdrant-client->-r requirements.txt (line 7)) (65.5.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in ./.venv/lib/python3.11/site-packages (from httpx[http2]>=0.20.0->qdrant-client->-r requirements.txt (line 7)) (4.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.20->fastembed->-r requirements.txt (line 8)) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.20->fastembed->-r requirements.txt (line 8)) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.20->fastembed->-r requirements.txt (line 8)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.20->fastembed->-r requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=3.7.1->reliability->-r requirements.txt (line 12)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.11/site-packages (from matplotlib>=3.7.1->reliability->-r requirements.txt (line 12)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.11/site-packages (from matplotlib>=3.7.1->reliability->-r requirements.txt (line 12)) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=3.7.1->reliability->-r requirements.txt (line 12)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.11/site-packages (from matplotlib>=3.7.1->reliability->-r requirements.txt (line 12)) (3.2.1)\n",
      "Requirement already satisfied: coloredlogs in ./.venv/lib/python3.11/site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed->-r requirements.txt (line 8)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./.venv/lib/python3.11/site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed->-r requirements.txt (line 8)) (24.12.23)\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.11/site-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed->-r requirements.txt (line 8)) (1.13.1)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 3)) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3.0,>=2.31->fastembed->-r requirements.txt (line 8)) (3.4.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 11)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 11)) (3.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.11/site-packages (from sympy->onnxruntime!=1.20.0,>=1.17.0->fastembed->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 11)) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 11)) (0.5.2)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in ./.venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client->-r requirements.txt (line 7)) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in ./.venv/lib/python3.11/site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client->-r requirements.txt (line 7)) (4.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./.venv/lib/python3.11/site-packages (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed->-r requirements.txt (line 8)) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 11)) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivanrobert/Documents/CS_2024/ORRA/GAIR/GAIR-riskhive/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.kaggle_submission import SubmissionBase, test_submission\n",
    "from src.client import openai_client, qdrant_client\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "from src.python_agent import PythonAgent\n",
    "from src.handler_rag import QdrantRAG\n",
    "from src.utils import save_json\n",
    "\n",
    "test_questions = pd.read_csv(\"data/test.csv\")\n",
    "train_questions = pd.read_csv(\"data/train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**V1 - Simple Context Improvement**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "You are an AI expert in reliability engineering. Your task is to answer multiple-choice questions (MCQs) accurately and concisely. Each question will have exactly one correct answer.\n",
    "\n",
    "Instructions:\n",
    "- Read the question and the possible answers.\n",
    "- Identify the single correct answer based on your expertise in reliability engineering.\n",
    "- Respond only with the letter of the correct answer (e.g., a, b, c, or d). Do not provide any explanations or additional text.\n",
    "For example, if you want to say that answer [d] is the right one, you should only retun \"d\".\n",
    "\n",
    "\n",
    "Example usage:\n",
    "Question: Which metric measures the average time between system failures?\n",
    "a. MTTR\n",
    "b. MTBF\n",
    "c. Availability\n",
    "d. Failure Rate\n",
    "\n",
    "Expected response:\n",
    "b\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SimpleContext(SubmissionBase):\n",
    "    def get_1_answer(self, q):\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": context},\n",
    "                {\"role\": \"user\", \"content\": q},\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "\n",
    "v1 = SimpleContext(test_questions, openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Prediction 1 for question 1 : d <-- \n",
      " --> Prediction 1 for question 2 : b <-- \n",
      " --> Prediction 1 for question 3 : a <-- \n",
      " --> Prediction 1 for question 4 : d <-- \n",
      " --> Prediction 1 for question 5 : a <-- \n",
      " --> Prediction 1 for question 6 : d <-- \n",
      " --> Prediction 1 for question 7 : b <-- \n",
      " --> Prediction 1 for question 8 : b <-- \n",
      " --> Prediction 1 for question 9 : c <-- \n",
      " --> Prediction 1 for question 10 : d <-- \n",
      " --> Prediction 1 for question 11 : d <-- \n",
      " --> Prediction 1 for question 12 : d <-- \n",
      " --> Prediction 1 for question 13 : b <-- \n",
      " --> Prediction 1 for question 14 : b <-- \n",
      " --> Prediction 1 for question 15 : c <-- \n",
      " --> Prediction 1 for question 16 : b <-- \n",
      " --> Prediction 1 for question 17 : d <-- \n",
      " --> Prediction 1 for question 18 : d <-- \n",
      " --> Prediction 1 for question 19 : a <-- \n",
      " --> Prediction 1 for question 20 : c <-- \n",
      " --> Prediction 1 for question 21 : b <-- \n",
      " --> Prediction 1 for question 22 : c <-- \n",
      " --> Prediction 1 for question 23 : a <-- \n",
      " --> Prediction 1 for question 24 : b <-- \n",
      " --> Prediction 1 for question 25 : a <-- \n",
      "--------------------\n",
      "Score : 0.6 for model SimpleContext\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_submission(v1, fake_multiple_attempts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Double Prompting**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullReasoning(BaseModel):\n",
    "    steps: list[str]\n",
    "    final_answer: Literal[\"a\", \"b\", \"c\", \"d\"]\n",
    "\n",
    "\n",
    "SYSTEM_PROMPT = context\n",
    "\n",
    "DOUBT_PROMPT = \"\"\"\n",
    "I have a doubt. Are you totally sure ? Double-check your answer and explain briefly in 2 steps.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class DoublePrompting(SubmissionBase):\n",
    "    messages_to_save = []\n",
    "    def get_1_answer(self, q):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": q},\n",
    "        ]\n",
    "\n",
    "        first_response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0.7,\n",
    "        )\n",
    "\n",
    "        messages += [\n",
    "            {\"role\": \"assistant\", \"content\": first_response.choices[0].message.content},\n",
    "            {\"role\": \"user\", \"content\": DOUBT_PROMPT},\n",
    "        ]\n",
    "\n",
    "        response = openai_client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=messages,\n",
    "            temperature=0.25,\n",
    "            response_format=FullReasoning,\n",
    "        )\n",
    "        answer = response.choices[0].message.parsed\n",
    "        messages += [\n",
    "            {\"role\": \"assistant\", \"content\": answer.steps},\n",
    "            {\"role\": \"assistant\", \"content\": answer.final_answer},\n",
    "        ]\n",
    "        self.messages_to_save += messages\n",
    "        return answer.final_answer\n",
    "    \n",
    "    def get_submission(self, save_path = \"generated/submission.csv\", fake_multiple_attempts=False):\n",
    "        self.messages_to_save = []\n",
    "        sub = super().get_submission(save_path, fake_multiple_attempts)\n",
    "        save_json(self.messages_to_save, \"generated/all_messages.json\")\n",
    "        return sub\n",
    "        \n",
    "\n",
    "\n",
    "double_prompting = DoublePrompting(test_questions, openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Prediction 1 for question 1 : d\n",
      " --> Prediction 1 for question 2 : b\n",
      " --> Prediction 1 for question 3 : a\n",
      " --> Prediction 1 for question 4 : d\n",
      " --> Prediction 1 for question 5 : a\n",
      " --> Prediction 1 for question 6 : d\n",
      " --> Prediction 1 for question 7 : b\n",
      " --> Prediction 1 for question 8 : b\n",
      " --> Prediction 1 for question 9 : c\n",
      " --> Prediction 1 for question 10 : c\n",
      " --> Prediction 1 for question 11 : d\n",
      " --> Prediction 1 for question 12 : d\n",
      " --> Prediction 1 for question 13 : b\n",
      " --> Prediction 1 for question 14 : b\n",
      " --> Prediction 1 for question 15 : b\n",
      " --> Prediction 1 for question 16 : b\n",
      " --> Prediction 1 for question 17 : d\n",
      " --> Prediction 1 for question 18 : d\n",
      " --> Prediction 1 for question 19 : c\n",
      " --> Prediction 1 for question 20 : c\n",
      " --> Prediction 1 for question 21 : c\n",
      " --> Prediction 1 for question 22 : c\n",
      " --> Prediction 1 for question 23 : a\n",
      " --> Prediction 1 for question 24 : b\n",
      " --> Prediction 1 for question 25 : a\n",
      "--------------------\n",
      "Score : 0.6 for model DoublePrompting\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_submission(double_prompting, fake_multiple_attempts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multiway prompting**\n",
    "\n",
    "This time we initially don't provide the choices to Chat. We first let it think about an answer, and then provide it the MCQ in order to choose the best answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a reliability expert. You will be asked to answer to several questions based on your knowledge and the definitions you know.\n",
    "You will need to explain your reasonning and explain the steps that allowed you to choose your answers.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class SimpleAnswer(BaseModel):\n",
    "    choice: Literal[\"a\", \"b\", \"c\", \"d\"]\n",
    "\n",
    "\n",
    "def provide_choices(choices):\n",
    "    return f\"\"\"Based on your previous answer, you should now assess the veracity of each of the following possible answers one by one:\n",
    "{choices}\"\"\"\n",
    "\n",
    "\n",
    "SELECTION_PROMPT = \"\"\"Now please select the 1 possibility that fits bests the initial question.\n",
    "It is possible that none of the possible answer seems acceptable to you. In this case, please choose the one that is the closest to your opinion.\"\"\"\n",
    "\n",
    "\n",
    "class MultiPrompting(SubmissionBase):\n",
    "    def get_1_answer(self, q):\n",
    "        question, choices = q.split(\"[Choices]\")\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "\n",
    "        first_response = (\n",
    "            openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                temperature=0.61,\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "\n",
    "        messages += [\n",
    "            {\"role\": \"assistant\", \"content\": first_response},\n",
    "            {\"role\": \"user\", \"content\": provide_choices(choices)},\n",
    "        ]\n",
    "\n",
    "        chat_opinion = (\n",
    "            openai_client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                temperature=0.61,\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "\n",
    "        messages += [\n",
    "            {\"role\": \"assistant\", \"content\": chat_opinion},\n",
    "            {\"role\": \"user\", \"content\": SELECTION_PROMPT},\n",
    "        ]\n",
    "\n",
    "        return (\n",
    "            openai_client.beta.chat.completions.parse(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages,\n",
    "                temperature=0.61,\n",
    "                response_format=SimpleAnswer,\n",
    "            )\n",
    "            .choices[0]\n",
    "            .message.parsed.choice\n",
    "        )\n",
    "\n",
    "\n",
    "mp = MultiPrompting(test_questions, openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " --> Prediction 1 for question 1 : d\n",
      " --> Prediction 1 for question 2 : d\n",
      " --> Prediction 1 for question 3 : a\n",
      " --> Prediction 1 for question 4 : d\n",
      " --> Prediction 1 for question 5 : a\n",
      " --> Prediction 1 for question 6 : d\n",
      " --> Prediction 1 for question 7 : b\n",
      " --> Prediction 1 for question 8 : c\n",
      " --> Prediction 1 for question 9 : c\n",
      " --> Prediction 1 for question 10 : d\n",
      " --> Prediction 1 for question 11 : d\n",
      " --> Prediction 1 for question 12 : a\n",
      " --> Prediction 1 for question 13 : a\n",
      " --> Prediction 1 for question 14 : b\n",
      " --> Prediction 1 for question 15 : b\n",
      " --> Prediction 1 for question 16 : a\n",
      " --> Prediction 1 for question 17 : d\n",
      " --> Prediction 1 for question 18 : d\n",
      " --> Prediction 1 for question 19 : c\n",
      " --> Prediction 1 for question 20 : b\n",
      " --> Prediction 1 for question 21 : a\n",
      " --> Prediction 1 for question 22 : a\n",
      " --> Prediction 1 for question 23 : a\n",
      " --> Prediction 1 for question 24 : c\n",
      " --> Prediction 1 for question 25 : a\n",
      "--------------------\n",
      "Score : 0.72 for model MultiPrompting\n",
      "--------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.72)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_submission(mp, fake_multiple_attempts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agentic system**\n",
    "\n",
    "Now we will give the model the capacity to write and execute Python scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import scipy.stats as stats\n",
      "\n",
      "# Given values\n",
      "mean = 150  # Mean (μ)\n",
      "std_dev = 20  # Standard deviation (σ)\n",
      "percentile = 0.10  # 10th percentile\n",
      "\n",
      "# Find the z-score for the 10th percentile\n",
      "z_score = stats.norm.ppf(percentile)\n",
      "\n",
      "# Calculate B10 life\n",
      "B10_life = mean + z_score * std_dev\n",
      "print(B10_life)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 124.36896868910799\n",
      " --> Prediction 1 for question 1 : b\n",
      " --> Prediction 1 for question 2 : a\n",
      " --> Prediction 1 for question 3 : c\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import scipy.stats as stats\n",
      "\n",
      "# Parameters\n",
      "n = 20  # sample size\n",
      "alpha = 0.05\n",
      "\n",
      "# Degrees of freedom\n",
      "df = n - 1\n",
      "\n",
      "# Chi-squared critical values\n",
      "chi2_lower = stats.chi2.ppf(alpha / 2, df)\n",
      "chi2_upper = stats.chi2.ppf(1 - alpha / 2, df)\n",
      "\n",
      "(chi2_lower, chi2_upper)\n",
      "```\n",
      "The script returned no output. Trying again\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import scipy.stats as stats\n",
      "\n",
      "# Parameters\n",
      "n = 20  # sample size\n",
      "alpha = 0.05\n",
      "\n",
      "# Degrees of freedom\n",
      "df = n - 1\n",
      "\n",
      "# Chi-squared critical values\n",
      "chi2_lower = stats.chi2.ppf(alpha / 2, df)\n",
      "chi2_upper = stats.chi2.ppf(1 - alpha / 2, df)\n",
      "\n",
      "# Sample variance\n",
      "s2 = 4\n",
      "\n",
      "# Confidence interval for population variance\n",
      "lower_bound = (df * s2) / chi2_upper\n",
      "upper_bound = (df * s2) / chi2_lower\n",
      "\n",
      "print(lower_bound, upper_bound)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 2.3133825594720308 8.533078016943891\n",
      " --> Prediction 1 for question 4 : a\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "# Given values\n",
      "mean = 10  # sample mean\n",
      "std_dev = 2  # sample standard deviation\n",
      "n = 50  # sample size\n",
      "Z = 1.96  # Z-score for 95% confidence\n",
      "\n",
      "# Calculate standard error\n",
      "SE = std_dev / math.sqrt(n)\n",
      "\n",
      "# Calculate margin of error\n",
      "ME = Z * SE\n",
      "\n",
      "# Calculate confidence interval\n",
      "lower_bound = mean - ME\n",
      "upper_bound = mean + ME\n",
      "lower_bound, upper_bound\n",
      "```\n",
      "The script returned no output. Trying again\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "# Given values\n",
      "mean = 10  # sample mean\n",
      "std_dev = 2  # sample standard deviation\n",
      "n = 50  # sample size\n",
      "Z = 1.96  # Z-score for 95% confidence\n",
      "\n",
      "# Calculate standard error\n",
      "SE = std_dev / math.sqrt(n)\n",
      "\n",
      "# Calculate margin of error\n",
      "ME = Z * SE\n",
      "\n",
      "# Calculate confidence interval\n",
      "lower_bound = mean - ME\n",
      "upper_bound = mean + ME\n",
      "\n",
      "print(lower_bound, upper_bound)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 9.445628283549746 10.554371716450254\n",
      " --> Prediction 1 for question 5 : d\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import numpy as np\n",
      "\n",
      "# Given data\n",
      "failure_times = np.array([700, 900, 1000, 1100, 1300])\n",
      "shape_parameter = 2\n",
      "n_failures = len(failure_times)\n",
      "\n",
      "# MLE for eta\n",
      "eta_hat = (1/n_failures) * np.sum(failure_times**shape_parameter) * (n_failures / np.sum(failure_times**(shape_parameter - 1)))**(1/shape_parameter)\n",
      "print(eta_hat)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 32887.687665751146\n",
      " --> Prediction 1 for question 6 : a\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "from math import comb\n",
      "\n",
      "R = 0.995  # Reliability\n",
      "F = 1 - R  # Failure probability\n",
      "\n",
      "# Probability of exactly 2 engines functioning\n",
      "P_2 = comb(3, 2) * (R ** 2) * (F ** 1)\n",
      "\n",
      "# Probability of all 3 engines functioning\n",
      "P_3 = comb(3, 3) * (R ** 3)\n",
      "\n",
      "# Total probability of success\n",
      "P_success = P_2 + P_3\n",
      "print(P_success)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.99992525\n",
      " --> Prediction 1 for question 7 : d\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "beta = 0.5\n",
      "eta = 10000\n",
      "t = 8760  # 1 year in hours\n",
      "\n",
      "# Calculate the probability that the component is still operational after 1 year\n",
      "probability_operational = math.exp(-((t / eta) ** beta))\n",
      "print(probability_operational)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.39221358944618695\n",
      " --> Prediction 1 for question 8 : a\n",
      " --> Prediction 1 for question 9 : a\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "# Given probabilities\n",
      "P_C = 0.05  # Probability of having a crack\n",
      "P_S_given_C = 0.98  # Probability of signaling a crack given there is a crack\n",
      "P_S_given_not_C = 0.03  # Probability of signaling a crack given there is no crack\n",
      "\n",
      "# Probability of not having a crack\n",
      "P_not_C = 1 - P_C\n",
      "\n",
      "# Total probability of signaling a crack\n",
      "P_S = (P_S_given_C * P_C) + (P_S_given_not_C * P_not_C)\n",
      "\n",
      "# Bayes' theorem to find the probability that a part has a crack given the signal\n",
      "P_C_given_S = (P_S_given_C * P_C) / P_S\n",
      "\n",
      "print(P_C_given_S)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.6322580645161291\n",
      " --> Prediction 1 for question 10 : c\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "lambda_ = 7\n",
      "\n",
      "# Poisson probability mass function\n",
      "def poisson_pmf(k, lambda_):\n",
      "    return (math.exp(-lambda_) * (lambda_ ** k)) / math.factorial(k)\n",
      "\n",
      "# Calculate P(X < 3)\n",
      "probability_less_than_3 = sum(poisson_pmf(k, lambda_) for k in range(3))\n",
      "\n",
      "print(probability_less_than_3)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.029636163880521777\n",
      " --> Prediction 1 for question 11 : c\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import numpy as np\n",
      "from scipy.stats import weibull_min\n",
      "\n",
      "data = np.array([-309, 229, 386, -104, 180, -217, -167, 168, 122, 138])\n",
      "uncensored_data = data[data > 0]\n",
      "params = weibull_min.fit(uncensored_data, floc=0)\n",
      "beta, loc, scale = params\n",
      "eta = scale\n",
      "t0 = loc\n",
      "beta, eta, t0\n",
      "```\n",
      "The script returned no output. Trying again\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import numpy as np\n",
      "from scipy.stats import weibull_min\n",
      "\n",
      "data = np.array([-309, 229, 386, -104, 180, -217, -167, 168, 122, 138])\n",
      "uncensored_data = data[data > 0]\n",
      "params = weibull_min.fit(uncensored_data, floc=0)\n",
      "beta, loc, scale = params\n",
      "eta = scale\n",
      "t0 = loc\n",
      "print(beta, eta, t0)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 2.446499106633478 230.7777675029722 0\n",
      " --> Prediction 1 for question 12 : c\n",
      " --> Prediction 1 for question 13 : a\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "from scipy.stats import binom\n",
      "\n",
      "# Parameters\n",
      "n = 25  # number of trials\n",
      "p = 0.10  # probability of a part being defective\n",
      "k = 6  # we want cumulative probability up to 6 defective parts\n",
      "\n",
      "# Cumulative probability of 6 or fewer defective parts\n",
      "P_X_leq_6 = binom.cdf(k, n, p)\n",
      "\n",
      "# Probability of 7 or more defective parts\n",
      "P_X_geq_7 = 1 - P_X_leq_6\n",
      "\n",
      "P_X_geq_7\n",
      "```\n",
      "The script returned no output. Trying again\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "from scipy.stats import binom\n",
      "\n",
      "# Parameters\n",
      "n = 25  # number of trials\n",
      "p = 0.10  # probability of a part being defective\n",
      "k = 6  # we want cumulative probability up to 6 defective parts\n",
      "\n",
      "# Cumulative probability of 6 or fewer defective parts\n",
      "P_X_leq_6 = binom.cdf(k, n, p)\n",
      "\n",
      "# Probability of 7 or more defective parts\n",
      "P_X_geq_7 = 1 - P_X_leq_6\n",
      "\n",
      "print(P_X_geq_7)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.009476360691506591\n",
      " --> Prediction 1 for question 14 : c\n",
      " --> Prediction 1 for question 15 : b\n",
      " --> Prediction 1 for question 16 : a\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import numpy as np\n",
      "from scipy.stats import chi2\n",
      "\n",
      "# Failure times\n",
      "failure_times = [632, 3450, 816, 928, 150]\n",
      "n = len(failure_times)\n",
      "\n",
      "# Calculate MTTF\n",
      "mttf = np.mean(failure_times)\n",
      "\n",
      "# Calculate chi-square values\n",
      "alpha = 0.1\n",
      "chi2_lower = chi2.ppf(alpha/2, 2*n)\n",
      "chi2_upper = chi2.ppf(1-alpha/2, 2*n)\n",
      "\n",
      "# Calculate confidence bounds\n",
      "lower_bound = (n / chi2_upper) * mttf\n",
      "upper_bound = (n / chi2_lower) * mttf\n",
      "\n",
      "lower_bound, upper_bound\n",
      "```\n",
      "The script returned no output. Trying again\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import numpy as np\n",
      "from scipy.stats import chi2\n",
      "\n",
      "# Failure times\n",
      "failure_times = [632, 3450, 816, 928, 150]\n",
      "n = len(failure_times)\n",
      "\n",
      "# Calculate MTTF\n",
      "mttf = np.mean(failure_times)\n",
      "\n",
      "# Calculate chi-square values\n",
      "alpha = 0.1\n",
      "chi2_lower = chi2.ppf(alpha/2, 2*n)\n",
      "chi2_upper = chi2.ppf(1-alpha/2, 2*n)\n",
      "\n",
      "# Calculate confidence bounds\n",
      "lower_bound = (n / chi2_upper) * mttf\n",
      "upper_bound = (n / chi2_lower) * mttf\n",
      "\n",
      "print(lower_bound, upper_bound)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 326.43183362646084 1516.6361216640962\n",
      " --> Prediction 1 for question 17 : c\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "MTTF = -100 / math.log(0.99)\n",
      "print(MTTF)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 9949.916247342207\n",
      " --> Prediction 1 for question 18 : b\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "# Given values\n",
      "R_t = 0.95  # reliability for one month\n",
      "t = 1       # time in months\n",
      "\n",
      "# Calculate the failure rate (lambda)\n",
      "lambda_ = -math.log(R_t) / t\n",
      "\n",
      "# Calculate the mean number of failures in one year\n",
      "mu = lambda_ * 12\n",
      "\n",
      "# Calculate P(X <= 2)\n",
      "P_X_le_2 = (math.exp(-mu) * (mu**0) / math.factorial(0) +\n",
      "             math.exp(-mu) * (mu**1) / math.factorial(1) +\n",
      "             math.exp(-mu) * (mu**2) / math.factorial(2))\n",
      "\n",
      "# Calculate P(X > 2)\n",
      "P_X_gt_2 = 1 - P_X_le_2\n",
      "\n",
      "# Print the result\n",
      "print(P_X_gt_2)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.02467615186688754\n",
      " --> Prediction 1 for question 19 : b\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "import math\n",
      "\n",
      "# Given values\n",
      "lambda_total = 1  # total failure rate\n",
      "t = 6  # time in months\n",
      "k = 2  # number of parts\n",
      "\n",
      "# Calculating P(X <= t)\n",
      "prob_X_le_t = 1 - math.exp(-lambda_total * t) * (1 + lambda_total * t)\n",
      "\n",
      "# Therefore, P(X > t)\n",
      "prob_X_gt_t = 1 - prob_X_le_t\n",
      "print(prob_X_gt_t)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.017351265236664526\n",
      " --> Prediction 1 for question 20 : c\n",
      " --> Prediction 1 for question 21 : c\n",
      " --> Prediction 1 for question 22 : a\n",
      " --> Prediction 1 for question 23 : a\n",
      " --> Prediction 1 for question 24 : d\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prediction_1</th>\n",
       "      <th>prediction_2</th>\n",
       "      <th>prediction_3</th>\n",
       "      <th>prediction_4</th>\n",
       "      <th>prediction_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_id prediction_1 prediction_2 prediction_3 prediction_4  \\\n",
       "0             1            b            b            b            b   \n",
       "1             2            a            a            a            a   \n",
       "2             3            c            c            c            c   \n",
       "3             4            a            a            a            a   \n",
       "4             5            d            d            d            d   \n",
       "5             6            a            a            a            a   \n",
       "6             7            d            d            d            d   \n",
       "7             8            a            a            a            a   \n",
       "8             9            a            a            a            a   \n",
       "9            10            c            c            c            c   \n",
       "10           11            c            c            c            c   \n",
       "11           12            c            c            c            c   \n",
       "12           13            a            a            a            a   \n",
       "13           14            c            c            c            c   \n",
       "14           15            b            b            b            b   \n",
       "15           16            a            a            a            a   \n",
       "16           17            c            c            c            c   \n",
       "17           18            b            b            b            b   \n",
       "18           19            b            b            b            b   \n",
       "19           20            c            c            c            c   \n",
       "20           21            c            c            c            c   \n",
       "21           22            a            a            a            a   \n",
       "22           23            a            a            a            a   \n",
       "23           24            d            d            d            d   \n",
       "\n",
       "   prediction_5  \n",
       "0             b  \n",
       "1             a  \n",
       "2             c  \n",
       "3             a  \n",
       "4             d  \n",
       "5             a  \n",
       "6             d  \n",
       "7             a  \n",
       "8             a  \n",
       "9             c  \n",
       "10            c  \n",
       "11            c  \n",
       "12            a  \n",
       "13            c  \n",
       "14            b  \n",
       "15            a  \n",
       "16            c  \n",
       "17            b  \n",
       "18            b  \n",
       "19            c  \n",
       "20            c  \n",
       "21            a  \n",
       "22            a  \n",
       "23            d  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PythonAgent.inject_python(openai_client=openai_client)\n",
    "test_submission(double_prompting, fake_multiple_attempts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RAG prompting**\n",
    "\n",
    "We now augment the knowledge of our model using Retrieval Augmented Generation. We built a database containing specific information about reliability engineering, and will use it to augment our prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 467/467 [00:40<00:00, 11.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "||                               ||\n",
      "||          _________            ||\n",
      "||         |         |           ||\n",
      "||         |R I S K  |           ||\n",
      "||         |  H I V E|           ||\n",
      "||         |_________|           ||\n",
      "||                               ||\n",
      "===================================\n",
      "[STATUS]: All systems up. Ready to analyze some risk!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.utils import init_string\n",
    "\n",
    "rag = QdrantRAG(rag_path=\"generated/extracted_rag.json\", qdrant_client=qdrant_client)\n",
    "\n",
    "MAX_CONTEXT_LENGTH = 1000\n",
    "\n",
    "\n",
    "class RiskHive(DoublePrompting):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.augment_max_length = MAX_CONTEXT_LENGTH\n",
    "        init_string()\n",
    "\n",
    "    def inject_context(self, q: str):\n",
    "        question, _ = q.split(\"[Choices]\")\n",
    "        search = rag.search(question)\n",
    "        element = f\"\"\"\n",
    "Information ranked by relevance:\n",
    "1 : {search[0]}\n",
    "2 : {search[1]}\n",
    "3 : {search[2]}\n",
    "\"\"\"\n",
    "        return f\"{q}\\n\\n Here is some information that could help. If there appears to be the correct mathematical formula, you can use it :  ```{element[0 : self.augment_max_length]}...```\"\n",
    "\n",
    "    def get_1_answer(self, q):\n",
    "        return super().get_1_answer(self.inject_context(q))\n",
    "\n",
    "\n",
    "rh = RiskHive(test_questions, openai_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Python agent initialized ---\n",
      "\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Given parameters\n",
      "mean = 150  # mean life\n",
      "variance = 400  # variance\n",
      "std_dev = np.sqrt(variance)  # standard deviation\n",
      "\n",
      "# To find B10 life, we need to find the 10th percentile of the normal distribution\n",
      "b10_life = np.percentile(np.random.normal(mean, std_dev, 100000), 10)\n",
      "print(b10_life)\n",
      "print(print(b10_life))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 124.49725094465978\n",
      "124.49725094465978\n",
      "None\n",
      " --> Prediction 1 for question 1 : b <-- \n",
      " --> Prediction 1 for question 2 : a <-- \n",
      " --> Prediction 1 for question 3 : c <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import scipy.stats as stats\n",
      "\n",
      "# Given data\n",
      "sample_variance = 4  # ohms^2\n",
      "n = 20  # sample size\n",
      "alpha = 0.05  # significance level for 95% confidence interval\n",
      "\n",
      "# Degrees of freedom\n",
      "df = n - 1\n",
      "\n",
      "# Chi-squared distribution critical values\n",
      "chi2_lower = stats.chi2.ppf(1 - alpha / 2, df)\n",
      "chi2_upper = stats.chi2.ppf(alpha / 2, df)\n",
      "\n",
      "# Confidence interval for the population variance\n",
      "ci_lower = (df * sample_variance) / chi2_upper\n",
      "ci_upper = (df * sample_variance) / chi2_lower\n",
      "\n",
      "ci_lower, ci_upper\n",
      "print((ci_lower, ci_upper))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: (np.float64(8.533078016943891), np.float64(2.3133825594720308))\n",
      " --> Prediction 1 for question 4 : a <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import scipy.stats as stats\n",
      "\n",
      "# Given data\n",
      "sample_mean = 10  # average resistance\n",
      "sample_std = 2     # standard deviation\n",
      "n = 50             # sample size\n",
      "confidence_level = 0.95\n",
      "\n",
      "# Calculate the critical value for the confidence level\n",
      "z_critical = stats.norm.ppf((1 + confidence_level) / 2)\n",
      "\n",
      "# Calculate the margin of error\n",
      "margin_of_error = z_critical * (sample_std / np.sqrt(n))\n",
      "\n",
      "# Calculate the confidence interval\n",
      "confidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n",
      "print(confidence_interval)\n",
      "print(print(confidence_interval))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: (np.float64(9.445638470260128), np.float64(10.554361529739872))\n",
      "(np.float64(9.445638470260128), np.float64(10.554361529739872))\n",
      "None\n",
      " --> Prediction 1 for question 5 : a <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy.stats import weibull_min\n",
      "\n",
      "# Given parameters\n",
      "beta = 2  # shape parameter\n",
      "eta = 600  # original characteristic life\n",
      "new_eta = 2 * eta  # new characteristic life\n",
      "failures = [700, 900, 1000, 1100, 1300]\n",
      "\n",
      "# Calculate the MLE for the new characteristic life\n",
      "new_shape = beta  # assuming the shape parameter remains the same\n",
      "new_scale = np.mean(failures)  # mean of failures as a scale parameter\n",
      "\n",
      "# Calculate the survival function for the new design\n",
      "survival_prob = weibull_min.sf(new_eta, new_shape, scale=new_scale)\n",
      "\n",
      "# Check if the survival probability is greater than 0.95\n",
      "is_double = survival_prob > 0.95\n",
      "is_double\n",
      "print(is_double)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: False\n",
      " --> Prediction 1 for question 6 : b <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "from scipy.stats import binom\n",
      "\n",
      "# Given values\n",
      "p = 0.995  # reliability of each engine\n",
      "n = 3      # number of engines\n",
      "T = 1      # time (not needed in calculation here)\n",
      "\n",
      "# Probability of at least 2 engines functioning\n",
      "# P(X >= 2) = 1 - P(X < 2) = 1 - (P(X = 0) + P(X = 1))\n",
      "P_0 = binom.pmf(0, n, p)  # Probability that 0 engines function\n",
      "P_1 = binom.pmf(1, n, p)  # Probability that 1 engine functions\n",
      "\n",
      "P_at_least_2 = 1 - (P_0 + P_1)\n",
      "\n",
      "print(P_at_least_2)\n",
      "print(print(P_at_least_2))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.99992525\n",
      "0.99992525\n",
      "None\n",
      " --> Prediction 1 for question 7 : d <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Given parameters\n",
      "beta = 0.5  # shape parameter\n",
      "eta = 10000  # scale parameter in hours\n",
      "\n",
      "# Time period in hours (1 year)\n",
      "time_period = 1 * 365 * 24  # converting 1 year to hours\n",
      "\n",
      "# Calculate the survival function S(t) = 1 - F(t)\n",
      "# where F(t) is the cumulative distribution function for the Weibull distribution\n",
      "S_t = np.exp(-(time_period / eta) ** beta)\n",
      "\n",
      "# Print the probability that the component will still be operational after one year\n",
      "print(S_t)\n",
      "print(print(S_t))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.39221358944618695\n",
      "0.39221358944618695\n",
      "None\n",
      " --> Prediction 1 for question 8 : d <-- \n",
      " --> Prediction 1 for question 9 : b <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "# Given data\n",
      "P_Crack = 0.05  # Probability that a part has a crack\n",
      "P_Signal_Crack_given_Crack = 0.98  # Probability that the kit signals a crack given there is a crack\n",
      "P_False_Alarm = 0.03  # Probability that the kit signals a crack given there is no crack\n",
      "\n",
      "# Probability that a part does not have a crack\n",
      "P_No_Crack = 1 - P_Crack\n",
      "\n",
      "# Total probability of the kit signaling a crack\n",
      "P_Signal_Crack = (P_Signal_Crack_given_Crack * P_Crack) + (P_False_Alarm * P_No_Crack)\n",
      "\n",
      "# Applying Bayes' theorem to find the probability that a part is really bad when a kit signals a crack\n",
      "P_Bad_given_Signal_Crack = (P_Signal_Crack_given_Crack * P_Crack) / P_Signal_Crack\n",
      "\n",
      "P_Bad_given_Signal_Crack\n",
      "print(P_Bad_given_Signal_Crack)\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.6322580645161291\n",
      " --> Prediction 1 for question 10 : b <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy.stats import poisson\n",
      "\n",
      "# Given average defects per chip\n",
      "lambda_defects = 7\n",
      "\n",
      "# Calculate the probability of less than 3 defects\n",
      "prob_less_than_3 = poisson.cdf(2, lambda_defects)\n",
      "\n",
      "print(prob_less_than_3)\n",
      "print(print(prob_less_than_3))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.029636163880521777\n",
      "0.029636163880521777\n",
      "None\n",
      " --> Prediction 1 for question 11 : c <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from reliability import Weibull_fitter\n",
      "\n",
      "# Given data with censored values represented as NaN\n",
      "data = np.array([-309, 229, 386, -104, 180, -217, -167, 168, 122, 138])\n",
      "# Censoring the negative values\n",
      "censored_data = np.where(data < 0, np.nan, data)\n",
      "\n",
      "# Fit the Weibull distribution to the data\n",
      "wf = Weibull_fitter(censored_data, timeline=None)\n",
      "\n",
      "# Extract the parameters\n",
      "beta = wf.beta\n",
      "eta = wf.eta\n",
      "t0 = wf.t0\n",
      "\n",
      "# Print the results\n",
      "print(beta, eta, t0)\n",
      "print(print(beta, eta, t0))\n",
      "```\n",
      "An error occurred: Script execution failed: Traceback (most recent call last):\n",
      "  File \"/Users/ivanrobert/Documents/CS_2024/ORRA/GAIR/GAIR-riskhive/sandbox_script.py\", line 2, in <module>\n",
      "    from reliability import Weibull_fitter\n",
      "ImportError: cannot import name 'Weibull_fitter' from 'reliability' (/Users/ivanrobert/Documents/CS_2024/ORRA/GAIR/GAIR-riskhive/.venv/lib/python3.11/site-packages/reliability/__init__.py). Scolding model and retrying...\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy.stats import weibull_min\n",
      "\n",
      "# Given data with censored values represented as NaN\n",
      "data = np.array([-309, 229, 386, -104, 180, -217, -167, 168, 122, 138])\n",
      "# Censoring the negative values\n",
      "censored_data = data[data > 0]\n",
      "\n",
      "# Fit the Weibull distribution to the data\n",
      "params = weibull_min.fit(censored_data, floc=0)  # Fix location to 0\n",
      "beta, loc, eta = params\n",
      "\n",
      "# Print the results\n",
      "print(beta, eta, loc)\n",
      "print(print(beta, eta, loc))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 2.446499106633478 230.7777675029722 0\n",
      "2.446499106633478 230.7777675029722 0\n",
      "None\n",
      " --> Prediction 1 for question 12 : c <-- \n",
      " --> Prediction 1 for question 13 : b <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "from scipy.stats import binom\n",
      "\n",
      "n = 25  # number of trials\n",
      "p = 0.1  # probability of defect\n",
      "k = 7  # number of defects\n",
      "\n",
      "# Calculate the probability of k or more defects\n",
      "probability = 1 - binom.cdf(k - 1, n, p)\n",
      "print(probability)\n",
      "print(print(probability))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.009476360691506591\n",
      "0.009476360691506591\n",
      "None\n",
      " --> Prediction 1 for question 14 : d <-- \n",
      " --> Prediction 1 for question 15 : b <-- \n",
      " --> Prediction 1 for question 16 : c <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from scipy import stats\n",
      "\n",
      "# Failure times\n",
      "failure_times = np.array([632, 3450, 816, 928, 150])\n",
      "\n",
      "# Calculate MTTF (mean)\n",
      "mttf = np.mean(failure_times)\n",
      "\n",
      "# Number of failures\n",
      "n = len(failure_times)\n",
      "\n",
      "# Calculate the standard error\n",
      "standard_error = np.std(failure_times, ddof=1) / np.sqrt(n)\n",
      "\n",
      "# Calculate the 90% confidence interval\n",
      "confidence_level = 0.90\n",
      "z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
      "margin_of_error = z_score * standard_error\n",
      "\n",
      "# Confidence bounds\n",
      "lower_bound = mttf - margin_of_error\n",
      "upper_bound = mttf + margin_of_error\n",
      "\n",
      "(lower_bound, upper_bound)\n",
      "print((lower_bound, upper_bound))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: (np.float64(242.53656232378512), np.float64(2147.8634376762147))\n",
      " --> Prediction 1 for question 17 : c <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import scipy.stats as stats\n",
      "\n",
      "# Given data\n",
      "reliability_target = 0.99\n",
      "n_vehicles = 10\n",
      "\n",
      "# Calculate the number of successful vehicles needed to meet the reliability target\n",
      "k_success = int(n_vehicles * reliability_target)\n",
      "\n",
      "# Using the binomial distribution to find the MTTF\n",
      "# The number of trials is the total distance (50,000 km) divided by the mission distance (100 km)\n",
      "trials = 50000 / 100\n",
      "\n",
      "# Calculate the MTTF using the formula for MTTF in terms of reliability\n",
      "mttf = trials / (1 - stats.binom.cdf(k_success - 1, n_vehicles, reliability_target))\n",
      "\n",
      "print(mttf)\n",
      "print(print(mttf))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 502.14223934342283\n",
      "502.14223934342283\n",
      "None\n",
      " --> Prediction 1 for question 18 : a <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Given values\n",
      "reliability_one_month = 0.95\n",
      "failure_rate = 1 - reliability_one_month  # Monthly failure rate\n",
      "months_in_year = 12\n",
      "\n",
      "# Calculate the number of failures in a year (Poisson distribution)\n",
      "lambda_year = failure_rate * months_in_year\n",
      "\n",
      "# Probability of needing more than 2 spare bulbs\n",
      "# Poisson probability mass function\n",
      "prob_more_than_2 = 1 - (np.exp(-lambda_year) * (lambda_year**0 / np.math.factorial(0)) +\n",
      "                         np.exp(-lambda_year) * (lambda_year**1 / np.math.factorial(1)) +\n",
      "                         np.exp(-lambda_year) * (lambda_year**2 / np.math.factorial(2)))\n",
      "\n",
      "print(prob_more_than_2)\n",
      "print(print(prob_more_than_2))\n",
      "```\n",
      "An error occurred: Script execution failed: Traceback (most recent call last):\n",
      "  File \"/Users/ivanrobert/Documents/CS_2024/ORRA/GAIR/GAIR-riskhive/sandbox_script.py\", line 13, in <module>\n",
      "    prob_more_than_2 = 1 - (np.exp(-lambda_year) * (lambda_year**0 / np.math.factorial(0)) +\n",
      "                                                                     ^^^^^^^\n",
      "  File \"/Users/ivanrobert/Documents/CS_2024/ORRA/GAIR/GAIR-riskhive/.venv/lib/python3.11/site-packages/numpy/__init__.py\", line 414, in __getattr__\n",
      "    raise AttributeError(\"module {!r} has no attribute \"\n",
      "AttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?. Scolding model and retrying...\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Given values\n",
      "reliability_one_month = 0.95\n",
      "failure_rate = 1 - reliability_one_month  # Monthly failure rate\n",
      "months_in_year = 12\n",
      "\n",
      "# Calculate the number of failures in a year (Poisson distribution)\n",
      "lambda_year = failure_rate * months_in_year\n",
      "\n",
      "# Probability of needing more than 2 spare bulbs\n",
      "# Poisson probability mass function using a loop\n",
      "prob_more_than_2 = 1.0\n",
      "for k in range(3):  # k = 0, 1, 2\n",
      "    prob_more_than_2 -= (np.exp(-lambda_year) * (lambda_year**k) / np.math.factorial(k))\n",
      "\n",
      "print(prob_more_than_2)\n",
      "print(print(prob_more_than_2))\n",
      "```\n",
      "An error occurred: Script execution failed: Traceback (most recent call last):\n",
      "  File \"/Users/ivanrobert/Documents/CS_2024/ORRA/GAIR/GAIR-riskhive/sandbox_script.py\", line 15, in <module>\n",
      "    prob_more_than_2 -= (np.exp(-lambda_year) * (lambda_year**k) / np.math.factorial(k))\n",
      "                                                                   ^^^^^^^\n",
      "  File \"/Users/ivanrobert/Documents/CS_2024/ORRA/GAIR/GAIR-riskhive/.venv/lib/python3.11/site-packages/numpy/__init__.py\", line 414, in __getattr__\n",
      "    raise AttributeError(\"module {!r} has no attribute \"\n",
      "AttributeError: module 'numpy' has no attribute 'math'. Did you mean: 'emath'?. Scolding model and retrying...\n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import math\n",
      "\n",
      "# Given values\n",
      "reliability_one_month = 0.95\n",
      "failure_rate = 1 - reliability_one_month  # Monthly failure rate\n",
      "months_in_year = 12\n",
      "\n",
      "# Calculate the number of failures in a year (Poisson distribution)\n",
      "lambda_year = failure_rate * months_in_year\n",
      "\n",
      "# Probability of needing more than 2 spare bulbs\n",
      "# Poisson probability mass function using a loop\n",
      "prob_more_than_2 = 1.0\n",
      "for k in range(3):  # k = 0, 1, 2\n",
      "    prob_more_than_2 -= (np.exp(-lambda_year) * (lambda_year**k) / math.factorial(k))\n",
      "\n",
      "print(prob_more_than_2)\n",
      "print(print(prob_more_than_2))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.023115287752632926\n",
      "0.023115287752632926\n",
      "None\n",
      " --> Prediction 1 for question 19 : b <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Given data\n",
      "MTTF = 2  # months\n",
      "spare_parts = 2\n",
      "total_time = 6  # months\n",
      "\n",
      "# Calculate the failure rate (lambda)\n",
      "# MTTF = 1/lambda -> lambda = 1/MTTF\n",
      "lambda_failure = 1 / MTTF\n",
      "\n",
      "# Calculate the probability of surviving the total time\n",
      "# The probability of surviving time t is given by R(t) = e^(-lambda * t)\n",
      "# Here, we need to consider the case with 2 spare parts\n",
      "# If we have 2 parts, we have 3 total parts in use (1 in use + 2 spares)\n",
      "# The engine can survive as long as at least one part is functioning.\n",
      "\n",
      "# Probability of failure of one part in total_time\n",
      "prob_failure_one_part = 1 - np.exp(-lambda_failure * total_time)\n",
      "\n",
      "# Probability of surviving with 3 parts\n",
      "# P(survive) = 1 - P(all parts fail)\n",
      "# P(all parts fail) = P(failure_one_part)^3\n",
      "prob_survive = 1 - (prob_failure_one_part ** (spare_parts + 1))\n",
      "\n",
      "# Print the result\n",
      "print(prob_survive)\n",
      "print(print(prob_survive))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.14204835837767948\n",
      "0.14204835837767948\n",
      "None\n",
      " --> Prediction 1 for question 20 : b <-- \n",
      "Found a python script to execute.\n",
      "Executing the following script: \n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "# Given values\n",
      "lambda_value = 1  # assuming a constant failure rate for calculation purposes\n",
      "\n",
      "# Variance formula for two units in active parallel\n",
      "variance = (3/4) * (lambda_value ** 2)\n",
      "print(variance)\n",
      "print(print(variance))\n",
      "```\n",
      "Script executed successfully.\n",
      "Prompting with the result: 0.75\n",
      "0.75\n",
      "None\n",
      " --> Prediction 1 for question 21 : a <-- \n",
      " --> Prediction 1 for question 22 : a <-- \n",
      " --> Prediction 1 for question 23 : a <-- \n",
      " --> Prediction 1 for question 24 : d <-- \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>prediction_1</th>\n",
       "      <th>prediction_2</th>\n",
       "      <th>prediction_3</th>\n",
       "      <th>prediction_4</th>\n",
       "      <th>prediction_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    question_id prediction_1 prediction_2 prediction_3 prediction_4  \\\n",
       "0             1            b            b            b            b   \n",
       "1             2            a            a            a            a   \n",
       "2             3            c            c            c            c   \n",
       "3             4            a            a            a            a   \n",
       "4             5            a            a            a            a   \n",
       "5             6            b            b            b            b   \n",
       "6             7            d            d            d            d   \n",
       "7             8            d            d            d            d   \n",
       "8             9            b            b            b            b   \n",
       "9            10            b            b            b            b   \n",
       "10           11            c            c            c            c   \n",
       "11           12            c            c            c            c   \n",
       "12           13            b            b            b            b   \n",
       "13           14            d            d            d            d   \n",
       "14           15            b            b            b            b   \n",
       "15           16            c            c            c            c   \n",
       "16           17            c            c            c            c   \n",
       "17           18            a            a            a            a   \n",
       "18           19            b            b            b            b   \n",
       "19           20            b            b            b            b   \n",
       "20           21            a            a            a            a   \n",
       "21           22            a            a            a            a   \n",
       "22           23            a            a            a            a   \n",
       "23           24            d            d            d            d   \n",
       "\n",
       "   prediction_5  \n",
       "0             b  \n",
       "1             a  \n",
       "2             c  \n",
       "3             a  \n",
       "4             a  \n",
       "5             b  \n",
       "6             d  \n",
       "7             d  \n",
       "8             b  \n",
       "9             b  \n",
       "10            c  \n",
       "11            c  \n",
       "12            b  \n",
       "13            d  \n",
       "14            b  \n",
       "15            c  \n",
       "16            c  \n",
       "17            a  \n",
       "18            b  \n",
       "19            b  \n",
       "20            a  \n",
       "21            a  \n",
       "22            a  \n",
       "23            d  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PythonAgent.inject_python(openai_client=openai_client)\n",
    "# test_submission(rh, fake_multiple_attempts=True)\n",
    "rh.get_submission(save_path=\"generated/risk_hive_submission.csv\", fake_multiple_attempts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
